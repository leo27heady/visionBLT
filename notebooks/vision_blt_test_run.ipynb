{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4548ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leoni\\Documents\\projects\\visionBLT\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875ce71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420bc07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 22:35:58.004000 25636 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "c:\\Users\\leoni\\Documents\\projects\\visionBLT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from bytelatent.args import (\n",
    "    TrainArgs, \n",
    "    DataloaderArgs, OptimArgs, ByteLatentTransformerArgs, DistributedArgs, TokenizerArgs, PatcherArgs,\n",
    "    ProfilerArgs, CheckpointArgs, LoggingArgs\n",
    ")\n",
    "from bytelatent.checkpoint import SaveEvery\n",
    "from bytelatent.model.local_models import LocalModelArgs, LocalEncoder, VisionModelArgs\n",
    "from bytelatent.model.blt import (\n",
    "    get_encoder_dim_token_emb, get_encoder_dim_patch_emb, compute_hash_embeddings, cross_attn_mask, create_patch_mask_from_ids, patch_ids_from_frames, \n",
    ")\n",
    "from bytelatent.model.utils import create_vision_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404f17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainArgs(\n",
    "    name=\"debug\",\n",
    "    dump_dir=\"/tmp/\",\n",
    "    seed=777,\n",
    "    steps=100_000,\n",
    "    optim=OptimArgs(\n",
    "        lr=4e-04,\n",
    "        warmup=500,\n",
    "        lr_min_ratio=0.1,\n",
    "        clip=10.0\n",
    "    ),\n",
    "    distributed=DistributedArgs(\n",
    "        fsdp_type=\"full_shard\",\n",
    "        model_dtype=\"bf16\",\n",
    "        matmul_allow_tf32=False,\n",
    "        selective_activation_checkpointing=False,\n",
    "        tp_size=1\n",
    "    ),\n",
    "    model=ByteLatentTransformerArgs(\n",
    "        vision=VisionModelArgs(\n",
    "            img_channels=1,\n",
    "            img_height=64,\n",
    "            img_width=64,\n",
    "            scale_factor=2,\n",
    "            inner_channels=96,\n",
    "            norm_channels=4,\n",
    "            num_process_layers=2,\n",
    "            tile_height=4,\n",
    "            tile_width=4,\n",
    "            patch_size=4\n",
    "        ),\n",
    "        n_heads=2,\n",
    "        n_heads_local_encoder=2,\n",
    "        n_heads_local_decoder=2,\n",
    "        dim=96,\n",
    "        vocab_size=260,\n",
    "        dim_token=96,\n",
    "        patch_size=6,\n",
    "        patching_mode=\"space\",\n",
    "        tie_local_encoder_decoder_logits=False,\n",
    "        patch_in_forward=False,\n",
    "        max_encoder_seq_length=3072,\n",
    "        pad_to_max_length=True,\n",
    "        patching_threshold=3.1439168453216553,\n",
    "        encoder_hash_byte_group_size=None,\n",
    "        encoder_hash_byte_group_vocab=50002,\n",
    "        encoder_hash_byte_group_nb_functions=3,\n",
    "        encoder_enable_byte_ngrams=False,\n",
    "        cross_attn_encoder=True, # assuming cross_attention is true\n",
    "        cross_attn_decoder=True, # assuming cross_attention is true\n",
    "        cross_attn_window_encoder=None,\n",
    "        cross_attn_window_decoder=None,\n",
    "        dim_local_encoder=96,\n",
    "        dim_local_decoder=96,\n",
    "        cross_attn_k=2,\n",
    "        cross_attn_nheads=2,\n",
    "        cross_attn_all_layers_decoder=True,\n",
    "        cross_attn_all_layers_encoder=True,\n",
    "        cross_attn_use_flex_attention=False,\n",
    "        cross_attn_init_by_pooling=True,\n",
    "        log_patch_lengths=True,\n",
    "        non_linearity=\"swiglu\",\n",
    "        use_rope=True,\n",
    "        recompute_fc1_out=False,\n",
    "        recompute_fc3_out=False,\n",
    "        recompute_attn=False,\n",
    "        custom_bwd=False,\n",
    "        layer_ckpt=\"none\",\n",
    "        use_local_encoder_transformer=True,\n",
    "        init_use_gaussian=True,\n",
    "        init_use_depth=\"current\",\n",
    "        attn_impl=\"xformers\",\n",
    "        attn_bias_type=\"block_causal\",\n",
    "        alpha_depth=\"disabled\",\n",
    "        max_length=256,\n",
    "        local_attention_window_len=512,\n",
    "        downsampling_by_pooling=\"max\",\n",
    "    ),\n",
    "    data=DataloaderArgs(\n",
    "        root_dir=\".\",\n",
    "        sources={\"alpaca-cleaned\": 1.0},\n",
    "        dataset_files=[\"alpaca-cleaned/1.json\"],\n",
    "        batch_size=2,\n",
    "        prefetch_size=64,\n",
    "        seq_len=1024,\n",
    "        max_encoder_seq_length=3072,\n",
    "        load_async=False,\n",
    "        preprocess_dir=\"preprocess_dir\",\n",
    "        patcher_args=PatcherArgs(patching_mode=\"space\"),\n",
    "        tokenizer_args=TokenizerArgs(name=\"blt\"),\n",
    "    ),\n",
    "    profiling=ProfilerArgs(run=False),\n",
    "    checkpoint=CheckpointArgs(\n",
    "        path=\"train_checkpoints\",\n",
    "        dump=SaveEvery(\n",
    "            every=500,\n",
    "            keep=3\n",
    "        ),\n",
    "        eval=SaveEvery(\n",
    "            every=1000,\n",
    "            keep=-1\n",
    "        )\n",
    "    ),\n",
    "    logging=LoggingArgs(freq=10),\n",
    "    eval_on_gpus=1,\n",
    "    # env=None\n",
    ")\n",
    "\n",
    "args = train_args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5f9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28b4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_encoder_args = LocalModelArgs(\n",
    "    # Updated args\n",
    "    vision=args.vision,\n",
    "    dim=args.dim_local_encoder,\n",
    "    n_layers=args.n_layers_local_encoder,\n",
    "    n_heads=args.n_heads_local_encoder,\n",
    "    dim_token_emb=get_encoder_dim_token_emb(args),\n",
    "    dim_patch_emb=get_encoder_dim_patch_emb(args),\n",
    "    cross_attn_encoder=args.cross_attn_encoder,\n",
    "    cross_attn_decoder=False,\n",
    "    cross_attn_k=args.cross_attn_k if args.cross_attn_encoder else None,\n",
    "    cross_attn_init_by_pooling=args.cross_attn_init_by_pooling,\n",
    "    # Defaults\n",
    "    head_dim=args.head_dim,\n",
    "    max_seqlen=args.max_encoder_seq_length,\n",
    "    dropout=args.dropout,\n",
    "    vocab_size=args.vocab_size + args.pm_size,\n",
    "    norm_eps=args.norm_eps,\n",
    "    patch_size=args.patch_size,\n",
    "    sliding_window=args.local_attention_window_len,\n",
    "    use_rope=args.use_rope,\n",
    "    rope_theta=args.rope_theta,\n",
    "    rope_use_fp32_in_outer_product=args.rope_use_fp32_in_outer_product,\n",
    "    init_base_std=args.init_base_std,\n",
    "    init_std_factor=args.init_std_factor,\n",
    "    n_kv_heads=args.n_kv_heads,\n",
    "    attn_impl=\"sdpa\",\n",
    "    attn_bias_type=\"local_block_causal\",\n",
    "    multiple_of=args.multiple_of,\n",
    "    ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "    patching_mode=args.patching_mode,\n",
    "    use_local_encoder_transformer=args.use_local_encoder_transformer,\n",
    "    downsampling_by_pooling=args.downsampling_by_pooling,\n",
    "    encoder_hash_byte_group_size=args.encoder_hash_byte_group_size,\n",
    "    cross_attn_all_layers_encoder=args.cross_attn_all_layers_encoder,\n",
    "    cross_attn_all_layers_decoder=args.cross_attn_all_layers_decoder,\n",
    "    cross_attn_nheads=args.cross_attn_nheads,\n",
    "    eos_id=args.eos_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3741933",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_encoder = LocalEncoder(local_encoder_args).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b7e9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (wk): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (wv): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (wo): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=96, out_features=256, bias=False)\n",
       "        (w3): Linear(in_features=96, out_features=256, bias=False)\n",
       "        (w2): Linear(in_features=256, out_features=96, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn_norm): RMSNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (rope): RotaryEmbeddingNd()\n",
       "  (patch_embedding_projection): Linear(in_features=96, out_features=192, bias=False)\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Conv2d(1, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Sequential(\n",
       "      (0): GroupNorm(4, 96, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): Conv2d(96, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (cross_attn_layers): ModuleList(\n",
       "    (0-7): 8 x CrossAttention(\n",
       "      (cross_attn_norm_q): RMSNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_attn_norm_kv): RMSNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (wq): Linear(in_features=96, out_features=96, bias=False)\n",
       "      (wk): Linear(in_features=96, out_features=96, bias=False)\n",
       "      (wv): Linear(in_features=96, out_features=96, bias=False)\n",
       "      (wo): Linear(in_features=96, out_features=96, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fb861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477c17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10deafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_attn_mask_enc: torch.Size([4, 1, 384, 10240])\n",
      "mask: torch.Size([10240, 10240])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_frames, channels, height, width = (\n",
    "    4, 10, 1, 64, 64\n",
    ")\n",
    "frames = torch.randn(batch_size, num_frames, channels, height, width).to(\"cuda\")\n",
    "frames = frames.reshape(batch_size*num_frames, channels, height, width)\n",
    "encoder_input = local_encoder.image_encoder(frames)\n",
    "\n",
    "frame_height, frame_width = encoder_input.shape[2:]\n",
    "patch_ids = patch_ids_from_frames(\n",
    "    batch_size=batch_size,\n",
    "    num_frames=num_frames,\n",
    "    height=frame_height,\n",
    "    width=frame_width,\n",
    "    tile_height=args.vision.tile_height, \n",
    "    tile_width=args.vision.tile_width, \n",
    "    patch_size=args.vision.patch_size,\n",
    "    device=encoder_input.device\n",
    ")\n",
    "\n",
    "frame_elements = frame_height * frame_width * num_frames\n",
    "encoder_input = encoder_input.permute(0, 2, 3, 1).reshape(batch_size, -1, local_encoder.inner_channels)\n",
    "\n",
    "patch_lengths = torch.unique(patch_ids).unsqueeze(dim=0).to(\"cuda\")\n",
    "local_encoder_embeds = None\n",
    "\n",
    "cross_attn_mask_enc = cross_attn_mask(\n",
    "    patch_ids,\n",
    "    patch_lengths,\n",
    "    frame_elements,\n",
    "    patches_as_queries=True,\n",
    "    cross_attn_k=args.cross_attn_k,\n",
    "    window=args.cross_attn_window_encoder,\n",
    "    block_mask=args.cross_attn_use_flex_attention,\n",
    ").to(\"cuda\")\n",
    "print(\"cross_attn_mask_enc:\", cross_attn_mask_enc.shape)\n",
    "\n",
    "mask = create_vision_causal_mask(\n",
    "    patch_ids.shape[1],\n",
    "    frame_elements,\n",
    "    local_encoder_args.attn_impl,\n",
    "    \"causal\"\n",
    ").to(\"cuda\")\n",
    "print(\"mask:\", mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6740abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(h_encoder, h_cross), cache_encoder = local_encoder(\n",
    "    frames=encoder_input,\n",
    "    mask=mask,\n",
    "    cross_mask=cross_attn_mask_enc,\n",
    "    patch_embeds=None,\n",
    "    num_patches=patch_lengths.shape[1],\n",
    "    patch_ids=patch_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a5b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 10240, 96]), torch.Size([4, 384, 96]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_encoder.shape, h_cross.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6808a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fda6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
