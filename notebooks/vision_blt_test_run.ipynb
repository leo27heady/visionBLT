{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4548ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leoni\\Documents\\projects\\visionBLT\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875ce71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420bc07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 10:11:43.272000 16600 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "c:\\Users\\leoni\\Documents\\projects\\visionBLT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from bytelatent.args import (\n",
    "    TrainArgs, \n",
    "    DataloaderArgs, OptimArgs, ByteLatentTransformerArgs, DistributedArgs, TokenizerArgs, PatcherArgs,\n",
    "    ProfilerArgs, CheckpointArgs, LoggingArgs\n",
    ")\n",
    "from bytelatent.checkpoint import SaveEvery\n",
    "from bytelatent.model.local_models import LocalModelArgs, LocalEncoder, LocalDecoder, VisionModelArgs\n",
    "from bytelatent.model.blt import (\n",
    "    GlobalTransformer, \n",
    "    get_global_dim_patch_emb, compute_hash_embeddings, cross_attn_mask, create_patch_mask_from_ids, patch_ids_from_frames, \n",
    "    create_local_encoder, create_local_decoder, \n",
    "    get_encoder_dim_token_emb, get_encoder_dim_patch_emb,\n",
    "    get_decoder_dim_token_emb, decoder_patch_ids_from_lengths, \n",
    ")\n",
    "from bytelatent.model.utils import create_vision_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc8ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b045a2",
   "metadata": {},
   "source": [
    "# Define args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404f17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainArgs(\n",
    "    name=\"debug\",\n",
    "    dump_dir=\"/tmp/\",\n",
    "    seed=777,\n",
    "    steps=100_000,\n",
    "    optim=OptimArgs(\n",
    "        lr=4e-04,\n",
    "        warmup=500,\n",
    "        lr_min_ratio=0.1,\n",
    "        clip=10.0\n",
    "    ),\n",
    "    distributed=DistributedArgs(\n",
    "        fsdp_type=\"full_shard\",\n",
    "        model_dtype=\"bf16\",\n",
    "        matmul_allow_tf32=False,\n",
    "        selective_activation_checkpointing=False,\n",
    "        tp_size=1\n",
    "    ),\n",
    "    model=ByteLatentTransformerArgs(\n",
    "        vision=VisionModelArgs(\n",
    "            img_channels=1,\n",
    "            img_height=64,\n",
    "            img_width=64,\n",
    "            scale_factor=2,\n",
    "            norm_channels=4,\n",
    "            num_process_layers=2,\n",
    "            tile_height=4,\n",
    "            tile_width=4,\n",
    "        ),\n",
    "        n_heads=2,\n",
    "        n_heads_local_encoder=2,\n",
    "        n_heads_local_decoder=2,\n",
    "        dim=96,\n",
    "        cross_attn_k=2,\n",
    "        dim_global=96*2,\n",
    "        vocab_size=260,\n",
    "        dim_token=96,\n",
    "        patch_size=5,  # number of consecutive frames sharing the same tile\n",
    "        patching_mode=\"space\",\n",
    "        tie_local_encoder_decoder_logits=False,\n",
    "        patch_in_forward=False,\n",
    "        max_encoder_seq_length=3072,\n",
    "        pad_to_max_length=True,\n",
    "        patching_threshold=3.1439168453216553,\n",
    "        encoder_hash_byte_group_size=None,\n",
    "        encoder_hash_byte_group_vocab=50002,\n",
    "        encoder_hash_byte_group_nb_functions=3,\n",
    "        encoder_enable_byte_ngrams=False,\n",
    "        cross_attn_encoder=True, # assuming cross_attention is true\n",
    "        cross_attn_decoder=True, # assuming cross_attention is true\n",
    "        cross_attn_window_encoder=None,\n",
    "        cross_attn_window_decoder=None,\n",
    "        dim_local_encoder=96,\n",
    "        dim_local_decoder=96,\n",
    "        cross_attn_nheads=2,\n",
    "        cross_attn_all_layers_decoder=True,\n",
    "        cross_attn_all_layers_encoder=True,\n",
    "        cross_attn_use_flex_attention=False,\n",
    "        cross_attn_init_by_pooling=True,\n",
    "        log_patch_lengths=True,\n",
    "        non_linearity=\"swiglu\",\n",
    "        use_rope=True,\n",
    "        recompute_fc1_out=False,\n",
    "        recompute_fc3_out=False,\n",
    "        recompute_attn=False,\n",
    "        custom_bwd=False,\n",
    "        layer_ckpt=\"none\",\n",
    "        use_local_encoder_transformer=True,\n",
    "        init_use_gaussian=True,\n",
    "        init_use_depth=\"current\",\n",
    "        attn_impl=\"sdpa\",\n",
    "        attn_bias_type=\"block_causal\",\n",
    "        alpha_depth=\"disabled\",\n",
    "        max_length=256,\n",
    "        local_attention_window_len=512,\n",
    "        downsampling_by_pooling=\"max\",\n",
    "    ),\n",
    "    data=DataloaderArgs(\n",
    "        root_dir=\".\",\n",
    "        sources={\"alpaca-cleaned\": 1.0},\n",
    "        dataset_files=[\"alpaca-cleaned/1.json\"],\n",
    "        batch_size=2,\n",
    "        prefetch_size=64,\n",
    "        seq_len=1024,\n",
    "        max_encoder_seq_length=3072,\n",
    "        load_async=False,\n",
    "        preprocess_dir=\"preprocess_dir\",\n",
    "        patcher_args=PatcherArgs(patching_mode=\"space\"),\n",
    "        tokenizer_args=TokenizerArgs(name=\"blt\"),\n",
    "    ),\n",
    "    profiling=ProfilerArgs(run=False),\n",
    "    checkpoint=CheckpointArgs(\n",
    "        path=\"train_checkpoints\",\n",
    "        dump=SaveEvery(\n",
    "            every=500,\n",
    "            keep=3\n",
    "        ),\n",
    "        eval=SaveEvery(\n",
    "            every=1000,\n",
    "            keep=-1\n",
    "        )\n",
    "    ),\n",
    "    logging=LoggingArgs(freq=10),\n",
    "    eval_on_gpus=1,\n",
    "    # env=None\n",
    ")\n",
    "\n",
    "args = train_args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5f9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4921e4aa",
   "metadata": {},
   "source": [
    "# Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7bab10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_encoder = create_local_encoder(args=args).to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09febbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_transformer = GlobalTransformer(args.model_copy(\n",
    "    deep=True,\n",
    "    update=dict(\n",
    "        dim=args.dim_global,\n",
    "        n_layers=args.n_layers_global,\n",
    "        n_heads=args.n_heads_global,\n",
    "        n_kv_heads=args.n_kv_heads_global,\n",
    "        local_attention_window_len=None,\n",
    "        dim_token_emb=get_global_dim_patch_emb(args),\n",
    "        dim_patch_emb=None,\n",
    "        cross_attn_encoder=False,\n",
    "        cross_attn_decoder=False,\n",
    "    ),\n",
    ")).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857f8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_decoder = create_local_decoder(args=args).to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477c17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e41ded35",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c39521e",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68bfc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input: torch.Size([4, 10240, 96])\n",
      "patch_lengths: torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_frames, channels, height, width = (4, 10, 1, 64, 64)\n",
    "frames = torch.randn(batch_size, num_frames, channels, height, width).to(\"cuda\")\n",
    "frames = frames.reshape(batch_size*num_frames, channels, height, width)\n",
    "encoder_input = local_encoder.image_encoder(frames)\n",
    "\n",
    "frame_height, frame_width = encoder_input.shape[2:]\n",
    "patch_ids = patch_ids_from_frames(\n",
    "    batch_size=batch_size,\n",
    "    num_frames=num_frames,\n",
    "    height=frame_height,\n",
    "    width=frame_width,\n",
    "    tile_height=args.vision.tile_height, \n",
    "    tile_width=args.vision.tile_width, \n",
    "    patch_size=args.patch_size,\n",
    "    device=encoder_input.device,\n",
    "    skip_from_start=1\n",
    ")\n",
    "\n",
    "encoder_input = encoder_input.permute(0, 2, 3, 1).reshape(batch_size, -1, local_encoder.dim)\n",
    "print(\"encoder_input:\", encoder_input.shape)\n",
    "\n",
    "N = encoder_input.shape[1]\n",
    "\n",
    "patch_lengths = torch.unique(patch_ids[0], return_counts=True)[1].unsqueeze(dim=0).to(\"cuda\")\n",
    "print(\"patch_lengths:\", patch_lengths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70971ba3",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10deafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_elements: 1024\n",
      "cross_attn_mask_enc: torch.Size([4, 1, 384, 10240])\n",
      "causal_mask_enc: torch.Size([10240, 10240])\n",
      "h_encoder: torch.Size([4, 10240, 96])\n",
      "h_cross: torch.Size([4, 384, 96])\n"
     ]
    }
   ],
   "source": [
    "frame_elements = frame_height * frame_width\n",
    "print(\"frame_elements:\", frame_elements)\n",
    "\n",
    "cross_attn_mask_enc = cross_attn_mask(\n",
    "    patch_ids,\n",
    "    patch_lengths,\n",
    "    N,\n",
    "    patches_as_queries=True,\n",
    "    cross_attn_k=args.cross_attn_k,\n",
    "    window=args.cross_attn_window_encoder,\n",
    "    block_mask=args.cross_attn_use_flex_attention,\n",
    ").to(\"cuda\")\n",
    "print(\"cross_attn_mask_enc:\", cross_attn_mask_enc.shape)\n",
    "\n",
    "causal_mask_enc = create_vision_causal_mask(\n",
    "    patch_ids.shape[1],\n",
    "    frame_elements,\n",
    "    args.attn_impl,\n",
    "    \"causal\"\n",
    ").to(\"cuda\")\n",
    "print(\"causal_mask_enc:\", causal_mask_enc.shape)\n",
    "\n",
    "(h_encoder, h_cross), cache_encoder = local_encoder(\n",
    "    frames=encoder_input,\n",
    "    mask=causal_mask_enc,\n",
    "    cross_mask=cross_attn_mask_enc,\n",
    "    patch_embeds=None,\n",
    "    num_patches=patch_lengths.shape[1],\n",
    "    patch_ids=patch_ids,\n",
    ")\n",
    "print(\"h_encoder:\", h_encoder.shape)\n",
    "print(\"h_cross:\", h_cross.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2202b",
   "metadata": {},
   "source": [
    "## Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8ddcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global transformer input shape: torch.Size([4, 192, 192]).\n",
      "latent_frame_elements: 64\n",
      "causal_mask_global: torch.Size([192, 192])\n",
      "Global transformer output shape: torch.Size([4, 192, 192]).\n"
     ]
    }
   ],
   "source": [
    "# Reshape h_cross\n",
    "h = h_cross.view(batch_size, patch_lengths.shape[1], -1)\n",
    "print(f\"Global transformer input shape: {h.shape}.\")\n",
    "\n",
    "tiles_y = frame_height // args.vision.tile_height\n",
    "tiles_x = frame_width  // args.vision.tile_width\n",
    "latent_frame_elements = tiles_y * tiles_x\n",
    "print(\"latent_frame_elements:\", latent_frame_elements)\n",
    "\n",
    "causal_mask_global = create_vision_causal_mask(\n",
    "    h.shape[1],\n",
    "    latent_frame_elements,\n",
    "    args.attn_impl,\n",
    "    \"causal\"\n",
    ").to(\"cuda\")\n",
    "print(\"causal_mask_global:\", causal_mask_global.shape)\n",
    "\n",
    "h, _ = global_transformer(\n",
    "    embeds=h,\n",
    "    mask=causal_mask_global\n",
    ")\n",
    "print(f\"Global transformer output shape: {h.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1b2da",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a5b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder embeddings `dec_embeds` shape: torch.Size([4, 10240, 96]).\n",
      "Decoder patch IDs shape: torch.Size([4, 10240]).\n",
      "cross_attn_mask_dec: torch.Size([4, 1, 10240, 384])\n",
      "causal_mask_dec: torch.Size([10240, 10240])\n",
      "Decoder output shape: torch.Size([4, 10240, 96])\n"
     ]
    }
   ],
   "source": [
    "# Unpatching\n",
    "dec_embeds = h_encoder\n",
    "print(f\"Decoder embeddings `dec_embeds` shape: {dec_embeds.shape}.\")\n",
    "\n",
    "# Generate decoder patch IDs\n",
    "decoder_patch_ids = patch_ids_from_frames(\n",
    "    batch_size=batch_size,\n",
    "    num_frames=num_frames,\n",
    "    height=frame_height,\n",
    "    width=frame_width,\n",
    "    tile_height=args.vision.tile_height, \n",
    "    tile_width=args.vision.tile_width, \n",
    "    patch_size=args.patch_size,\n",
    "    device=encoder_input.device,\n",
    "    skip_from_end=1\n",
    ")\n",
    "decoder_patch_lengths = torch.unique(decoder_patch_ids[0], return_counts=True)[1].unsqueeze(dim=0).to(\"cuda\")\n",
    "print(f\"Decoder patch IDs shape: {decoder_patch_ids.shape}.\")\n",
    "\n",
    "# Cross-attention decoder\n",
    "cross_attn_mask_dec = cross_attn_mask(\n",
    "    decoder_patch_ids,\n",
    "    decoder_patch_lengths,\n",
    "    N,\n",
    "    patches_as_queries=False,\n",
    "    cross_attn_k=args.cross_attn_k,\n",
    "    window=args.cross_attn_window_decoder,\n",
    "    block_mask=args.cross_attn_use_flex_attention,\n",
    ").to(\"cuda\")\n",
    "print(\"cross_attn_mask_dec:\", cross_attn_mask_dec.shape)\n",
    "\n",
    "causal_mask_dec = create_vision_causal_mask(\n",
    "    decoder_patch_ids.shape[1],\n",
    "    frame_elements,\n",
    "    args.attn_impl,\n",
    "    \"causal\"\n",
    ").to(\"cuda\")\n",
    "print(\"causal_mask_dec:\", causal_mask_dec.shape)\n",
    "\n",
    "# Local decoder\n",
    "decoder_output, _ = local_decoder(\n",
    "    embeds=dec_embeds,\n",
    "    patch_embeds=h,\n",
    "    mask=causal_mask_dec,\n",
    "    cross_mask=cross_attn_mask_dec,\n",
    ")\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db402d7",
   "metadata": {},
   "source": [
    "## Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b815ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_frames: torch.Size([40, 96, 32, 32])\n",
      "frames_features: torch.Size([4, 40960, 96])\n",
      "logits: torch.Size([4, 40960, 256])\n"
     ]
    }
   ],
   "source": [
    "latent_frames = decoder_output.reshape(batch_size*num_frames, frame_height, frame_width, local_decoder.dim).permute(0, 3, 1, 2)\n",
    "print(\"latent_frames:\", latent_frames.shape)\n",
    "\n",
    "frames_features = local_decoder.image_decoder(latent_frames)\n",
    "frames_features = frames_features.permute(0, 2, 3, 1).reshape(batch_size, -1, local_decoder.dim)\n",
    "print(\"frames_features:\", frames_features.shape)\n",
    "\n",
    "logits = local_decoder.head(frames_features).float()\n",
    "print(\"logits:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d183d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
