{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ac1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fd30f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0614 00:20:49.846000 21176 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "c:\\Users\\leoni\\Documents\\projects\\blt\\new_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "# from bytelatent.distributed import DistributedArgs, setup_torch_distributed\n",
    "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
    "from bytelatent.generate_blt import generate_nocache\n",
    "from bytelatent.model.blt import (\n",
    "    ByteLatentTransformer, \n",
    "    patch_ids_from_lengths,\n",
    "    get_blt_input,\n",
    "    compute_hash_embeddings,\n",
    "    decoder_patch_ids_from_lengths,\n",
    "    cross_attn_mask\n",
    ")\n",
    "from bytelatent.model.utils import downsample\n",
    "from bytelatent.distributed import (\n",
    "    DistributedArgs,\n",
    "    dist_max,\n",
    "    dist_min,\n",
    "    dist_sum,\n",
    "    get_device_mesh,\n",
    "    setup_torch_distributed,\n",
    ")\n",
    "from bytelatent.tokenizers.blt_tokenizer import BltTokenizer\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba3a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed_args = DistributedArgs()\n",
    "# distributed_args.configure_world()\n",
    "# if not torch.distributed.is_initialized():\n",
    "#     setup_torch_distributed(distributed_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ff8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(input_tokens: list[list[int]] | None) -> int:\n",
    "    # reduce max length prompt over all processes to have an equal number of call on each process with fsdp\n",
    "    if input_tokens is None:\n",
    "        max_length = 0\n",
    "    else:\n",
    "        max_length = max([len(t) for t in input_tokens])\n",
    "    if torch.distributed.is_initialized():\n",
    "        max_length = int(dist_max(max_length))\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def get_min_length(input_tokens: list[list[int]] | None) -> int:\n",
    "    # reduce min length prompt over all processes to have an equal number of call on each process with fsdp\n",
    "    if input_tokens is None:\n",
    "        # TODO: Double check this change from int(1e9) is correct\n",
    "        min_length = 0\n",
    "    else:\n",
    "        min_length = min([len(t) for t in input_tokens])\n",
    "    if torch.distributed.is_initialized():\n",
    "        min_length = int(dist_min(min_length))\n",
    "    return min_length\n",
    "\n",
    "\n",
    "def get_generation_range(\n",
    "    prompt_tokens: list[list[int]] | None, max_gen_len: int\n",
    ") -> tuple[int, int]:\n",
    "    batch_min_prompt_length = get_min_length(prompt_tokens)\n",
    "    batch_max_prompt_length = get_max_length(prompt_tokens)\n",
    "    return batch_min_prompt_length, batch_max_prompt_length + max_gen_len\n",
    "\n",
    "\n",
    "def sample_top_k(probs, k):\n",
    "    topk_value, _ = torch.topk(probs, k)  # batch_sz x topk\n",
    "    min_value_top_k = topk_value[:, [-1]]\n",
    "    probs[probs < min_value_top_k] = 0.0\n",
    "    probs.div_(probs.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m bytelatent.train config=bytelatent/configs/debug.yaml\n",
    "# python -m bytelatent.checkpoint consolidate train_checkpoints/0000018000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f32bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLT model: blt_1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Update checkpoint to load attn and sliding window args from checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entropy model and patcher\n"
     ]
    }
   ],
   "source": [
    "model_name = \"blt_1b\"\n",
    "\n",
    "print(f\"Loading BLT model: {model_name}\")\n",
    "model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(f\"train_checkpoints/0000018000/consolidated\")\n",
    "# model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(f\"hf-weights/{model_name}\")\n",
    "assert isinstance(model, ByteLatentTransformer)\n",
    "assert isinstance(tokenizer, BltTokenizer)\n",
    "patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
    "patcher_args.realtime_patching = True\n",
    "\n",
    "print(\"Loading entropy model and patcher\")\n",
    "patcher_args.entropy_model_checkpoint_dir = \"hf-weights/entropy_model\"\n",
    "patcher = patcher_args.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69a0cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"### Instruction:\\nGive thre\"\n",
    "prompt = \"### Instruction:\\nCreate a sentence using the following words: \\\"apple, banana, pencil.\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "015e8a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  6,  6,  2, 23]]), None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patcher.patch(torch.tensor([tokenizer.encode(\"hello world l ksjhlkjshdlkjhsdlkjsdh\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8104595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [prompt]\n",
    "\n",
    "# max_prompt_len: int = 256\n",
    "# max_gen_len: int = 5\n",
    "# use_sampling: bool = False\n",
    "# temp: float = 1.0\n",
    "# top_k: int = 0\n",
    "# top_p: float = 0.0\n",
    "# remove_prompts: bool = True\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# prompt_tokens = [tokenizer.encode(t, add_eos=False) for t in prompts]\n",
    "# n_truncated_prompts = sum([max_prompt_len < len(t) for t in prompt_tokens])\n",
    "# total_truncated_prompts = dist_sum(n_truncated_prompts)\n",
    "\n",
    "# # Truncation\n",
    "# prompt_tokens = [\n",
    "#     t if len(t) < max_prompt_len else t[len(t) - max_prompt_len :]\n",
    "#     for t in prompt_tokens\n",
    "# ]\n",
    "\n",
    "# if total_truncated_prompts > 0:\n",
    "#     logger.info(\n",
    "#         f\"There are {total_truncated_prompts} prompts that are truncated on the left, \"\n",
    "#         f\"length greater than max_prompt_len = {max_prompt_len}, \"\n",
    "#         f\"maximum prompt length = {get_max_length(prompt_tokens)} across all gpus.\"\n",
    "#     )\n",
    "\n",
    "# start_pos, end_pos = get_generation_range(prompt_tokens, max_gen_len)\n",
    "# batch_size = len(prompt_tokens)\n",
    "# tokens = torch.full((batch_size, end_pos), tokenizer.pad_id).cuda().long()\n",
    "\n",
    "# # Copy inputs to tensor for generated tokens\n",
    "# for i, row_tokens in enumerate(prompt_tokens):\n",
    "#     tokens[i, : len(row_tokens)] = torch.tensor(row_tokens).long()\n",
    "# input_text_mask = tokens != tokenizer.pad_id\n",
    "\n",
    "# for i, curr_pos in enumerate(range(start_pos, end_pos)):\n",
    "#     current_tokens = tokens[:, :curr_pos]\n",
    "#     patch_lengths, _ = patcher.patch(current_tokens, include_next_token=True)\n",
    "#     logits = model(current_tokens, patch_lengths=patch_lengths)[:, -1]\n",
    "\n",
    "#     if use_sampling:\n",
    "#         probs = torch.softmax(logits / temp, dim=-1)\n",
    "#         if top_p > 0.0:\n",
    "#             next_token = sample_top_p(probs, top_p)\n",
    "#         elif top_k > 0:\n",
    "#             next_token = sample_top_k(probs, top_k)\n",
    "#         else:\n",
    "#             next_token = torch.multinomial(probs, num_samples=1)\n",
    "#     else:\n",
    "#         next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "#     next_token = torch.where(\n",
    "#         input_text_mask[:, curr_pos], tokens[:, curr_pos], next_token\n",
    "#     )\n",
    "#     tokens[:, curr_pos] = next_token\n",
    "\n",
    "# if remove_prompts:\n",
    "#     generated_tokens = [\n",
    "#         t[len(prompt_tokens[i]) : len(prompt_tokens[i]) + max_gen_len].tolist()\n",
    "#         for i, t in enumerate(tokens)\n",
    "#     ]\n",
    "# else:\n",
    "#     generated_tokens = [\n",
    "#         t[: len(prompt_tokens[i]) + max_gen_len].tolist()\n",
    "#         for i, t in enumerate(tokens)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18b765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0330ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1, sequence length: 87\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 87])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 87, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.2363,  1.8984, -0.9062,  ...,  1.7422,  0.3496, -0.2236],\n",
      "        [-0.0059,  0.3789,  1.2578,  ...,  0.0835, -0.9766,  0.2637],\n",
      "        [-0.2158, -0.0518,  0.2812,  ...,  0.0176,  1.3672, -0.8945]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [   91.0000,   -14.0000,   -82.0000,  ...,   240.0000,\n",
      "            -69.0000,   -85.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ -1224.,  -3808.,   2080.,  ...,    912.,   -580.,  -2992.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 87, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.2363,  1.8984, -0.9062,  ...,  1.7422,  0.3496, -0.2236],\n",
      "        [-0.0059,  0.3789,  1.2578,  ...,  0.0835, -0.9766,  0.2637],\n",
      "        [-0.2158, -0.0518,  0.2812,  ...,  0.0176,  1.3672, -0.8945]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 87]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 12]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 87, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-3.3125, -2.3750,  3.9219, -3.3594, -3.2656, -3.3438, -3.2969, -3.3594,\n",
      "         -3.3125, -3.2188, -3.4219, -3.3906, -3.3125, -2.2656, 10.5000, -3.3906,\n",
      "         -3.3594, -3.1562, -3.4062, -3.3906, -3.3281, -3.2031, -3.2500, -3.2969,\n",
      "         -3.2500, -3.3438, -3.3125, -3.3281, -3.2500, -3.3281, -3.2969, -3.3125,\n",
      "         -3.2969, -3.3438, -3.3594, -3.3750,  6.3125,  0.2734,  1.4453,  2.3594,\n",
      "         -0.3555, -0.8047, -1.4453, -0.9531, -0.9609,  3.9219, -1.5625, -0.2021,\n",
      "          5.0938,  0.0913,  7.5312, -0.2393,  1.0547,  0.2139, -0.1641, -1.2031,\n",
      "         -0.6836,  0.4160, -0.5195,  0.3047,  1.0078,  1.0781,  2.4219,  1.7578,\n",
      "         -1.5547, -0.2070,  1.7188,  0.5508, -2.0781,  1.5312,  0.6055,  0.7852,\n",
      "          0.2129, -0.1260,  0.6172, -0.3105,  0.7148,  0.6719, -0.5234, -0.8594,\n",
      "         -1.2109, -0.0728, -0.8242,  1.5000, -0.0393, -0.3750,  0.2051,  1.5312,\n",
      "          0.4141, -1.0625, -0.2119,  0.1157, -1.0859, -0.3887, -0.4395, -2.2812,\n",
      "         -2.4844,  0.5156, -0.5820, -0.9141, -1.1562, -0.9023, -0.8984, -1.5156,\n",
      "         -1.5156,  0.0874, -1.0859, -2.6562, -2.0156, -1.0938, -1.2109, -1.0859,\n",
      "         -2.5625, -1.1250, -1.7734, -1.1953, -0.8438, -1.6641, -1.5156,  0.1069,\n",
      "         -2.0312, -2.2812, -0.9297, -1.4141, -0.2119, -1.5781, -2.3750, -2.6406,\n",
      "         -0.5469, -1.6875, -2.8750, -3.3281, -0.7852, -2.0000, -2.0156, -2.5938,\n",
      "         -2.5156, -2.4375, -2.1094, -2.8438, -2.3438, -1.4844, -2.1250, -2.2656,\n",
      "         -1.6484, -1.9297, -1.9766, -2.1406, -2.0781, -2.2344, -1.8984, -1.4297,\n",
      "         -1.2891, -2.6719, -2.5156, -1.8203, -0.8945, -1.4375, -1.9375, -2.3750,\n",
      "         -0.4453,  0.2422, -2.9219, -0.8711, -1.9531, -2.1875, -2.1562, -2.4219,\n",
      "         -1.7891, -1.7188, -2.3594, -2.1875, -2.3125, -1.4922, -2.3438, -2.7500,\n",
      "         -2.6719, -2.7188, -2.5625, -2.0000, -0.6250, -1.9531, -1.5625, -1.7969,\n",
      "         -2.6406, -2.8438, -2.7344, -2.1094, -1.2031, -2.3906, -2.4531, -2.4062,\n",
      "         -2.0469, -2.1875, -2.6875, -2.4062, -3.3750, -3.3125,  0.2119, -1.7578,\n",
      "         -2.8281, -3.0938, -3.3281, -3.0469, -3.1719, -3.2188, -3.2188, -3.1562,\n",
      "         -2.9375, -3.3125, -2.4844, -2.0000, -2.7969, -2.7812, -3.2656, -3.3594,\n",
      "         -3.3750, -3.4219, -3.3438, -3.3281, -3.3125, -3.3281, -3.5000, -3.4062,\n",
      "         -3.2656, -3.4844, -3.3438, -3.3594, -3.1562, -3.2812,  0.0344, -2.5312,\n",
      "         -2.7031, -2.5938, -2.8281, -2.8281, -2.9062, -3.1562, -3.1875, -3.2812,\n",
      "         -3.2344, -3.2969, -3.2656, -2.0938, -1.1562, -3.3438, -3.2812, -3.2656,\n",
      "         -3.4219, -3.3594, -3.2188, -3.3125, -3.3125, -3.4062, -3.2812, -3.2812,\n",
      "         -3.2812, -3.4219, -3.1875, -3.4062]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 88\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 88])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 88, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0059,  0.3789,  1.2578,  ...,  0.0835, -0.9766,  0.2637],\n",
      "        [-0.2158, -0.0518,  0.2812,  ...,  0.0176,  1.3672, -0.8945],\n",
      "        [-0.8477, -0.7656,  1.6484,  ..., -1.1406, -1.2656, -0.6602]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [  -48.5000,  -248.0000,   136.0000,  ...,   824.0000,\n",
      "             49.7500,   -70.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  2816.,  -4704.,  -8192.,  ...,    195.,   3072., -14272.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 88, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0059,  0.3789,  1.2578,  ...,  0.0835, -0.9766,  0.2637],\n",
      "        [-0.2158, -0.0518,  0.2812,  ...,  0.0176,  1.3672, -0.8945],\n",
      "        [-0.8477, -0.7656,  1.6484,  ..., -1.1406, -1.2656, -0.6602]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 88]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 12]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 88, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.0000, -8.8125, -2.0156, -8.1250, -8.1250, -8.1250, -8.0625, -8.1250,\n",
      "         -8.1250, -8.0000, -8.1875, -8.1875, -8.1250, -5.2188, 11.0000, -8.1250,\n",
      "         -8.1875, -7.6562, -8.2500, -8.0625, -8.0000, -8.0625, -8.1250, -8.0625,\n",
      "         -8.0625, -8.0625, -8.1250, -8.0625, -8.0625, -8.0625, -8.0625, -8.0625,\n",
      "         -8.0000, -8.1250, -8.0625, -8.1875,  1.4766, -4.0625,  2.7500,  2.2812,\n",
      "         -3.6094, -2.8906, -3.3594, -1.5156, -1.2266, -2.4688, -2.7344, -1.2188,\n",
      "         -1.2734,  1.3750,  0.7617, -2.5312, -1.2891, -0.2051,  0.0732, -2.0938,\n",
      "         -2.5156, -1.0078, -2.5469, -2.1094, -1.0625, -2.0312, -0.5391, -3.1875,\n",
      "         -2.6719, -1.5938, -2.3125, -3.5156, -5.0625,  2.0625,  0.6641,  1.7266,\n",
      "          1.0312,  1.0156,  1.1875,  0.3691,  0.0566,  1.0234, -0.5547, -1.5547,\n",
      "          0.3105,  0.9570, -0.5547,  2.7344,  1.5781, -0.3496,  0.7656,  2.3750,\n",
      "          2.0781, -0.0261, -1.3359,  1.1250, -2.6406, -0.5352, -3.6094, -2.4844,\n",
      "         -4.9688, -3.8125, -3.0156, -2.7344, -1.5469,  0.0649, -0.6055, -1.2969,\n",
      "         -1.1328, -0.9727, -1.5781, -3.3906, -1.8984, -1.3672, -3.7344, -2.4219,\n",
      "         -2.0312, -1.3594, -1.6719, -0.0200, -0.7344, -2.6875, -2.3125, -0.6211,\n",
      "         -1.7422, -1.6406, -2.5938, -0.3184, -2.3906, -1.8438, -5.5625, -3.4844,\n",
      "         -3.0156, -3.2031, -6.9062, -8.1250, -4.2812, -5.7188, -5.5938, -6.6250,\n",
      "         -6.2500, -6.1562, -5.9062, -7.3125, -5.0625, -4.7812, -6.0000, -5.9688,\n",
      "         -5.2188, -6.0000, -5.9062, -5.1875, -6.0938, -6.4062, -4.9375, -4.6250,\n",
      "         -4.9062, -6.8125, -6.9375, -4.9688, -3.5312, -4.8125, -5.2812, -6.7188,\n",
      "         -2.5469, -3.5000, -7.3750, -5.0625, -5.8750, -5.9688, -6.0000, -6.5625,\n",
      "         -6.2500, -6.0312, -6.3438, -6.1250, -6.3125, -5.4688, -6.2500, -6.9375,\n",
      "         -6.9688, -6.8438, -6.5000, -6.4062, -3.6406, -5.0938, -5.4688, -5.8750,\n",
      "         -6.5312, -6.8125, -7.0625, -5.7188, -5.3750, -6.6562, -6.6562, -6.0938,\n",
      "         -5.0938, -5.1250, -6.4375, -6.5000, -8.0000, -8.1250, -2.7031, -4.3125,\n",
      "         -7.3750, -7.7500, -8.0625, -7.6250, -7.9688, -7.9688, -8.0625, -7.9062,\n",
      "         -7.4375, -8.1250, -6.3125, -5.5938, -7.0312, -7.0938, -8.1250, -8.1250,\n",
      "         -8.1250, -8.1250, -8.0625, -8.0625, -8.0625, -8.0625, -8.1875, -8.1250,\n",
      "         -8.0625, -8.1250, -8.0625, -8.0625, -7.9688, -8.0625,  0.5430, -6.4375,\n",
      "         -6.8750, -6.7812, -7.0000, -7.1875, -7.3438, -7.6250, -7.9062, -7.9688,\n",
      "         -7.9688, -8.0625, -8.0625, -5.9375, -4.2500, -8.1250, -8.0625, -8.0625,\n",
      "         -8.1250, -8.0625, -8.0625, -8.0625, -8.1250, -8.0625, -8.1250, -8.0625,\n",
      "         -8.0625, -8.1875, -8.0000, -8.1875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 89\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 89])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 89, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2158, -0.0518,  0.2812,  ...,  0.0176,  1.3672, -0.8945],\n",
      "        [-0.8477, -0.7656,  1.6484,  ..., -1.1406, -1.2656, -0.6602],\n",
      "        [-1.9141, -0.3477,  0.1582,  ..., -1.2344,  0.1934,  0.9844]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [  -82.0000,  -308.0000,   198.0000,  ...,   936.0000,\n",
      "            179.0000,   -86.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  5952.,  -2656., -15424.,  ...,   -350.,   5600., -19072.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 89, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2158, -0.0518,  0.2812,  ...,  0.0176,  1.3672, -0.8945],\n",
      "        [-0.8477, -0.7656,  1.6484,  ..., -1.1406, -1.2656, -0.6602],\n",
      "        [-1.9141, -0.3477,  0.1582,  ..., -1.2344,  0.1934,  0.9844]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 89]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 12]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 89, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.0312, -5.0625, -2.2031, -5.0000, -5.0000, -5.0000, -5.0000, -5.0625,\n",
      "         -5.0312, -4.9062, -5.0625, -5.0000, -5.0000, -4.2812,  3.1250, -5.0000,\n",
      "         -5.0312, -4.9688, -5.1250, -5.0312, -4.9688, -4.9688, -5.0312, -4.9375,\n",
      "         -4.9688, -5.0625, -5.0312, -5.0938, -4.9062, -5.0312, -5.0312, -5.0625,\n",
      "         -4.9375, -5.0312, -4.9688, -5.0938,  1.1016, -2.0469,  2.3906, 12.5000,\n",
      "         -1.9375, -1.2578, -2.0312, -0.8984, -0.5430, -1.0469, -0.1182, -0.4648,\n",
      "         -1.3281,  0.3848, -0.3516, -1.7891, -0.4199, -0.4219,  1.1406, -0.2393,\n",
      "         -1.2031,  0.1523, -2.3438, -0.6328,  0.2891, -0.9766, -2.0469, -2.8438,\n",
      "         -0.9922, -0.1182, -0.1328, -1.5156, -2.4062,  1.5469, -0.1582,  1.4141,\n",
      "          0.8477,  0.9453,  1.0312,  1.1016,  0.4102,  2.2188, -0.4355, -1.3203,\n",
      "         -0.3457,  0.2715, -0.0928,  1.9844,  1.0781,  0.2539,  0.8203,  1.5000,\n",
      "          2.6406, -0.6406, -1.3281,  0.5625, -0.8828,  0.7656, -2.7812, -1.2812,\n",
      "         -3.2969, -2.5312, -2.4062, -1.3672, -2.0938,  0.5820, -1.3125, -0.3242,\n",
      "         -0.0874, -0.8633,  0.4668, -1.7109, -1.3750,  0.9258, -1.9141, -1.0234,\n",
      "         -0.8945, -1.1172, -2.7031, -1.1484, -0.1138, -1.2031, -0.3066, -0.5898,\n",
      "         -2.4219, -1.2031, -2.3125, -1.2578,  0.1089, -0.8398, -3.7188, -1.6250,\n",
      "         -1.3594, -3.7031, -4.3750, -5.0312, -2.5625, -3.6719, -3.5469, -4.3750,\n",
      "         -4.0938, -4.1250, -3.8594, -4.3438, -4.0312, -2.6406, -3.8906, -3.8594,\n",
      "         -3.5000, -3.5781, -3.8906, -3.3906, -4.0000, -3.8750, -2.3906, -2.2812,\n",
      "         -3.2500, -4.2500, -4.1562, -2.8125, -2.4688, -3.6094, -3.2344, -4.1562,\n",
      "         -1.2812, -1.2031, -4.2812, -3.3281, -3.7188, -4.0625, -3.4688, -4.2500,\n",
      "         -3.6406, -3.4375, -3.4062, -3.9688, -4.0312, -3.9219, -3.7344, -4.4062,\n",
      "         -4.2500, -4.5312, -4.2812, -3.9062, -3.5000, -3.0000, -3.7969, -3.8125,\n",
      "         -4.4375, -4.3750, -4.5625, -3.4375, -3.0781, -4.2812, -4.1562, -3.8438,\n",
      "         -3.3906, -3.4844, -4.1562, -3.9688, -5.0312, -5.0000, -2.4844, -4.0312,\n",
      "         -4.4375, -4.7500, -5.0000, -4.6875, -4.8750, -4.8438, -4.9375, -4.9062,\n",
      "         -4.4688, -5.0000, -4.3125, -4.0312, -4.6875, -4.7188, -5.0000, -5.0625,\n",
      "         -5.0938, -5.0938, -5.0312, -5.0312, -5.0000, -5.0312, -5.0625, -5.0625,\n",
      "         -5.0312, -4.9688, -5.0312, -4.9062, -4.9062, -5.0000,  0.4160, -4.2812,\n",
      "         -4.2188, -4.3125, -4.4688, -4.4062, -4.4062, -4.7812, -4.8750, -4.9688,\n",
      "         -4.9375, -5.0625, -5.0625, -3.9375, -1.5312, -5.1250, -4.9375, -4.9688,\n",
      "         -5.0312, -4.9375, -5.0000, -5.0312, -5.0625, -5.0000, -5.0312, -4.9375,\n",
      "         -5.0625, -5.0938, -4.9375, -5.0312]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 90\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 90])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 90, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8477, -0.7656,  1.6484,  ..., -1.1406, -1.2656, -0.6602],\n",
      "        [-1.9141, -0.3477,  0.1582,  ..., -1.2344,  0.1934,  0.9844],\n",
      "        [-1.6328,  0.2930, -0.1895,  ..., -1.2188,  0.6914,  0.2930]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -106.0000,  -368.0000,   242.0000,  ...,  1008.0000,\n",
      "            181.0000,  -102.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  7040.,  -3120., -18176.,  ...,   -532.,   6592., -22272.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 90, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8477, -0.7656,  1.6484,  ..., -1.1406, -1.2656, -0.6602],\n",
      "        [-1.9141, -0.3477,  0.1582,  ..., -1.2344,  0.1934,  0.9844],\n",
      "        [-1.6328,  0.2930, -0.1895,  ..., -1.2188,  0.6914,  0.2930]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 90]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 12]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 90, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-3.8281e+00, -1.8750e+00, -1.0312e+00, -3.7031e+00, -3.6719e+00,\n",
      "         -3.6719e+00, -3.7188e+00, -3.7344e+00, -3.7031e+00, -3.5938e+00,\n",
      "         -3.8281e+00, -3.6562e+00, -3.6875e+00, -2.7812e+00,  1.6953e+00,\n",
      "         -3.6719e+00, -3.7656e+00, -3.5938e+00, -3.8750e+00, -3.7812e+00,\n",
      "         -3.6875e+00, -3.6562e+00, -3.7812e+00, -3.6094e+00, -3.7188e+00,\n",
      "         -3.6875e+00, -3.7656e+00, -3.8750e+00, -3.6406e+00, -3.7188e+00,\n",
      "         -3.7500e+00, -3.8281e+00, -3.7188e+00, -3.8125e+00, -3.7656e+00,\n",
      "         -3.7812e+00,  1.9688e+00,  2.6758e-01,  2.5938e+00,  1.4562e+01,\n",
      "         -1.0547e+00, -5.8984e-01, -9.2578e-01,  2.0020e-01,  1.0596e-01,\n",
      "         -4.1797e-01, -1.3750e+00, -4.7266e-01, -5.5078e-01,  5.8594e-01,\n",
      "          4.6289e-01, -1.7734e+00,  6.6016e-01, -5.0391e-01,  4.0430e-01,\n",
      "         -1.1484e+00, -2.1094e+00, -9.2578e-01, -2.3750e+00, -9.6484e-01,\n",
      "          1.1328e+00,  9.4238e-02, -1.4453e+00, -2.1250e+00, -1.3906e+00,\n",
      "         -2.6562e-01, -7.1411e-03, -1.7871e-01, -9.8047e-01, -2.7734e-01,\n",
      "         -8.8281e-01,  2.2095e-02, -4.3750e-01,  1.8359e-01, -3.6133e-02,\n",
      "          1.4258e-01, -6.2891e-01,  1.8164e-01, -5.8984e-01, -1.0781e+00,\n",
      "         -7.3828e-01, -1.1953e+00, -1.5938e+00, -9.4727e-02,  4.3945e-01,\n",
      "          3.9844e-01,  3.4180e-01,  3.7305e-01,  9.4922e-01, -1.5156e+00,\n",
      "         -1.4062e+00, -4.9609e-01,  3.0859e-01,  3.2227e-02, -8.7891e-01,\n",
      "         -5.1172e-01, -1.0625e+00, -2.1562e+00, -1.5469e+00, -5.5859e-01,\n",
      "         -1.6719e+00,  1.0781e+00,  8.4961e-02,  1.2598e-01,  6.6016e-01,\n",
      "         -3.2031e-01,  4.2773e-01, -4.2578e-01, -9.3262e-02,  1.8750e+00,\n",
      "         -6.7578e-01, -7.1094e-01,  1.9238e-01, -1.3672e+00, -1.5859e+00,\n",
      "         -2.1289e-01,  6.6797e-01,  7.5912e-04,  7.1484e-01,  1.5820e-01,\n",
      "         -2.2812e+00, -3.8672e-01, -4.6289e-01, -2.0938e+00,  8.2031e-01,\n",
      "          5.5859e-01, -2.1875e+00, -5.0781e-01, -2.2949e-01, -2.8125e+00,\n",
      "         -3.0312e+00, -3.7031e+00, -3.6328e-01, -2.3125e+00, -1.9844e+00,\n",
      "         -3.2500e+00, -2.6875e+00, -2.7812e+00, -2.4531e+00, -2.9531e+00,\n",
      "         -2.1094e+00, -8.6328e-01, -2.4219e+00, -2.3125e+00, -2.2031e+00,\n",
      "         -2.1094e+00, -2.6094e+00, -2.3438e+00, -2.6406e+00, -2.4688e+00,\n",
      "         -1.7812e+00, -1.0547e+00, -1.8359e+00, -2.8906e+00, -2.8281e+00,\n",
      "         -1.2734e+00, -9.5312e-01, -1.3594e+00, -1.6406e+00, -2.9531e+00,\n",
      "         -3.7305e-01,  5.8594e-01, -2.9375e+00, -1.7266e+00, -2.2969e+00,\n",
      "         -2.5938e+00, -2.1250e+00, -2.8125e+00, -2.0469e+00, -1.9062e+00,\n",
      "         -1.6953e+00, -2.6406e+00, -2.1562e+00, -1.6484e+00, -2.4688e+00,\n",
      "         -3.2812e+00, -2.9062e+00, -3.2031e+00, -3.1250e+00, -2.3438e+00,\n",
      "         -3.1719e+00, -1.6875e+00, -2.2656e+00, -2.1250e+00, -3.3125e+00,\n",
      "         -3.1719e+00, -2.8906e+00, -2.2500e+00, -1.7031e+00, -3.0000e+00,\n",
      "         -2.8750e+00, -2.7188e+00, -2.2031e+00, -2.8281e+00, -2.7969e+00,\n",
      "         -2.9531e+00, -3.7969e+00, -3.7656e+00, -1.3047e+00, -2.0938e+00,\n",
      "         -3.1250e+00, -3.2812e+00, -3.7344e+00, -3.4219e+00, -3.5625e+00,\n",
      "         -3.6094e+00, -3.6875e+00, -3.7500e+00, -3.0938e+00, -3.6406e+00,\n",
      "         -2.9688e+00, -2.5312e+00, -3.7188e+00, -3.6719e+00, -3.6250e+00,\n",
      "         -3.7656e+00, -3.8438e+00, -3.8906e+00, -3.7031e+00, -3.7812e+00,\n",
      "         -3.7188e+00, -3.7500e+00, -3.7188e+00, -3.7812e+00, -3.7031e+00,\n",
      "         -3.6406e+00, -3.7031e+00, -3.6406e+00, -3.5469e+00, -3.7188e+00,\n",
      "          7.1484e-01, -2.9219e+00, -2.8125e+00, -3.2188e+00, -3.1875e+00,\n",
      "         -3.1875e+00, -3.0781e+00, -3.4531e+00, -3.6875e+00, -3.7188e+00,\n",
      "         -3.6875e+00, -3.8438e+00, -3.7812e+00, -2.5312e+00,  3.2422e-01,\n",
      "         -3.9219e+00, -3.7188e+00, -3.6562e+00, -3.7344e+00, -3.6719e+00,\n",
      "         -3.7188e+00, -3.8281e+00, -3.7812e+00, -3.7656e+00, -3.7188e+00,\n",
      "         -3.6719e+00, -3.7812e+00, -3.7969e+00, -3.6562e+00, -3.7812e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 91\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 91])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 91, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.9141, -0.3477,  0.1582,  ..., -1.2344,  0.1934,  0.9844],\n",
      "        [-1.6328,  0.2930, -0.1895,  ..., -1.2188,  0.6914,  0.2930],\n",
      "        [-1.6094,  0.4570, -0.0234,  ..., -1.4688,  0.7617, -0.1387]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -110.0000,  -372.0000,   248.0000,  ...,  1016.0000,\n",
      "            181.0000,  -105.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  7136.,  -3184., -18432.,  ...,   -552.,   6656., -22528.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 91, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.9141, -0.3477,  0.1582,  ..., -1.2344,  0.1934,  0.9844],\n",
      "        [-1.6328,  0.2930, -0.1895,  ..., -1.2188,  0.6914,  0.2930],\n",
      "        [-1.6094,  0.4570, -0.0234,  ..., -1.4688,  0.7617, -0.1387]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 91]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 91, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-4.1562e+00, -1.5391e+00, -2.9492e-01, -4.0312e+00, -4.0312e+00,\n",
      "         -4.0312e+00, -4.0312e+00, -4.0312e+00, -4.0312e+00, -3.9375e+00,\n",
      "         -4.1875e+00, -4.0000e+00, -4.0312e+00, -2.5312e+00,  2.4219e+00,\n",
      "         -4.0625e+00, -4.1250e+00, -3.8750e+00, -4.2188e+00, -4.1250e+00,\n",
      "         -4.0312e+00, -4.0312e+00, -4.1250e+00, -3.9375e+00, -4.0625e+00,\n",
      "         -4.0312e+00, -4.0938e+00, -4.2188e+00, -4.0312e+00, -4.0625e+00,\n",
      "         -4.1250e+00, -4.1875e+00, -4.0625e+00, -4.1875e+00, -4.1250e+00,\n",
      "         -4.1562e+00,  2.6875e+00,  3.3398e-01,  2.7031e+00,  1.4438e+01,\n",
      "         -6.3672e-01, -7.4219e-01, -8.5156e-01,  2.8564e-02, -1.4587e-02,\n",
      "         -5.1953e-01, -1.9375e+00, -4.6094e-01, -4.4336e-01,  4.5898e-01,\n",
      "          7.1094e-01, -1.7109e+00,  8.6328e-01, -6.4453e-01,  2.0703e-01,\n",
      "         -1.6172e+00, -2.3594e+00, -1.0781e+00, -2.0469e+00, -7.8516e-01,\n",
      "          1.2812e+00,  8.6914e-02, -1.0156e+00, -1.7656e+00, -1.4375e+00,\n",
      "         -3.7109e-01, -4.2419e-03,  1.0400e-01, -8.5547e-01, -4.1016e-01,\n",
      "         -9.7266e-01, -3.9453e-01, -6.5625e-01,  1.7188e-01, -2.3926e-01,\n",
      "         -1.2988e-01, -4.9023e-01, -1.4258e-01, -5.9375e-01, -1.1719e+00,\n",
      "         -6.5234e-01, -1.3906e+00, -1.8750e+00, -1.0864e-02,  1.9434e-01,\n",
      "          5.6250e-01,  3.3008e-01,  4.5898e-01,  7.6172e-01, -1.4688e+00,\n",
      "         -1.4219e+00, -3.8086e-01,  6.4844e-01,  1.4355e-01, -4.7461e-01,\n",
      "         -4.1992e-01, -8.2031e-01, -2.2656e+00, -1.6328e+00, -3.7891e-01,\n",
      "         -1.4453e+00,  1.6797e+00, -1.2207e-02, -8.8867e-02,  6.8750e-01,\n",
      "         -4.9609e-01,  1.5820e-01, -4.9609e-01,  6.5430e-02,  1.2578e+00,\n",
      "         -7.4609e-01, -1.1250e+00,  6.6406e-02, -1.5781e+00, -1.4844e+00,\n",
      "          6.7871e-02,  5.7031e-01,  4.3750e-01,  7.0312e-01,  5.0000e-01,\n",
      "         -2.4219e+00, -1.7383e-01, -2.2754e-01, -2.2344e+00,  1.2109e+00,\n",
      "          1.4531e+00, -1.8594e+00, -2.8125e-01,  2.1680e-01, -2.7344e+00,\n",
      "         -3.2812e+00, -4.0312e+00, -4.9438e-03, -2.2031e+00, -1.8984e+00,\n",
      "         -3.4531e+00, -2.8438e+00, -2.9219e+00, -2.4219e+00, -3.2188e+00,\n",
      "         -1.9062e+00, -9.4141e-01, -2.4531e+00, -2.2812e+00, -2.2031e+00,\n",
      "         -2.1250e+00, -2.7188e+00, -2.6094e+00, -2.7812e+00, -2.5938e+00,\n",
      "         -1.9375e+00, -1.0156e+00, -1.7500e+00, -3.0469e+00, -3.0469e+00,\n",
      "         -1.3516e+00, -7.8125e-01, -1.0469e+00, -1.5703e+00, -3.0938e+00,\n",
      "         -3.8477e-01,  6.3281e-01, -3.2188e+00, -1.8125e+00, -2.2656e+00,\n",
      "         -2.6094e+00, -2.3438e+00, -2.9688e+00, -2.0938e+00, -2.0000e+00,\n",
      "         -1.7422e+00, -2.7812e+00, -2.1719e+00, -1.2578e+00, -2.6562e+00,\n",
      "         -3.5000e+00, -3.0781e+00, -3.4219e+00, -3.3281e+00, -2.4062e+00,\n",
      "         -3.1875e+00, -1.7188e+00, -2.3125e+00, -2.1562e+00, -3.5000e+00,\n",
      "         -3.3750e+00, -3.0469e+00, -2.4062e+00, -1.6797e+00, -3.2656e+00,\n",
      "         -3.0469e+00, -2.8906e+00, -2.2500e+00, -3.0000e+00, -3.0000e+00,\n",
      "         -3.1875e+00, -4.1250e+00, -4.1250e+00, -9.5312e-01, -1.7188e+00,\n",
      "         -3.3594e+00, -3.5000e+00, -4.0938e+00, -3.7188e+00, -3.9219e+00,\n",
      "         -3.9062e+00, -4.0000e+00, -4.0312e+00, -3.4062e+00, -3.9375e+00,\n",
      "         -3.0781e+00, -2.5625e+00, -4.0000e+00, -3.9375e+00, -3.9531e+00,\n",
      "         -4.0625e+00, -4.1875e+00, -4.2188e+00, -4.0312e+00, -4.1250e+00,\n",
      "         -4.0625e+00, -4.0625e+00, -4.0312e+00, -4.1250e+00, -4.0625e+00,\n",
      "         -4.0000e+00, -4.0312e+00, -3.9844e+00, -3.8750e+00, -4.0625e+00,\n",
      "          5.2344e-01, -2.9688e+00, -3.0312e+00, -3.3281e+00, -3.3438e+00,\n",
      "         -3.4062e+00, -3.3438e+00, -3.7188e+00, -4.0625e+00, -4.0938e+00,\n",
      "         -4.0625e+00, -4.1562e+00, -4.0938e+00, -2.5625e+00,  6.2109e-01,\n",
      "         -4.2812e+00, -4.0625e+00, -3.9844e+00, -4.0938e+00, -4.0312e+00,\n",
      "         -4.0625e+00, -4.1562e+00, -4.1250e+00, -4.1250e+00, -4.0625e+00,\n",
      "         -3.9844e+00, -4.1250e+00, -4.1562e+00, -4.0000e+00, -4.1250e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 92\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  6]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 92])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 92, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6328,  0.2930, -0.1895,  ..., -1.2188,  0.6914,  0.2930],\n",
      "        [-1.6094,  0.4570, -0.0234,  ..., -1.4688,  0.7617, -0.1387],\n",
      "        [-2.9062,  0.0303,  0.4238,  ..., -0.6797,  0.9180, -1.6719]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -112.0000,  -392.0000,   280.0000,  ...,  1048.0000,\n",
      "            193.0000,  -115.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  7488.,  -3424., -19328.,  ...,   -608.,   7008., -23680.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 92, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6328,  0.2930, -0.1895,  ..., -1.2188,  0.6914,  0.2930],\n",
      "        [-1.6094,  0.4570, -0.0234,  ..., -1.4688,  0.7617, -0.1387],\n",
      "        [-2.9062,  0.0303,  0.4238,  ..., -0.6797,  0.9180, -1.6719]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 92]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 92, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.1875,  -6.4375,   0.7305, -10.1875, -10.3125, -10.2500, -10.3125,\n",
      "         -10.1250, -10.2500, -10.1875, -10.3125, -10.2500, -10.2500,  -6.0625,\n",
      "           4.4688, -10.2500, -10.2500,  -9.8750, -10.2500, -10.3125, -10.1250,\n",
      "         -10.3125, -10.3125, -10.1250, -10.1875, -10.2500, -10.2500, -10.3750,\n",
      "         -10.3125, -10.2500, -10.3125, -10.4375, -10.2500, -10.3125, -10.2500,\n",
      "         -10.3750,  11.0625,  -1.8125,  -0.4824,   1.2656,  -3.4062,  -5.5000,\n",
      "          -5.4375,  -1.9922,  -1.1406,  -1.6172,  -3.6406,  -3.8906,   1.8281,\n",
      "          -1.5625,   2.9062,  -1.4766,  -2.1406,  -3.4062,  -2.8438,  -3.6406,\n",
      "          -4.1562,  -3.4062,  -3.5625,  -3.0469,  -2.7344,  -3.5469,   1.1406,\n",
      "          -2.4062,  -4.3438,  -2.9062,  -4.2500,  -1.8750,  -5.5938,  -3.4688,\n",
      "          -3.5469,  -3.1562,  -4.0312,  -2.2969,  -3.2656,  -2.4219,  -3.1719,\n",
      "          -2.4219,  -3.8125,  -4.6250,  -3.8438,  -3.5312,  -3.8125,  -2.4219,\n",
      "          -2.5312,  -3.2500,  -1.5312,  -1.8906,  -1.7578,  -3.9375,  -4.8750,\n",
      "          -2.2031,  -4.1250,  -3.4219,  -3.6875,  -4.3750,  -5.7500,  -3.9062,\n",
      "          -4.6250,  -2.1094,  -4.2500,  -0.9023,  -1.7812,  -0.8789,  -1.0078,\n",
      "          -0.6523,  -0.7969,  -0.5195,  -2.7969,  -0.3262,  -3.1250,  -3.3438,\n",
      "          -1.3438,  -2.0469,   0.4648,  -0.0845,  -0.9297,  -2.0781,  -0.1055,\n",
      "           1.7891,  -0.9766,  -1.6875,  -1.8203,  -2.4062,   0.1660,  -0.1963,\n",
      "          -4.0938,  -4.4688,  -2.1562,  -4.3438,  -9.0000, -10.2500,  -4.6562,\n",
      "          -7.1875,  -7.0000,  -8.5000,  -8.2500,  -8.2500,  -7.3750,  -9.1250,\n",
      "          -5.7812,  -5.9375,  -7.4688,  -7.0312,  -6.7188,  -6.8750,  -7.7500,\n",
      "          -7.6875,  -7.8750,  -7.9688,  -6.3438,  -5.7500,  -6.1562,  -8.3750,\n",
      "          -8.8125,  -6.3438,  -5.8125,  -5.8125,  -6.0625,  -8.6250,  -5.6562,\n",
      "          -5.0625,  -9.3125,  -6.9062,  -7.6562,  -7.3438,  -7.4062,  -8.4375,\n",
      "          -7.4375,  -7.1562,  -7.0625,  -8.0000,  -7.2812,  -4.6250,  -8.0000,\n",
      "          -9.0625,  -8.5625,  -8.7500,  -8.8125,  -7.7812,  -7.7812,  -6.7500,\n",
      "          -6.3438,  -6.9062,  -8.6250,  -8.8125,  -8.4375,  -7.7188,  -6.9375,\n",
      "          -8.8125,  -8.6250,  -8.3125,  -6.9062,  -7.3438,  -8.5000,  -8.5000,\n",
      "         -10.2500, -10.2500,  -4.5312,  -4.1875,  -9.3125,  -9.5000, -10.2500,\n",
      "          -9.8125, -10.1250,  -9.9375, -10.2500,  -9.9375,  -9.4375, -10.1875,\n",
      "          -8.3750,  -7.4375,  -9.6875,  -9.5000, -10.1875, -10.1875, -10.2500,\n",
      "         -10.3125, -10.1875, -10.1875, -10.3125, -10.2500, -10.1250, -10.2500,\n",
      "         -10.2500, -10.2500, -10.2500, -10.1875, -10.0625, -10.1875,  -2.8438,\n",
      "          -8.0000,  -9.0000,  -8.7500,  -8.8125,  -9.3125,  -9.3750,  -9.7500,\n",
      "         -10.1875, -10.2500, -10.1875, -10.1875, -10.1250,  -7.4375,  -4.7500,\n",
      "         -10.3750, -10.1875, -10.1875, -10.3125, -10.3125, -10.2500, -10.2500,\n",
      "         -10.3125, -10.1875, -10.3750, -10.2500, -10.2500, -10.3125, -10.1875,\n",
      "         -10.3125]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 93\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  7]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 93])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 93, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6094,  0.4570, -0.0234,  ..., -1.4688,  0.7617, -0.1387],\n",
      "        [-2.9062,  0.0303,  0.4238,  ..., -0.6797,  0.9180, -1.6719],\n",
      "        [-0.1055,  0.3828,  0.1914,  ...,  0.3320,  1.1484, -1.8906]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -121.0000,  -424.0000,   300.0000,  ...,  1072.0000,\n",
      "            213.0000,  -134.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  8000.,  -3504., -20736.,  ...,   -724.,   7456., -25088.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 93, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6094,  0.4570, -0.0234,  ..., -1.4688,  0.7617, -0.1387],\n",
      "        [-2.9062,  0.0303,  0.4238,  ..., -0.6797,  0.9180, -1.6719],\n",
      "        [-0.1055,  0.3828,  0.1914,  ...,  0.3320,  1.1484, -1.8906]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 93]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 93, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.5625e+00, -6.5312e+00, -2.4902e-01, -6.6250e+00, -6.5312e+00,\n",
      "         -6.6875e+00, -6.5938e+00, -6.5938e+00, -6.5312e+00, -6.6250e+00,\n",
      "         -6.6250e+00, -6.6875e+00, -6.6250e+00, -2.0312e+00,  1.2500e+00,\n",
      "         -6.5312e+00, -6.7188e+00, -6.2188e+00, -6.7188e+00, -6.5938e+00,\n",
      "         -6.5000e+00, -6.6875e+00, -6.6250e+00, -6.5938e+00, -6.4688e+00,\n",
      "         -6.5938e+00, -6.6562e+00, -6.7500e+00, -6.6250e+00, -6.5938e+00,\n",
      "         -6.5312e+00, -6.5625e+00, -6.5938e+00, -6.5312e+00, -6.5938e+00,\n",
      "         -6.5625e+00,  7.9297e-01, -1.3047e+00, -1.2031e+00,  5.4297e-01,\n",
      "         -2.9531e+00, -1.6953e+00, -2.0312e-01, -1.1094e+00,  1.2988e-01,\n",
      "         -1.8203e+00, -1.2266e+00,  1.1328e+00,  8.4375e-01, -4.2969e-01,\n",
      "          1.9531e+00,  6.8750e-01, -1.3281e+00, -9.8438e-01, -5.4297e-01,\n",
      "         -1.2969e+00, -6.9922e-01, -1.5938e+00, -1.1562e+00, -1.2734e+00,\n",
      "         -1.5703e+00, -1.8672e+00,  1.2500e-01,  1.7383e-01, -1.0469e+00,\n",
      "         -1.3906e+00, -1.6328e+00, -1.8984e+00, -2.2656e+00,  6.7188e-01,\n",
      "          1.5564e-02,  1.4297e+00,  1.3906e+00,  1.3438e+00,  1.3125e+00,\n",
      "          1.3750e+00,  1.6016e+00,  8.3750e+00,  4.0039e-01,  9.8047e-01,\n",
      "          1.3359e+00,  1.3965e-01,  1.8516e+00,  2.1875e+00,  1.5781e+00,\n",
      "         -4.8047e-01,  1.2125e+01,  2.0156e+00,  1.7109e+00,  1.0000e+00,\n",
      "          1.2891e+00,  1.4219e+00, -2.3047e-01,  1.4941e-01, -1.1328e+00,\n",
      "         -8.0859e-01, -1.6406e+00, -8.9844e-01, -1.8984e+00, -1.0938e+00,\n",
      "         -2.0312e+00, -4.4531e-01, -2.4844e+00, -1.9922e+00, -5.9766e-01,\n",
      "         -2.4531e+00, -5.7422e-01, -8.7109e-01, -1.9453e+00, -6.9922e-01,\n",
      "         -5.7031e-01, -1.3516e+00,  4.2383e-01, -2.7188e+00, -2.7734e-01,\n",
      "         -8.3008e-02, -1.8984e+00, -2.1406e+00,  2.0625e+00,  2.1582e-01,\n",
      "         -1.0312e+00, -3.9844e-01, -1.8438e+00, -1.8750e+00, -1.7090e-02,\n",
      "         -9.2578e-01, -1.3379e-01, -1.0156e+00,  1.1597e-02, -1.0312e+00,\n",
      "         -5.3438e+00, -6.5625e+00, -1.1484e+00, -3.5938e+00, -3.4531e+00,\n",
      "         -4.7500e+00, -4.8438e+00, -4.6562e+00, -4.0000e+00, -5.4062e+00,\n",
      "         -2.6094e+00, -2.8125e+00, -4.0312e+00, -3.4531e+00, -3.0312e+00,\n",
      "         -3.3438e+00, -4.3438e+00, -4.0938e+00, -4.2500e+00, -4.9062e+00,\n",
      "         -2.4844e+00, -1.2969e+00, -1.7891e+00, -5.0312e+00, -5.1250e+00,\n",
      "         -2.9844e+00, -2.3594e+00, -2.5000e+00, -2.5156e+00, -5.0625e+00,\n",
      "         -2.1094e+00, -1.5234e+00, -5.7812e+00, -4.8438e+00, -3.4531e+00,\n",
      "         -3.4844e+00, -3.6719e+00, -4.8438e+00, -4.1250e+00, -3.8281e+00,\n",
      "         -3.7500e+00, -4.1875e+00, -4.2188e+00, -2.3125e+00, -4.6250e+00,\n",
      "         -5.3438e+00, -5.0312e+00, -5.2812e+00, -5.2500e+00, -4.0938e+00,\n",
      "         -2.7500e+00, -3.4531e+00, -2.5156e+00, -2.9219e+00, -4.8125e+00,\n",
      "         -5.2812e+00, -5.2500e+00, -3.9844e+00, -3.6719e+00, -5.1562e+00,\n",
      "         -5.1250e+00, -4.7812e+00, -3.1562e+00, -3.2031e+00, -4.7500e+00,\n",
      "         -4.7188e+00, -6.5938e+00, -6.5938e+00, -1.6719e+00, -1.8203e+00,\n",
      "         -5.6875e+00, -5.8125e+00, -6.6562e+00, -6.0000e+00, -6.4375e+00,\n",
      "         -5.9688e+00, -6.5000e+00, -6.2500e+00, -5.6562e+00, -6.5938e+00,\n",
      "         -4.0625e+00, -2.9844e+00, -5.7500e+00, -5.7500e+00, -6.6250e+00,\n",
      "         -6.5000e+00, -6.6875e+00, -6.5625e+00, -6.6562e+00, -6.5000e+00,\n",
      "         -6.5625e+00, -6.5000e+00, -6.5938e+00, -6.5000e+00, -6.5625e+00,\n",
      "         -6.7500e+00, -6.5938e+00, -6.5312e+00, -6.3750e+00, -6.5312e+00,\n",
      "          1.0742e-02, -4.2812e+00, -5.0625e+00, -5.0625e+00, -5.0312e+00,\n",
      "         -5.5312e+00, -5.6562e+00, -6.0625e+00, -6.5000e+00, -6.4688e+00,\n",
      "         -6.4062e+00, -6.5000e+00, -6.5625e+00, -3.9531e+00, -1.3672e+00,\n",
      "         -6.6562e+00, -6.5938e+00, -6.6562e+00, -6.5312e+00, -6.6250e+00,\n",
      "         -6.5312e+00, -6.5312e+00, -6.6562e+00, -6.5312e+00, -6.6562e+00,\n",
      "         -6.6562e+00, -6.5625e+00, -6.6250e+00, -6.5000e+00, -6.6250e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 94\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  8]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 94])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 94, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-2.9062,  0.0303,  0.4238,  ..., -0.6797,  0.9180, -1.6719],\n",
      "        [-0.1055,  0.3828,  0.1914,  ...,  0.3320,  1.1484, -1.8906],\n",
      "        [ 1.0078, -1.1172,  0.6484,  ..., -0.2520,  0.5781,  0.3594]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -128.0000,  -444.0000,   324.0000,  ...,  1104.0000,\n",
      "            225.0000,  -156.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  8320.,  -3600., -21632.,  ...,   -764.,   7776., -26112.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 94, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-2.9062,  0.0303,  0.4238,  ..., -0.6797,  0.9180, -1.6719],\n",
      "        [-0.1055,  0.3828,  0.1914,  ...,  0.3320,  1.1484, -1.8906],\n",
      "        [ 1.0078, -1.1172,  0.6484,  ..., -0.2520,  0.5781,  0.3594]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 94]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 94, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.1562, -4.9062, -0.5703, -5.1562, -5.1250, -5.1875, -5.1250, -5.1250,\n",
      "         -5.1562, -5.1875, -5.1562, -5.2500, -5.2188, -1.5703, -1.4141, -5.1562,\n",
      "         -5.2188, -4.9375, -5.1875, -5.2188, -5.0938, -5.1562, -5.1875, -5.1562,\n",
      "         -5.0625, -5.1875, -5.1562, -5.1875, -5.1875, -5.1562, -5.1562, -5.1250,\n",
      "         -5.0938, -5.0938, -5.1875, -5.1250, -1.0000, -1.0156, -1.0156, -0.8281,\n",
      "         -2.1875, -0.5859, -1.6953, -0.3965, -1.3438, -0.9766, -1.9688, -1.1719,\n",
      "         -1.1172, -0.9336, -1.0547, -1.5859, -1.3438, -2.2812, -1.0781, -1.1250,\n",
      "         -2.2969, -3.0625, -1.7109, -1.6250, -0.2246, -2.0000, -1.2734, -1.0312,\n",
      "         -3.1094, -0.0640, -0.7539, -1.9688, -1.7812, -1.7031, -2.4219, -1.9688,\n",
      "         -1.5312,  2.5000, -1.5625, -1.8438, -1.8047, -2.2031, -2.1875, -2.5938,\n",
      "         -2.4688, -2.4375, -0.9023, -2.0000, -0.7773, -1.1875, -1.3125, -1.3594,\n",
      "         -1.7656, -0.9805, -1.5781, -1.6562, -0.4277, -1.0469, -1.7969, -1.8359,\n",
      "         -1.8203, -2.3906, -1.1406, -0.8242, -2.1719,  0.8516, -0.7930, -1.0859,\n",
      "          0.6914, 13.3125, -2.3594, -1.1094, -0.3203,  0.5352, -2.1562, -2.0781,\n",
      "         -0.5547, -0.7188,  0.0801,  1.1016,  1.5625, -1.1172,  0.6094, -1.4141,\n",
      "         -1.9297,  2.7969, -0.3848, -0.7266,  0.8008,  1.8906,  0.9414, -1.7656,\n",
      "         -0.8438, -1.5703, -4.0625, -5.1875, -1.8125, -2.0312, -2.5625, -3.7344,\n",
      "         -3.4688, -3.7656, -3.4688, -3.9688, -1.6094, -1.5078, -3.1562, -3.0625,\n",
      "         -2.6875, -2.3125, -2.7969, -2.8906, -3.3906, -3.4688, -1.7500, -1.8516,\n",
      "         -1.6797, -3.6719, -4.1875, -2.1562, -2.3281, -0.7695, -0.7461, -3.6094,\n",
      "         -1.6641, -1.6094, -4.3750, -1.9453, -2.4219, -2.8125, -2.3750, -3.4844,\n",
      "         -3.2188, -2.8281, -2.2969, -2.9531, -2.7031, -0.8828, -3.2812, -4.0000,\n",
      "         -3.8594, -3.7969, -3.6406, -3.2812, -2.0938, -1.2266, -0.2119, -1.7656,\n",
      "         -3.5938, -3.9688, -3.9062, -2.6719, -2.2812, -3.9688, -3.5000, -3.6406,\n",
      "         -2.6562, -2.8750, -3.5781, -3.3750, -5.1250, -5.2188, -0.5352,  1.8906,\n",
      "         -4.5000, -4.4375, -5.2812, -4.7812, -5.0000, -4.9062, -5.1250, -4.8125,\n",
      "         -4.5312, -5.1562, -3.3438, -2.4375, -4.5938, -4.5938, -5.1562, -5.0938,\n",
      "         -5.2500, -5.1562, -5.2500, -5.1875, -5.2188, -5.0625, -5.1875, -5.1562,\n",
      "         -5.1875, -5.2188, -5.1250, -5.1250, -5.0312, -5.1875, -0.9102, -3.2812,\n",
      "         -3.8125, -3.8125, -3.9375, -4.1875, -4.3125, -4.4688, -5.1562, -5.1562,\n",
      "         -5.0312, -5.1562, -5.1562, -3.2656, -1.4844, -5.2188, -5.1875, -5.1250,\n",
      "         -5.1562, -5.1875, -5.1562, -5.1250, -5.1250, -5.2188, -5.1562, -5.1875,\n",
      "         -5.1250, -5.1562, -5.1562, -5.2188]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 95\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8,  9]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 95])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 95, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1055,  0.3828,  0.1914,  ...,  0.3320,  1.1484, -1.8906],\n",
      "        [ 1.0078, -1.1172,  0.6484,  ..., -0.2520,  0.5781,  0.3594],\n",
      "        [ 0.5117,  0.5898, -2.0625,  ..., -0.4688,  0.6406, -0.2812]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -132.0000,  -468.0000,   334.0000,  ...,  1120.0000,\n",
      "            234.0000,  -170.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  8640.,  -3664., -22400.,  ...,   -816.,   8032., -27008.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 95, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1055,  0.3828,  0.1914,  ...,  0.3320,  1.1484, -1.8906],\n",
      "        [ 1.0078, -1.1172,  0.6484,  ..., -0.2520,  0.5781,  0.3594],\n",
      "        [ 0.5117,  0.5898, -2.0625,  ..., -0.4688,  0.6406, -0.2812]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 95]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 95, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-9.0000, -8.8750, -2.3438, -9.0625, -9.1875, -9.0625, -9.0625, -9.0625,\n",
      "         -9.1250, -9.0000, -9.1250, -9.1875, -9.1875, -4.3438, -1.9297, -9.0625,\n",
      "         -9.1875, -8.6875, -9.1875, -9.1250, -9.0000, -9.0625, -9.1250, -9.0625,\n",
      "         -8.9375, -9.1250, -9.1250, -9.1250, -9.0625, -9.1250, -9.1250, -9.0625,\n",
      "         -8.9375, -9.1250, -9.1250, -9.1875,  0.0762, -2.5469, -2.7500, -2.8438,\n",
      "         -3.7031, -3.8125, -4.6250, -2.3594, -2.6562, -2.2969, -3.3750, -2.6406,\n",
      "         -1.0547, -0.5977, -0.8672, -2.7188, -2.8281, -2.3281, -2.5625, -3.9062,\n",
      "         -3.7031, -3.7812, -2.3281, -3.0312, -3.1250, -4.8750,  1.3516, -2.8594,\n",
      "         -3.6719, -1.6484, -3.2188, -3.4062, -4.8750, -2.7812, -3.6406, -2.7031,\n",
      "         -1.8047, -2.3438, -3.7344, -2.6719, -3.5000, -2.1875, -3.1719, -3.6406,\n",
      "         -2.9062, -2.6719, -3.0938, -3.2500, -1.5703, -3.5781, -1.1953,  1.2031,\n",
      "         -2.7969, -3.3594, -2.6562, -3.0781, -2.9062, -3.8750, -3.6875, -3.9062,\n",
      "         -4.5938, -3.0781, -3.9688, -2.2188, -4.6875,  0.0197,  0.4629,  0.9805,\n",
      "          3.0781, -0.4355, -0.4219,  2.1250, -1.2188, -2.6875, -0.2910, -3.1562,\n",
      "          0.5312,  0.9805,  2.0312, -0.8477,  1.9219, -0.0236,  1.8047, 13.1875,\n",
      "          0.3379, -2.1250,  1.1016, -1.5938, -0.9102, -1.9219, -0.0796, -3.2500,\n",
      "         -2.7031, -2.5781, -7.8438, -9.1250, -4.2500, -5.5312, -5.4375, -7.2500,\n",
      "         -6.7500, -6.8438, -6.5000, -7.6875, -3.1094, -3.9688, -6.1250, -5.4375,\n",
      "         -5.1562, -5.6875, -6.1875, -6.0625, -6.4688, -6.9062, -6.1250, -3.5781,\n",
      "         -4.8750, -7.4375, -7.6562, -4.1250, -4.7188, -4.1562, -5.2188, -7.3125,\n",
      "         -4.0312, -6.5312, -7.8125, -6.8125, -5.7812, -5.7500, -5.9062, -7.3125,\n",
      "         -5.9688, -6.0938, -5.6562, -6.5625, -6.3750, -4.0938, -6.9375, -7.7812,\n",
      "         -7.3438, -7.3750, -7.2812, -6.2188, -5.7188, -5.4062, -5.2500, -5.1562,\n",
      "         -7.4062, -7.5000, -7.5938, -5.6562, -5.4375, -7.8750, -7.2500, -7.0000,\n",
      "         -5.2812, -5.9062, -7.2500, -6.8750, -9.0625, -9.1250, -2.5781, -3.8281,\n",
      "         -8.2500, -7.9688, -9.0625, -8.6875, -8.9375, -8.8125, -9.1250, -8.6875,\n",
      "         -8.1875, -9.0625, -6.8750, -5.7500, -8.1875, -8.1875, -9.1875, -9.1250,\n",
      "         -9.2500, -9.1250, -9.2500, -9.0625, -9.1250, -9.0000, -9.1250, -9.1250,\n",
      "         -9.0625, -9.0625, -8.9375, -9.1250, -8.7500, -9.0625, -3.3438, -6.7188,\n",
      "         -7.2500, -7.3750, -7.3438, -7.9375, -8.1250, -8.5000, -8.9375, -9.0000,\n",
      "         -8.9375, -9.1250, -9.1250, -6.6250, -4.8125, -9.1875, -9.0625, -9.0625,\n",
      "         -9.0625, -9.1875, -9.0625, -9.0625, -9.0625, -9.0625, -9.1250, -9.0625,\n",
      "         -9.1250, -9.1250, -9.0000, -9.1250]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 96\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 10]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 96])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 96, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.0078, -1.1172,  0.6484,  ..., -0.2520,  0.5781,  0.3594],\n",
      "        [ 0.5117,  0.5898, -2.0625,  ..., -0.4688,  0.6406, -0.2812],\n",
      "        [-0.8906,  0.5977, -1.0156,  ..., -0.2520,  0.5234, -1.0625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -126.0000,  -480.0000,   362.0000,  ...,  1152.0000,\n",
      "            245.0000,  -178.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  9088.,  -3888., -23424.,  ...,   -836.,   8384., -28288.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 96, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.0078, -1.1172,  0.6484,  ..., -0.2520,  0.5781,  0.3594],\n",
      "        [ 0.5117,  0.5898, -2.0625,  ..., -0.4688,  0.6406, -0.2812],\n",
      "        [-0.8906,  0.5977, -1.0156,  ..., -0.2520,  0.5234, -1.0625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 96]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 96, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.6562, -8.8125, -2.4844, -7.6562, -7.6562, -7.6562, -7.6562, -7.6250,\n",
      "         -7.6875, -7.6562, -7.7188, -7.7812, -7.6562, -4.5000, -1.8594, -7.6875,\n",
      "         -7.7500, -7.0938, -7.6875, -7.7500, -7.5938, -7.6875, -7.7188, -7.6562,\n",
      "         -7.5625, -7.6562, -7.6562, -7.6562, -7.6875, -7.6250, -7.6875, -7.6562,\n",
      "         -7.5312, -7.6562, -7.6875, -7.6875, -0.1865, -3.0625, -2.2812, -0.8164,\n",
      "         -3.1094, -2.2344, -3.8750, -2.4219, -2.5625, -1.5000, -3.0156, -2.2188,\n",
      "         -1.1719, -1.2422, -0.7773, -1.9844, -2.2344, -2.6562, -2.9219, -3.7969,\n",
      "         -3.9375, -3.3438, -3.0156, -2.7656, -3.0781, -4.4688, -1.3438, -2.4375,\n",
      "         -4.4375, -2.9844, -3.0625, -4.0000, -3.9688, -2.7656, -4.1562, -3.9375,\n",
      "         -2.8750, -2.3906, -4.0938, -2.5938, -3.8906, -3.6562, -3.1562, -3.7344,\n",
      "         -2.5156, -3.7031, -2.9062, -3.6562,  0.9805, -2.2344, -2.4219, -1.4297,\n",
      "         -3.4062, -3.8594, -4.3438, -2.5469, -1.7734, -3.6250, -2.8125, -3.4062,\n",
      "         -4.9062, -3.3281, -2.4375, -2.0000, -3.8594,  0.6523,  0.1689, -0.4473,\n",
      "          1.3828,  1.0703, -0.2949,  0.1182, -0.5078,  0.6172, -1.5312, -0.5430,\n",
      "          0.9531,  1.4375,  1.0547,  0.7227, 13.5000,  0.5352,  0.2793,  3.2188,\n",
      "          1.0859, -0.2578,  0.4668, -0.7031, -1.4219, -0.2422, -2.3906, -3.7031,\n",
      "         -2.9375, -3.1562, -6.4062, -7.7500, -3.2188, -4.6562, -4.3750, -5.8750,\n",
      "         -5.9062, -5.7500, -5.3750, -6.7812, -3.2656, -4.5312, -5.0625, -4.3750,\n",
      "         -4.9062, -5.1250, -5.2812, -4.8125, -5.7812, -5.9375, -4.8438, -3.5938,\n",
      "         -3.9375, -5.8750, -6.4062, -4.7500, -4.4688, -2.7969, -3.8750, -6.1250,\n",
      "         -4.2188, -5.4062, -6.9375, -5.9375, -4.4688, -5.2812, -5.2188, -5.9062,\n",
      "         -5.3125, -5.3750, -4.5625, -5.2812, -4.8125, -1.9375, -5.6875, -6.5000,\n",
      "         -6.1875, -6.3438, -6.1250, -4.9688, -5.3750, -4.0625, -4.1875, -4.4688,\n",
      "         -6.0625, -6.3438, -6.3125, -4.8438, -4.6250, -6.6875, -6.0938, -5.8125,\n",
      "         -5.0625, -5.0625, -5.8438, -5.9062, -7.6562, -7.6875, -4.9375, -4.3125,\n",
      "         -7.0312, -6.9062, -7.7188, -7.4062, -7.5625, -7.5000, -7.6875, -7.5312,\n",
      "         -6.9688, -7.6562, -5.9688, -4.5000, -6.9062, -6.8438, -7.6875, -7.5938,\n",
      "         -7.8125, -7.6875, -7.7188, -7.5938, -7.7188, -7.6250, -7.7188, -7.6562,\n",
      "         -7.6562, -7.6875, -7.5938, -7.6875, -7.5312, -7.5938, -2.9219, -5.5312,\n",
      "         -6.4375, -6.0625, -6.3125, -6.6562, -6.7812, -7.0938, -7.5312, -7.5938,\n",
      "         -7.5312, -7.6250, -7.5938, -5.3750, -3.6406, -7.6875, -7.6562, -7.6250,\n",
      "         -7.5938, -7.6562, -7.6250, -7.5938, -7.6562, -7.6875, -7.6875, -7.6875,\n",
      "         -7.6562, -7.6875, -7.5938, -7.6875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 97\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 11]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 97])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 97, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.5117,  0.5898, -2.0625,  ..., -0.4688,  0.6406, -0.2812],\n",
      "        [-0.8906,  0.5977, -1.0156,  ..., -0.2520,  0.5234, -1.0625],\n",
      "        [ 0.3438,  0.6172,  0.0874,  ...,  1.1484,  1.1484,  0.2314]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -122.0000,  -496.0000,   374.0000,  ...,  1184.0000,\n",
      "            260.0000,  -190.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  9344.,  -4000., -24064.,  ...,   -872.,   8640., -29056.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 97, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.5117,  0.5898, -2.0625,  ..., -0.4688,  0.6406, -0.2812],\n",
      "        [-0.8906,  0.5977, -1.0156,  ..., -0.2520,  0.5234, -1.0625],\n",
      "        [ 0.3438,  0.6172,  0.0874,  ...,  1.1484,  1.1484,  0.2314]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 97]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 97, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.3750, -5.5000, -0.7852, -7.2812, -7.4375, -7.4062, -7.3750, -7.3750,\n",
      "         -7.3438, -7.3125, -7.4688, -7.5000, -7.3438, -3.2344, -0.5234, -7.4062,\n",
      "         -7.4688, -6.9375, -7.3750, -7.3750, -7.3438, -7.4688, -7.3438, -7.2812,\n",
      "         -7.3125, -7.3438, -7.2500, -7.3438, -7.3125, -7.3438, -7.4062, -7.4062,\n",
      "         -7.1562, -7.3438, -7.3125, -7.4688, -0.5078, -1.6328, -0.9883, -1.0781,\n",
      "         -2.6875, -2.5625, -2.5938, -2.4219, -1.2969, -1.6250, -1.8203, -1.0938,\n",
      "         -0.9375, -0.5312, -0.7109, -1.5391, -1.2734, -2.1406, -1.7344, -1.9453,\n",
      "         -2.1250, -1.8672, -1.3750, -1.5000, -1.8672, -3.4844, -1.2031, -1.7344,\n",
      "         -2.6094, -1.4609, -2.6406, -2.0156, -3.2812, -1.7344, -1.9062, -2.6406,\n",
      "         -1.1641, -2.3281, -2.5625, -2.1406, -1.6484, -2.3906, -1.5938, -3.0156,\n",
      "         -1.3125, -1.7500, -2.3125,  1.7969,  0.1045, -1.8438, -1.3359, -0.9844,\n",
      "         -2.1719, -3.2969, -2.8750, -2.3125, -3.4062, -2.4531, -2.4688, -0.8750,\n",
      "         -2.6719, -3.0625, -1.7891, -2.1094, -2.0781,  0.0334,  0.2812, -0.7812,\n",
      "          0.0918,  1.0469, -1.5000, -1.6016,  1.2109, -0.2949, -1.6172, -1.6641,\n",
      "          1.9062,  0.2852,  0.6953, 12.8125,  3.1250, -1.5547,  0.0312, -0.4238,\n",
      "         -1.6953,  1.2109, -0.6484, -1.1328, -0.9648, -1.0938, -2.7656, -1.3828,\n",
      "         -0.5898, -2.5781, -6.0938, -7.5000, -2.7500, -3.8594, -3.5156, -5.6562,\n",
      "         -5.2500, -5.2812, -4.4375, -6.3125, -1.3672, -3.4531, -4.1250, -3.6719,\n",
      "         -4.3438, -3.7969, -4.5312, -4.3750, -5.0312, -5.2812, -3.7188, -2.1562,\n",
      "         -2.5938, -5.4062, -5.8438, -4.0625, -2.3906, -0.6641, -3.7969, -5.6562,\n",
      "         -3.1094, -4.1562, -6.3125, -4.3438, -4.4375, -4.1875, -4.7500, -5.3125,\n",
      "         -4.4062, -4.4375, -4.1250, -5.0000, -4.0938, -1.0391, -5.1250, -5.7812,\n",
      "         -5.5000, -5.5938, -5.6250, -4.5312, -4.4062, -4.3438, -2.9375, -3.5312,\n",
      "         -5.7500, -6.0000, -5.2188, -4.3750, -4.0000, -5.7500, -5.6250, -5.0000,\n",
      "         -3.7500, -3.5938, -5.3438, -5.2500, -7.3125, -7.3750, -2.4844, -1.6016,\n",
      "         -6.5625, -6.6562, -7.4375, -7.0000, -7.1562, -7.2188, -7.4375, -7.0625,\n",
      "         -6.4688, -7.2500, -5.0000, -4.1562, -6.5625, -6.5938, -7.3750, -7.2812,\n",
      "         -7.5000, -7.3750, -7.4375, -7.3125, -7.3750, -7.2812, -7.4062, -7.3125,\n",
      "         -7.3438, -7.2812, -7.3750, -7.3125, -7.1562, -7.4062, -2.5156, -5.0312,\n",
      "         -5.9062, -5.7812, -5.9375, -6.2188, -6.4375, -6.7500, -7.2500, -7.3125,\n",
      "         -7.3750, -7.4062, -7.3125, -5.2812, -3.3125, -7.4375, -7.3750, -7.3438,\n",
      "         -7.3125, -7.3125, -7.4062, -7.3125, -7.3438, -7.3438, -7.3438, -7.3750,\n",
      "         -7.4062, -7.5312, -7.3438, -7.4062]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 98\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 12]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 98])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 98, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8906,  0.5977, -1.0156,  ..., -0.2520,  0.5234, -1.0625],\n",
      "        [ 0.3438,  0.6172,  0.0874,  ...,  1.1484,  1.1484,  0.2314],\n",
      "        [-1.2031,  0.8086,  0.0479,  ...,  0.6172,  1.5781, -0.0430]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -134.0000,  -536.0000,   422.0000,  ...,  1216.0000,\n",
      "            292.0000,  -197.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  9856.,  -4224., -25600.,  ...,   -972.,   9152., -30720.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 98, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8906,  0.5977, -1.0156,  ..., -0.2520,  0.5234, -1.0625],\n",
      "        [ 0.3438,  0.6172,  0.0874,  ...,  1.1484,  1.1484,  0.2314],\n",
      "        [-1.2031,  0.8086,  0.0479,  ...,  0.6172,  1.5781, -0.0430]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 98]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 98, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-4.2500, -3.2188, -2.7188, -4.2500, -4.3125, -4.2812, -4.3438, -4.2500,\n",
      "         -4.2188, -4.2500, -4.2812, -4.4062, -4.2188, -4.1250, -0.6289, -4.2500,\n",
      "         -4.4062, -4.2188, -4.3125, -4.2500, -4.2500, -4.3125, -4.3750, -4.2188,\n",
      "         -4.2500, -4.3125, -4.2812, -4.2812, -4.2500, -4.2188, -4.2188, -4.2812,\n",
      "         -4.0938, -4.2188, -4.2812, -4.3438, -0.5039, -2.5469, -2.6406, -4.4688,\n",
      "         -4.1875, -2.9531, -3.0000, -3.2656, -2.6406, -3.9531, -3.2344, -3.3438,\n",
      "         -0.6523, -0.8086, -1.2578, -3.3125, -1.3906, -2.9219, -2.5938, -3.2969,\n",
      "         -2.3906, -3.7344, -2.9219, -2.3750, -3.7500, -3.8594, -1.5391, -3.2969,\n",
      "         -3.0781, -2.8906, -3.3438, -2.8125, -4.6250, -2.9375, -3.6406, -3.2344,\n",
      "         -2.5000, -3.9531, -3.5781, -3.0781, -2.6094, -2.6094, -3.0156, -3.6719,\n",
      "         -3.2188, -2.8125,  0.5977, -1.7891, -2.4688, -2.8125, -2.2969, -2.1562,\n",
      "         -3.0781, -3.3906, -3.5469, -2.4688, -3.2500, -3.9219, -4.4375, -3.9844,\n",
      "         -4.5938, -4.8125, -3.1250, -3.0469, -4.4688,  0.6680, -1.4609,  0.0942,\n",
      "         -0.0410,  0.2969, -0.7070, -1.8203,  0.7227,  0.3574, -2.4062, -0.0664,\n",
      "          0.6797,  1.2109, 14.3750,  1.5547, -0.1494, -0.6445,  0.8438,  0.8203,\n",
      "          1.2578,  1.1406, -1.3906,  0.1299, -1.4609, -1.4219, -3.4062, -2.8594,\n",
      "         -1.8516, -4.6562, -3.6875, -4.3438, -4.0938, -3.5781, -3.7500, -3.7031,\n",
      "         -3.6250, -3.7344, -3.6875, -3.9844, -3.5781, -4.4375, -3.7812, -3.8438,\n",
      "         -3.6719, -4.0625, -3.7031, -3.4062, -3.4531, -3.8438, -3.3438, -3.3906,\n",
      "         -3.3125, -3.7188, -3.8906, -4.0625, -3.9062, -1.8750, -4.6562, -4.0312,\n",
      "         -3.7344, -4.0312, -4.2500, -3.9531, -4.1250, -3.9844, -4.0312, -3.8125,\n",
      "         -4.0312, -3.9688, -3.8906, -4.0625, -3.8125, -4.2188, -3.8281, -3.8125,\n",
      "         -3.9844, -3.7031, -3.7656, -3.8438, -4.6875, -3.9688, -2.0000, -3.1406,\n",
      "         -3.6562, -3.8594, -3.9375, -4.0938, -3.5000, -3.6719, -3.8281, -3.7188,\n",
      "         -3.6094, -3.7031, -4.2188, -4.2500, -4.2500, -4.3125, -4.0000, -3.5312,\n",
      "         -4.0938, -4.3438, -4.3125, -4.0625, -4.1875, -4.1875, -4.3438, -4.3125,\n",
      "         -3.9375, -4.3438, -4.1562, -3.7344, -4.0938, -4.0625, -4.2500, -4.2188,\n",
      "         -4.2812, -4.2500, -4.2500, -4.2500, -4.3125, -4.2188, -4.3125, -4.2812,\n",
      "         -4.2812, -4.2812, -4.2812, -4.2500, -4.3125, -4.2500, -2.6562, -3.6719,\n",
      "         -3.7188, -3.7656, -3.9531, -4.0312, -4.0000, -4.2812, -4.1875, -4.2188,\n",
      "         -4.1875, -4.2188, -4.2188, -3.1875, -2.6250, -4.3750, -4.2812, -4.2812,\n",
      "         -4.2500, -4.1875, -4.2188, -4.2500, -4.2188, -4.2500, -4.3438, -4.3125,\n",
      "         -4.2500, -4.3438, -4.2188, -4.3438]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 99\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 13]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 99])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 99, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.3438,  0.6172,  0.0874,  ...,  1.1484,  1.1484,  0.2314],\n",
      "        [-1.2031,  0.8086,  0.0479,  ...,  0.6172,  1.5781, -0.0430],\n",
      "        [ 0.6953, -0.1416, -1.9141,  ..., -0.7930, -1.6562, -0.4414]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -138.0000,  -544.0000,   430.0000,  ...,  1232.0000,\n",
      "            300.0000,  -200.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10048.,  -4256., -25984.,  ...,  -1000.,   9280., -31104.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 99, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.3438,  0.6172,  0.0874,  ...,  1.1484,  1.1484,  0.2314],\n",
      "        [-1.2031,  0.8086,  0.0479,  ...,  0.6172,  1.5781, -0.0430],\n",
      "        [ 0.6953, -0.1416, -1.9141,  ..., -0.7930, -1.6562, -0.4414]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 99]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 99, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.5625, -7.5938, -1.5781, -8.6875, -8.6250, -8.6250, -8.7500, -8.6875,\n",
      "         -8.6875, -8.7500, -8.7500, -8.7500, -8.6875, -4.3438, -1.1172, -8.6250,\n",
      "         -8.7500, -8.2500, -8.7500, -8.6875, -8.6250, -8.6875, -8.6875, -8.6250,\n",
      "         -8.6250, -8.6875, -8.6250, -8.6875, -8.6875, -8.7500, -8.7500, -8.6875,\n",
      "         -8.5000, -8.6250, -8.6250, -8.6875,  0.1680, -1.8359, -2.5000, -2.4688,\n",
      "         -3.8281, -2.8750, -4.6562, -1.5234, -2.2656, -1.0938, -3.0781, -1.5547,\n",
      "         -1.2812, -1.8047, -1.1641, -2.0312, -2.9219, -2.4688, -2.8281, -2.8281,\n",
      "         -3.5000, -3.1719, -1.8750, -2.9531, -2.7969, -4.4375, -0.4414, -1.9844,\n",
      "         -3.4688, -1.3047, -3.0625, -2.8594, -4.0000, -3.8125, -4.1875, -2.9688,\n",
      "         -1.6875, -1.4609, -4.0625, -2.5781, -3.2500, -2.7656, -3.0625, -4.3125,\n",
      "         -2.3438, -3.8594, -3.0781, -3.3906, -1.6797, -3.2812, -1.2578,  0.8633,\n",
      "         -2.4531, -3.5938, -2.3906, -2.8281, -1.9922, -3.4688, -3.3906, -3.2344,\n",
      "         -4.3125, -2.7188, -3.5000, -1.8516, -3.5469, -0.6016, -0.7031,  0.2207,\n",
      "          3.2500,  0.5508, -0.9531,  0.7734, -1.5156, -0.6250, -1.0547, -3.0156,\n",
      "          1.3359, -0.1631,  0.5586, -0.8672,  2.0469,  0.6172,  1.7969, 13.2500,\n",
      "          0.2949, -1.3906,  0.7578, -1.4297, -1.6562, -1.8750, -1.4844, -2.5156,\n",
      "         -2.8438, -2.4062, -7.1875, -8.6875, -3.9062, -4.9375, -4.7188, -6.6875,\n",
      "         -6.2812, -6.3125, -5.7812, -7.7188, -3.3906, -4.3438, -5.8750, -5.0312,\n",
      "         -4.7188, -5.2188, -5.7812, -5.3125, -6.1875, -6.4688, -5.5000, -2.7344,\n",
      "         -4.2812, -6.8438, -7.4062, -4.2812, -4.7500, -2.8125, -4.7500, -6.8750,\n",
      "         -4.4375, -6.6250, -7.5312, -6.0000, -5.3125, -5.2188, -5.3438, -6.7188,\n",
      "         -5.7500, -6.0000, -4.9688, -5.9062, -5.8438, -3.9062, -6.4375, -7.1250,\n",
      "         -7.1250, -6.9375, -6.9688, -5.4688, -4.5938, -4.5625, -4.4688, -4.6875,\n",
      "         -6.6250, -6.8438, -7.0625, -5.4688, -5.2188, -7.2500, -6.7500, -6.4688,\n",
      "         -4.9688, -5.4688, -6.8438, -6.7812, -8.6250, -8.6875, -2.8750, -2.9844,\n",
      "         -7.9062, -7.7500, -8.7500, -8.2500, -8.5000, -8.4375, -8.7500, -8.2500,\n",
      "         -7.9375, -8.6875, -6.6250, -5.1875, -7.4062, -7.5000, -8.6250, -8.6250,\n",
      "         -8.7500, -8.7500, -8.6875, -8.6875, -8.6250, -8.5625, -8.6875, -8.6250,\n",
      "         -8.8125, -8.7500, -8.5625, -8.6875, -8.3750, -8.6875, -2.3438, -6.1875,\n",
      "         -7.0938, -6.8438, -6.9375, -7.5938, -7.8438, -8.1250, -8.5000, -8.5625,\n",
      "         -8.4375, -8.6250, -8.6875, -5.8750, -3.9688, -8.6875, -8.6875, -8.7500,\n",
      "         -8.6875, -8.6875, -8.6875, -8.5625, -8.6250, -8.5625, -8.7500, -8.6875,\n",
      "         -8.6875, -8.6875, -8.6250, -8.7500]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 100\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 14]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 100])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 100, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.2031,  0.8086,  0.0479,  ...,  0.6172,  1.5781, -0.0430],\n",
      "        [ 0.6953, -0.1416, -1.9141,  ..., -0.7930, -1.6562, -0.4414],\n",
      "        [-0.8086,  0.6562, -0.1377,  ..., -1.0547,  1.0781, -0.4844]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -138.0000,  -544.0000,   430.0000,  ...,  1232.0000,\n",
      "            300.0000,  -200.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [  9984.,  -4288., -25984.,  ...,   -996.,   9280., -31104.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 100, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.2031,  0.8086,  0.0479,  ...,  0.6172,  1.5781, -0.0430],\n",
      "        [ 0.6953, -0.1416, -1.9141,  ..., -0.7930, -1.6562, -0.4414],\n",
      "        [-0.8086,  0.6562, -0.1377,  ..., -1.0547,  1.0781, -0.4844]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 100]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 100, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.7812, -8.8125, -1.6875, -7.7500, -7.6875, -7.6875, -7.7812, -7.7812,\n",
      "         -7.7188, -7.7812, -7.7812, -7.9062, -7.7188, -3.8594, -0.8711, -7.7812,\n",
      "         -7.9375, -7.3750, -7.7812, -7.8125, -7.7188, -7.7500, -7.7188, -7.7188,\n",
      "         -7.5625, -7.6875, -7.7188, -7.6875, -7.7188, -7.6875, -7.7500, -7.6562,\n",
      "         -7.5938, -7.7188, -7.7188, -7.7500,  0.5586, -2.2656, -1.4453, -3.1250,\n",
      "         -4.0625, -2.4062, -4.5938, -1.7656, -2.3594, -2.0781, -3.5781, -3.2969,\n",
      "         -1.0469, -2.4375, -1.4766, -3.2344, -2.4062, -2.7344, -2.2969, -2.3125,\n",
      "         -3.1250, -4.3750, -3.3750, -2.9375, -2.1875, -3.2969,  0.4473, -2.3906,\n",
      "         -3.8438, -1.6172, -2.0781, -3.5469, -4.3125, -2.9375, -4.0312, -2.7031,\n",
      "         -3.4375,  0.8633, -2.8281, -3.1094, -2.8906, -2.8125, -3.6250, -4.1875,\n",
      "         -4.1562, -3.3125, -3.0312, -3.5000, -1.7188, -4.0938, -3.2031, -1.7500,\n",
      "         -2.3750, -2.8750, -4.1250, -3.5156, -3.0312, -3.6094, -3.2188, -3.1719,\n",
      "         -4.1875, -4.4688, -3.0156, -1.8906, -3.7031,  0.3145, -0.4785,  0.2490,\n",
      "         -0.6719, 13.1250, -1.9531, -0.0884, -0.4375,  1.0156, -2.6719, -1.3125,\n",
      "         -0.8164, -1.0234,  0.5664,  0.1504,  2.7031, -1.9688,  0.1953,  1.3281,\n",
      "         -0.1094,  0.1592, -2.3125, -1.8438, -2.8594, -0.3105, -1.0312, -2.6875,\n",
      "         -2.5938, -2.2969, -6.4062, -7.7812, -3.5156, -4.3750, -4.7500, -6.1875,\n",
      "         -5.8750, -6.0938, -5.6562, -6.5312, -3.3594, -3.1562, -5.7500, -5.1875,\n",
      "         -4.6250, -5.0312, -5.0000, -5.0000, -5.6250, -5.9688, -4.3125, -3.7500,\n",
      "         -4.4062, -6.0625, -6.6250, -4.5000, -4.9062, -4.0312, -4.0000, -6.0000,\n",
      "         -4.1562, -6.1875, -6.8438, -5.5625, -4.7188, -5.1250, -4.6562, -6.0938,\n",
      "         -5.6250, -5.1562, -5.0000, -5.3438, -5.3750, -3.9844, -5.7500, -6.6250,\n",
      "         -6.3750, -6.2500, -6.0938, -5.5000, -4.0938, -3.7812, -2.8594, -4.1875,\n",
      "         -6.1562, -6.4062, -6.5625, -5.2500, -5.0938, -6.5312, -6.0938, -6.1250,\n",
      "         -4.9688, -5.6250, -6.0000, -5.9375, -7.6875, -7.8125, -2.5312, -0.4023,\n",
      "         -7.0312, -6.9062, -7.7188, -7.2812, -7.5312, -7.6250, -7.6562, -7.3438,\n",
      "         -7.0625, -7.7188, -6.0625, -4.5000, -7.0312, -7.0000, -7.7500, -7.6250,\n",
      "         -7.9062, -7.7500, -7.7812, -7.6875, -7.7188, -7.6562, -7.7812, -7.7188,\n",
      "         -7.7188, -7.7812, -7.6250, -7.7188, -7.4688, -7.7812, -3.0781, -5.7500,\n",
      "         -6.2188, -6.1875, -6.3750, -6.7500, -6.7812, -7.0938, -7.5625, -7.6875,\n",
      "         -7.6250, -7.7188, -7.6562, -5.6562, -3.9531, -7.7812, -7.8125, -7.6875,\n",
      "         -7.7188, -7.7812, -7.7188, -7.6562, -7.7500, -7.8438, -7.7500, -7.7812,\n",
      "         -7.7812, -7.8438, -7.6250, -7.9062]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 101\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 15]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 101])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 101, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.6953, -0.1416, -1.9141,  ..., -0.7930, -1.6562, -0.4414],\n",
      "        [-0.8086,  0.6562, -0.1377,  ..., -1.0547,  1.0781, -0.4844],\n",
      "        [-0.9141,  0.9492, -0.7188,  ...,  0.3379,  1.5625, -1.5625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -138.0000,  -544.0000,   430.0000,  ...,  1232.0000,\n",
      "            296.0000,  -200.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10048.,  -4256., -25984.,  ...,  -1004.,   9280., -31104.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 101, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.6953, -0.1416, -1.9141,  ..., -0.7930, -1.6562, -0.4414],\n",
      "        [-0.8086,  0.6562, -0.1377,  ..., -1.0547,  1.0781, -0.4844],\n",
      "        [-0.9141,  0.9492, -0.7188,  ...,  0.3379,  1.5625, -1.5625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 101]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 101, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.0000e+00, -7.0000e+00,  1.1094e+00, -5.9688e+00, -6.0000e+00,\n",
      "         -6.0000e+00, -5.9688e+00, -6.0000e+00, -5.9688e+00, -5.9062e+00,\n",
      "         -6.0938e+00, -6.1250e+00, -6.0938e+00, -9.7656e-01,  3.0469e+00,\n",
      "         -6.0625e+00, -6.1875e+00, -5.6562e+00, -6.1250e+00, -5.9375e+00,\n",
      "         -6.0312e+00, -6.0312e+00, -6.0312e+00, -6.0938e+00, -5.9062e+00,\n",
      "         -6.0000e+00, -6.0625e+00, -6.0312e+00, -5.9688e+00, -6.0000e+00,\n",
      "         -6.0000e+00, -5.9688e+00, -5.8750e+00, -6.1250e+00, -6.0625e+00,\n",
      "         -6.1875e+00,  3.4062e+00,  7.8125e-01,  8.6328e-01, -1.2344e+00,\n",
      "         -4.1809e-03, -1.5781e+00, -1.3594e+00, -1.9062e+00, -2.1777e-01,\n",
      "          4.1992e-01,  7.6562e-01, -1.1094e+00,  1.9141e+00,  8.2422e-01,\n",
      "          1.8672e+00, -1.0312e+00, -8.9453e-01, -7.2656e-01, -1.3574e-01,\n",
      "         -9.3359e-01, -3.3008e-01, -1.8594e+00, -2.1289e-01, -5.3125e-01,\n",
      "         -1.0156e+00, -2.5781e+00,  1.3562e+01, -1.6113e-01, -1.4141e+00,\n",
      "          7.2266e-01, -1.7969e-01, -5.8594e-01, -3.4375e+00, -1.1016e+00,\n",
      "         -9.4141e-01, -9.4531e-01, -5.8350e-02, -1.7422e+00, -3.2471e-02,\n",
      "         -1.4688e+00, -6.2891e-01, -5.5859e-01, -2.3125e+00, -2.2656e+00,\n",
      "         -4.3359e-01, -1.4688e+00, -2.2031e+00, -4.2969e-01, -1.5078e+00,\n",
      "         -2.6562e+00, -6.4453e-01, -9.1406e-01, -1.5781e+00, -5.9375e-01,\n",
      "         -4.0283e-02, -2.6094e+00, -2.1562e+00, -5.0781e-01, -6.6797e-01,\n",
      "         -1.5625e+00, -2.4219e+00, -1.0234e+00, -1.8750e+00, -1.1172e+00,\n",
      "         -2.2656e+00,  1.1953e+00, -1.2031e+00, -4.2188e-01,  2.7539e-01,\n",
      "          4.3359e-01, -1.9043e-02, -7.2656e-01, -7.1484e-01,  8.4375e-01,\n",
      "         -3.0938e+00, -2.5469e+00,  1.3750e+00, -1.1641e+00, -1.5547e+00,\n",
      "          2.1191e-01, -1.3906e+00, -2.0469e+00,  3.3203e-01, -9.4141e-01,\n",
      "         -1.0547e+00,  2.6250e+00, -4.0820e-01, -7.5781e-01, -1.6406e+00,\n",
      "          3.0469e-01, -2.0469e+00,  4.4336e-01,  1.3379e-01, -1.3281e-01,\n",
      "         -4.7500e+00, -6.0625e+00, -1.3516e+00, -3.0000e+00, -2.3594e+00,\n",
      "         -4.5625e+00, -3.7344e+00, -4.1562e+00, -3.9062e+00, -4.7188e+00,\n",
      "         -4.8438e-01, -1.2422e+00, -3.3906e+00, -2.9219e+00, -2.8438e+00,\n",
      "         -3.0938e+00, -3.6562e+00, -3.4219e+00, -3.8750e+00, -4.0938e+00,\n",
      "         -3.3594e+00, -1.0234e+00, -1.4531e+00, -4.5938e+00, -4.6250e+00,\n",
      "         -1.8281e+00, -1.7109e+00, -1.4531e+00, -2.5938e+00, -4.4375e+00,\n",
      "         -1.2344e+00, -2.3438e+00, -4.7812e+00, -4.5000e+00, -3.1719e+00,\n",
      "         -2.6875e+00, -3.7031e+00, -3.9688e+00, -3.6562e+00, -2.9375e+00,\n",
      "         -3.5625e+00, -3.5312e+00, -3.5781e+00, -7.0703e-01, -3.8438e+00,\n",
      "         -4.9688e+00, -4.5000e+00, -4.2188e+00, -4.3125e+00, -3.6094e+00,\n",
      "         -2.9062e+00, -3.0312e+00, -2.6094e+00, -2.3125e+00, -4.5625e+00,\n",
      "         -4.7812e+00, -4.3750e+00, -2.6250e+00, -3.4375e+00, -4.8125e+00,\n",
      "         -4.4688e+00, -4.0938e+00, -2.5625e+00, -3.7031e+00, -4.5938e+00,\n",
      "         -4.1250e+00, -5.9688e+00, -6.0625e+00,  8.9111e-03, -1.5547e+00,\n",
      "         -5.4688e+00, -5.2188e+00, -6.0312e+00, -5.4688e+00, -5.8750e+00,\n",
      "         -5.6562e+00, -6.0938e+00, -5.7500e+00, -5.2500e+00, -5.9688e+00,\n",
      "         -3.9219e+00, -2.5312e+00, -5.4062e+00, -5.1875e+00, -6.0625e+00,\n",
      "         -5.9688e+00, -6.1562e+00, -5.9688e+00, -6.1562e+00, -5.9375e+00,\n",
      "         -6.0000e+00, -5.9062e+00, -6.0625e+00, -6.0000e+00, -6.0625e+00,\n",
      "         -6.0312e+00, -6.0000e+00, -6.0312e+00, -5.6250e+00, -5.9688e+00,\n",
      "         -2.6406e+00, -4.0312e+00, -4.2500e+00, -4.6250e+00, -4.5625e+00,\n",
      "         -5.0625e+00, -4.9688e+00, -5.5000e+00, -5.7500e+00, -5.9375e+00,\n",
      "         -5.9062e+00, -6.0938e+00, -6.0625e+00, -4.3750e+00, -2.9375e+00,\n",
      "         -6.0625e+00, -5.9688e+00, -6.0000e+00, -5.9375e+00, -6.0625e+00,\n",
      "         -6.0000e+00, -6.0000e+00, -5.9062e+00, -6.0625e+00, -6.0625e+00,\n",
      "         -6.0938e+00, -6.0000e+00, -6.0938e+00, -5.9062e+00, -6.0938e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 102\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 104, 102])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 102, 256]), Cross output shape: torch.Size([1, 104, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8086,  0.6562, -0.1377,  ..., -1.0547,  1.0781, -0.4844],\n",
      "        [-0.9141,  0.9492, -0.7188,  ...,  0.3379,  1.5625, -1.5625],\n",
      "        [-1.5312,  0.1250,  1.4375,  ..., -0.1006, -1.4297, -0.7812]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 13, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -19.5000,  -448.0000,   300.0000,  ...,   920.0000,\n",
      "            -39.2500,  -206.0000],\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 13, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  5536.,  -6496., -15168.,  ...,   -430.,   5696., -23168.],\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 102, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8086,  0.6562, -0.1377,  ..., -1.0547,  1.0781, -0.4844],\n",
      "        [-0.9141,  0.9492, -0.7188,  ...,  0.3379,  1.5625, -1.5625],\n",
      "        [-1.5312,  0.1250,  1.4375,  ..., -0.1006, -1.4297, -0.7812]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 102]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 102, 104])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.0312e+00, -4.6875e+00,  2.2812e+00, -5.0000e+00, -5.0938e+00,\n",
      "         -5.1562e+00, -5.0625e+00, -4.9688e+00, -5.0938e+00, -5.0938e+00,\n",
      "         -5.0312e+00, -5.1562e+00, -5.1250e+00, -3.9844e+00,  1.3062e+01,\n",
      "         -5.1562e+00, -5.0000e+00, -4.6875e+00, -5.0000e+00, -5.1250e+00,\n",
      "         -4.9062e+00, -5.1250e+00, -5.0938e+00, -5.0938e+00, -5.0312e+00,\n",
      "         -5.1250e+00, -5.0000e+00, -5.0938e+00, -5.0938e+00, -5.0938e+00,\n",
      "         -5.0312e+00, -5.1250e+00, -4.9688e+00, -5.0625e+00, -5.0938e+00,\n",
      "         -5.0938e+00,  4.2812e+00, -3.4219e+00,  3.1719e+00,  5.3223e-02,\n",
      "         -3.3594e+00, -3.2031e+00, -2.4844e+00, -1.5312e+00, -2.8125e-01,\n",
      "          1.7676e-01, -3.9648e-01, -2.0312e+00,  3.5156e-01,  1.3281e+00,\n",
      "          1.6250e+00,  1.2451e-01,  3.1055e-01, -1.0400e-01,  6.3672e-01,\n",
      "         -1.1094e+00, -1.1875e+00, -1.2656e+00, -1.3203e+00, -1.6406e+00,\n",
      "         -2.1875e+00, -2.7344e+00,  5.1953e-01,  1.9141e-01,  9.7168e-02,\n",
      "         -1.3594e+00, -1.3828e+00, -2.7031e+00, -3.5469e+00, -5.6250e-01,\n",
      "         -5.8984e-01, -1.0547e+00, -9.1797e-01, -2.0625e+00, -4.0234e-01,\n",
      "         -1.1016e+00, -2.1118e-02, -2.6953e-01, -2.5625e+00, -2.9062e+00,\n",
      "         -1.7578e+00, -1.1536e-02, -2.8125e+00,  2.9688e-01, -1.7344e+00,\n",
      "         -2.2500e+00, -9.1797e-01, -7.1875e-01,  2.2559e-01, -1.6504e-01,\n",
      "         -1.5859e+00, -3.2227e-01, -3.1250e+00, -2.0625e+00, -3.7500e+00,\n",
      "         -2.2031e+00, -3.2500e+00, -2.6250e+00, -1.8750e+00, -3.0938e+00,\n",
      "         -1.0078e+00, -1.1621e-01, -3.9844e-01, -8.4375e-01, -3.5938e-01,\n",
      "         -1.9375e+00, -1.0938e+00, -3.0625e+00, -1.1016e+00, -7.5781e-01,\n",
      "         -1.8984e+00, -2.3594e+00, -1.6094e+00, -8.9453e-01, -3.9258e-01,\n",
      "         -1.0234e+00, -1.5859e+00, -1.8906e+00, -4.5508e-01, -1.5391e+00,\n",
      "         -4.6094e-01, -1.7383e-01, -1.4297e+00, -1.5703e+00, -1.8125e+00,\n",
      "         -4.8828e-01, -1.6719e+00, -2.7344e+00, -1.3047e+00, -2.0781e+00,\n",
      "         -4.5625e+00, -5.2500e+00, -2.4375e+00, -4.0000e+00, -3.7031e+00,\n",
      "         -4.0625e+00, -4.1250e+00, -4.2188e+00, -3.8281e+00, -4.5000e+00,\n",
      "         -2.6406e+00, -3.7344e+00, -4.0312e+00, -3.8906e+00, -3.9531e+00,\n",
      "         -3.7656e+00, -3.7969e+00, -4.1250e+00, -3.9688e+00, -4.2812e+00,\n",
      "         -3.4062e+00, -3.0781e+00, -2.7500e+00, -4.2500e+00, -4.3750e+00,\n",
      "         -3.2188e+00, -3.1250e+00, -4.4375e+00, -3.5938e+00, -4.5938e+00,\n",
      "         -1.9922e+00, -3.2656e+00, -4.9375e+00, -2.8281e+00, -3.5938e+00,\n",
      "         -3.7812e+00, -4.1562e+00, -4.2500e+00, -4.0938e+00, -4.3125e+00,\n",
      "         -4.4375e+00, -3.8906e+00, -3.8906e+00, -3.5156e+00, -4.1875e+00,\n",
      "         -4.4062e+00, -4.2188e+00, -4.1875e+00, -4.0938e+00, -4.0312e+00,\n",
      "         -3.1875e+00, -3.2656e+00, -3.5312e+00, -3.8281e+00, -4.4062e+00,\n",
      "         -4.3438e+00, -4.5000e+00, -3.7031e+00, -3.7812e+00, -4.1875e+00,\n",
      "         -4.2188e+00, -3.9688e+00, -3.4531e+00, -3.8750e+00, -4.2500e+00,\n",
      "         -3.9688e+00, -5.0625e+00, -5.0625e+00, -1.5703e+00, -3.1250e+00,\n",
      "         -4.6250e+00, -5.0938e+00, -5.0625e+00, -4.8750e+00, -5.0938e+00,\n",
      "         -5.1250e+00, -5.0625e+00, -5.0312e+00, -4.8750e+00, -5.0312e+00,\n",
      "         -4.2500e+00, -3.5312e+00, -4.6250e+00, -4.6250e+00, -5.1250e+00,\n",
      "         -5.1562e+00, -5.1562e+00, -5.0938e+00, -5.0312e+00, -5.0938e+00,\n",
      "         -5.0312e+00, -5.0625e+00, -5.0625e+00, -5.1562e+00, -5.1250e+00,\n",
      "         -5.0000e+00, -4.9688e+00, -5.0625e+00, -5.0312e+00, -5.0625e+00,\n",
      "          4.4678e-02, -4.0938e+00, -4.4375e+00, -4.3125e+00, -4.5625e+00,\n",
      "         -4.5938e+00, -4.7500e+00, -4.9688e+00, -5.0312e+00, -5.0000e+00,\n",
      "         -5.0312e+00, -5.0312e+00, -5.1250e+00, -4.1562e+00, -3.0312e+00,\n",
      "         -5.0938e+00, -5.0000e+00, -4.9375e+00, -4.9688e+00, -4.9375e+00,\n",
      "         -5.1250e+00, -5.0312e+00, -5.0312e+00, -4.9375e+00, -5.1250e+00,\n",
      "         -5.0312e+00, -5.0938e+00, -5.0625e+00, -5.1250e+00, -5.1250e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 103\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 112, 103])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 103, 256]), Cross output shape: torch.Size([1, 112, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.9141,  0.9492, -0.7188,  ...,  0.3379,  1.5625, -1.5625],\n",
      "        [-1.5312,  0.1250,  1.4375,  ..., -0.1006, -1.4297, -0.7812],\n",
      "        [-1.2031,  0.7188,  1.0312,  ..., -1.4844, -1.8125, -2.1250]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 14, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [   14.1250,   -84.0000,   -83.0000,  ...,   616.0000,\n",
      "             47.0000,    33.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 14, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  2384.,  -1400.,  -6112.,  ...,    178.,   2400.,  -8256.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 103, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.9141,  0.9492, -0.7188,  ...,  0.3379,  1.5625, -1.5625],\n",
      "        [-1.5312,  0.1250,  1.4375,  ..., -0.1006, -1.4297, -0.7812],\n",
      "        [-1.2031,  0.7188,  1.0312,  ..., -1.4844, -1.8125, -2.1250]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 103]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 13]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 103, 112])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.4062, -6.7500, -2.9688, -6.4062, -6.4375, -6.4062, -6.5625, -6.4062,\n",
      "         -6.4375, -6.4062, -6.4375, -6.4375, -6.4688, -4.5938, -0.6289, -6.3750,\n",
      "         -6.5000, -6.2188, -6.4688, -6.4062, -6.4375, -6.4375, -6.4062, -6.3438,\n",
      "         -6.3438, -6.3438, -6.4375, -6.4375, -6.3438, -6.4688, -6.4375, -6.4062,\n",
      "         -6.2812, -6.3750, -6.3750, -6.4375, -1.7578, -3.5625,  4.5312, -0.0150,\n",
      "         -2.6406, -3.1719, -3.0469, -1.6172, -0.3516, -3.4219,  0.5391, -2.1719,\n",
      "         -4.6562,  1.8125, -3.1562, -2.4844, -2.3281,  0.6172, -2.0625, -1.5938,\n",
      "         -1.7891, -2.3906, -2.8594, -1.9844, -2.2344, -2.2188, -2.4531, -3.4844,\n",
      "         -0.5664, -2.1875, -1.4766, -4.0312, -4.6875,  5.3125,  3.4844,  2.7500,\n",
      "          2.8750,  1.7188,  2.5938,  1.7266,  2.8125,  4.4375,  1.3516,  0.1963,\n",
      "          1.6641,  2.7500,  1.4766,  3.4688,  3.1406,  0.2441,  1.4141,  3.7344,\n",
      "          5.0312,  1.8516,  1.8047,  3.7344, -3.6875,  1.7969, -2.7500, -1.1641,\n",
      "         -3.7031, -4.4062, -3.1562, -2.3750, -1.1719,  0.1631,  0.0099, -1.1406,\n",
      "         -1.3672, -2.7812, -1.7656, -3.0781, -1.4375, -1.6953, -2.7969, -2.7188,\n",
      "         -2.3906, -2.1094, -3.1250, -1.0078, -1.5312, -2.6094, -2.4688, -1.7500,\n",
      "         -1.0391, -1.5156, -2.9375, -1.1328, -2.8438, -1.5625, -2.9844, -2.0469,\n",
      "         -0.8945, -3.5938, -5.6875, -6.4688, -3.7500, -4.5000, -4.4062, -5.4688,\n",
      "         -5.5000, -5.3125, -5.0312, -5.6562, -4.1562, -4.3750, -4.8750, -4.6875,\n",
      "         -4.7812, -4.9688, -4.7812, -4.9375, -5.3438, -5.3750, -4.2188, -3.8906,\n",
      "         -4.7500, -5.4062, -5.5625, -4.5938, -4.0312, -4.9375, -4.1250, -5.2812,\n",
      "         -3.3438, -4.0938, -5.8438, -4.3750, -4.7500, -5.2500, -4.8438, -5.6875,\n",
      "         -5.1875, -4.9062, -4.8438, -4.9688, -5.0625, -3.8125, -5.2812, -5.6250,\n",
      "         -5.5625, -5.5938, -5.5938, -4.9688, -4.1250, -4.6250, -4.7188, -4.6250,\n",
      "         -5.5000, -5.5938, -5.6562, -5.2812, -4.6875, -5.7188, -5.3750, -5.2812,\n",
      "         -4.6562, -4.6250, -5.4688, -5.3750, -6.3750, -6.4062, -2.3594, -3.8750,\n",
      "         -6.0625, -5.9688, -6.4375, -6.1250, -6.2812, -6.2812, -6.4688, -6.3438,\n",
      "         -5.9688, -6.4688, -5.5312, -4.7188, -5.9062, -5.9062, -6.3750, -6.4375,\n",
      "         -6.4688, -6.4375, -6.4688, -6.5312, -6.4062, -6.4688, -6.4375, -6.3438,\n",
      "         -6.3438, -6.4688, -6.4688, -6.4062, -6.2500, -6.4062, -0.0089, -5.0000,\n",
      "         -5.6562, -5.2500, -5.5000, -5.4375, -5.8438, -5.9688, -6.3438, -6.4062,\n",
      "         -6.3125, -6.4688, -6.4688, -5.0938, -2.3125, -6.4375, -6.3125, -6.4375,\n",
      "         -6.4375, -6.4062, -6.4375, -6.4062, -6.4062, -6.4062, -6.4375, -6.4375,\n",
      "         -6.4062, -6.4375, -6.4062, -6.4688]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 104\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  2]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 112, 104])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 104, 256]), Cross output shape: torch.Size([1, 112, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.5312,  0.1250,  1.4375,  ..., -0.1006, -1.4297, -0.7812],\n",
      "        [-1.2031,  0.7188,  1.0312,  ..., -1.4844, -1.8125, -2.1250],\n",
      "        [-0.6055, -1.6328,  0.5000,  ...,  0.5938, -0.9844, -0.4082]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 14, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -37.0000,  -253.0000,   114.5000,  ...,   864.0000,\n",
      "            159.0000,   -41.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 14, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  5472.,  -2176., -13824.,  ...,    -98.,   5120., -16640.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 104, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.5312,  0.1250,  1.4375,  ..., -0.1006, -1.4297, -0.7812],\n",
      "        [-1.2031,  0.7188,  1.0312,  ..., -1.4844, -1.8125, -2.1250],\n",
      "        [-0.6055, -1.6328,  0.5000,  ...,  0.5938, -0.9844, -0.4082]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 104]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 13]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 104, 112])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-11.0000,  -8.6875,  -3.5312, -10.9375, -10.9375, -11.0000, -11.0000,\n",
      "         -11.0000, -10.9375, -10.9375, -11.0625, -11.0625, -11.0000,  -7.4688,\n",
      "          -3.0625, -11.0000, -11.0625, -10.5000, -11.0000, -11.0000, -11.0000,\n",
      "         -11.0000, -11.0000, -10.9375, -10.9375, -11.0000, -10.9375, -11.0625,\n",
      "         -10.8750, -10.9375, -11.0625, -11.0625, -10.8125, -10.9375, -10.8750,\n",
      "         -11.0000,   4.2812,  -5.1250,  -3.5312,  -5.4375,  -6.5312,  -6.1875,\n",
      "          -7.9062,  -3.4531,  -4.9062,  -1.7031,  -4.8125,  -4.6875,  -1.6797,\n",
      "          -2.8750,  -1.8906,  -2.1562,  -4.8750,  -3.2812,  -3.4688,  -4.8125,\n",
      "          -4.0625,  -4.7188,  -4.5000,  -3.8438,  -5.5625,  -6.0938,  -3.1562,\n",
      "          -5.3125,  -5.7500,  -4.6562,  -6.6250,  -6.8750,  -7.5938,  -4.2812,\n",
      "          -2.8438,  -4.0625,  -6.1250,  -4.5938,  -4.2188,  -5.7188,  -6.2500,\n",
      "          -1.9297,  -5.3125,  -6.4375,  -5.4062,  -3.6562,  -4.9375,  -6.3438,\n",
      "          -0.7266,  -6.7500,  -4.9062,  -3.1094,  -3.0938,  -4.9375,  -6.0312,\n",
      "          -4.7812,  -7.1250,  -7.0000,  -7.0625,  -4.6250,  -7.4688,  -7.3125,\n",
      "          -4.6250,  -1.8828,  -6.7812,  -1.7344,   2.0312,   1.1406,   0.9609,\n",
      "          -1.1172,   2.8594,  -0.6016,  -1.1641,  -0.3535,  -2.4531,  -3.2188,\n",
      "           2.4688,   2.1250,   2.9688,  -2.2812,   5.8125,  -1.8516,   0.9609,\n",
      "           3.9688,   2.1406,   0.1533,  -0.2812,  -0.4863,  -4.1250,  -2.4844,\n",
      "          -4.5625,  -5.5625,  -4.2500,  -6.5625,  -9.9375, -10.8750,  -5.8438,\n",
      "          -8.3125,  -8.5000,  -9.5000,  -9.3750,  -9.2500,  -8.6250, -10.5000,\n",
      "          -7.5625,  -7.0000,  -8.8750,  -8.5625,  -8.1250,  -8.7500,  -9.2500,\n",
      "          -8.5000,  -9.5000,  -9.6250,  -8.6250,  -6.6562,  -8.0000,  -9.5625,\n",
      "          -9.7500,  -7.8750,  -8.0625,  -9.3125,  -8.1250,  -9.6875,  -7.0625,\n",
      "          -7.8125, -10.1250,  -7.2500,  -8.5000,  -8.3125,  -8.5000,  -9.5000,\n",
      "          -9.1250,  -8.7500,  -8.0000,  -8.6250,  -8.6875,  -6.3125,  -9.3125,\n",
      "          -9.8750, -10.0000,  -9.6250,  -9.9375,  -8.8750,  -7.5938,  -7.5312,\n",
      "          -8.3750,  -8.3125,  -9.6250,  -9.4375,  -9.7500,  -8.5625,  -8.5625,\n",
      "          -9.8750,  -9.5625,  -9.1250,  -7.5312,  -7.9375,  -9.5625,  -9.8750,\n",
      "         -10.8750, -10.8750,  -6.3750,  -6.0000, -10.1250, -10.3125, -10.9375,\n",
      "         -10.5000, -10.8125, -10.8125, -11.0000, -10.8750, -10.3125, -11.0625,\n",
      "          -9.5000,  -8.1875, -10.3750, -10.1250, -10.9375, -10.9375, -11.0625,\n",
      "         -10.9375, -11.0000, -11.0000, -11.0000, -10.9375, -11.0000, -10.8750,\n",
      "         -10.8750, -11.0625, -11.0000, -10.9375, -10.8750, -11.0625,  -6.5000,\n",
      "          -9.3125, -10.0000,  -9.5625,  -9.7500,  -9.9375, -10.3125, -10.3125,\n",
      "         -10.6250, -10.8125, -10.7500, -10.9375, -11.0625,  -8.8750,  -7.6250,\n",
      "         -11.0000, -10.8125, -10.9375, -10.9375, -11.0000, -11.0625, -11.0000,\n",
      "         -11.0000, -10.9375, -11.0625, -10.9375, -11.0625, -11.0000, -10.9375,\n",
      "         -11.0625]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 105\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  3]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 112, 105])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 105, 256]), Cross output shape: torch.Size([1, 112, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.2031,  0.7188,  1.0312,  ..., -1.4844, -1.8125, -2.1250],\n",
      "        [-0.6055, -1.6328,  0.5000,  ...,  0.5938, -0.9844, -0.4082],\n",
      "        [-1.3828, -0.7539,  0.0137,  ..., -0.5000, -0.2031, -0.4434]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 14, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -51.5000,  -324.0000,   182.0000,  ...,   968.0000,\n",
      "            197.0000,   -95.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 14, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  6624.,  -2592., -16896.,  ...,   -306.,   6176., -20096.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 105, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.2031,  0.7188,  1.0312,  ..., -1.4844, -1.8125, -2.1250],\n",
      "        [-0.6055, -1.6328,  0.5000,  ...,  0.5938, -0.9844, -0.4082],\n",
      "        [-1.3828, -0.7539,  0.0137,  ..., -0.5000, -0.2031, -0.4434]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 105]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 13]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 105, 112])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.7812, -7.4688, -4.3125, -7.8438, -7.9375, -7.8750, -7.7812, -7.7500,\n",
      "         -7.8125, -7.8125, -7.8750, -7.8750, -7.9688, -6.2188, -3.4531, -7.9375,\n",
      "         -7.8438, -7.5312, -7.7812, -7.9062, -7.7500, -7.7812, -7.8438, -7.6875,\n",
      "         -7.9062, -7.9688, -7.7812, -7.7812, -7.8438, -7.7812, -7.8438, -7.8125,\n",
      "         -7.7188, -7.8750, -7.8125, -7.8438, -0.3613, -5.2812, -4.2812, -3.6719,\n",
      "         -5.8750, -5.3125, -5.8750, -4.0312, -3.4688, -3.9219, -4.9375, -4.6562,\n",
      "         -1.6250, -2.6875, -3.7344, -2.6719, -3.4531, -3.5781, -3.4844, -4.3438,\n",
      "         -4.2500, -4.9062, -4.5000, -3.6094, -4.7500, -5.8125, -2.7812, -4.1875,\n",
      "         -6.2812, -5.6250, -5.0000, -6.5625, -6.1250, -3.5312, -4.6250, -4.1562,\n",
      "         -5.5312, -4.5000, -5.0625, -4.5625, -4.0625, -4.4375, -4.1562, -5.5625,\n",
      "         -5.1250, -4.5000, -4.8438, -4.0000, -1.0547, -4.6875, -4.1875, -3.1719,\n",
      "         -3.7344, -5.4062, -5.9688, -3.9219, -4.8125, -6.1250, -4.6250, -4.5938,\n",
      "         -6.9062, -5.0625, -3.6719, -2.2500, -6.0000,  3.7500, -0.4082,  0.1289,\n",
      "         -0.6797,  2.0000, -1.3516, -0.3613, -0.2910,  2.1094, -1.6719, -1.0391,\n",
      "         -0.0845,  0.2773,  1.6406,  2.2344, 10.0000, -1.5781,  4.5312,  0.6719,\n",
      "          1.4062,  0.8320, -1.1328, -1.5391, -2.0625, -2.0625, -2.2188, -4.6250,\n",
      "         -4.8750, -3.7344, -7.2188, -7.9375, -5.2500, -6.6562, -6.1562, -7.0625,\n",
      "         -7.1875, -7.0625, -6.6562, -7.4688, -5.4688, -6.0000, -6.6562, -6.2500,\n",
      "         -6.0000, -6.9062, -6.9062, -6.1875, -6.8125, -6.7188, -5.6250, -5.4688,\n",
      "         -6.2812, -6.6250, -7.1875, -7.0625, -6.1562, -6.2188, -5.7500, -7.1250,\n",
      "         -5.8438, -8.0625, -7.4062, -6.3125, -6.1562, -6.8750, -6.6250, -6.9688,\n",
      "         -6.5938, -6.5312, -6.2188, -6.7812, -6.6562, -4.9375, -6.6562, -7.1875,\n",
      "         -7.0938, -7.2500, -6.9375, -6.6250, -7.3438, -6.5312, -6.5625, -6.8750,\n",
      "         -7.4062, -7.1250, -7.2188, -6.5000, -6.7188, -7.2188, -7.0000, -6.8125,\n",
      "         -6.2812, -6.0312, -6.7812, -7.0312, -7.7812, -7.7812, -6.9062, -4.1562,\n",
      "         -7.4375, -7.4062, -7.8438, -7.5625, -7.7500, -7.7812, -7.8438, -7.7500,\n",
      "         -7.4062, -7.8438, -7.3125, -6.2188, -7.4688, -7.5312, -7.8438, -7.9062,\n",
      "         -7.8750, -7.8125, -7.7500, -7.8438, -7.8750, -7.8750, -7.8125, -7.8750,\n",
      "         -7.8125, -7.7812, -7.8438, -7.8750, -7.7500, -8.0000, -4.6250, -7.1250,\n",
      "         -7.2812, -7.0000, -7.2500, -7.2812, -7.5312, -7.6250, -7.7500, -7.7812,\n",
      "         -7.7500, -7.7812, -7.8438, -6.5625, -5.9375, -7.9062, -7.8438, -7.7500,\n",
      "         -7.8125, -7.8125, -7.9375, -7.7812, -7.8750, -7.8125, -7.8750, -7.7812,\n",
      "         -7.8125, -7.8438, -7.8125, -7.8125]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 106\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  4]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 112, 106])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 106, 256]), Cross output shape: torch.Size([1, 112, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.6055, -1.6328,  0.5000,  ...,  0.5938, -0.9844, -0.4082],\n",
      "        [-1.3828, -0.7539,  0.0137,  ..., -0.5000, -0.2031, -0.4434],\n",
      "        [ 1.9688, -0.0254,  0.9258,  ..., -1.0156,  0.4844, -0.7031]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 14, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -51.5000,  -328.0000,   184.0000,  ...,   968.0000,\n",
      "            199.0000,   -97.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 14, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  6656.,  -2608., -17024.,  ...,   -312.,   6208., -20224.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 106, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.6055, -1.6328,  0.5000,  ...,  0.5938, -0.9844, -0.4082],\n",
      "        [-1.3828, -0.7539,  0.0137,  ..., -0.5000, -0.2031, -0.4434],\n",
      "        [ 1.9688, -0.0254,  0.9258,  ..., -1.0156,  0.4844, -0.7031]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 106]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 13]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 106, 112])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.0625, -8.6875, -1.8281, -8.2500, -8.3125, -8.1250, -8.1875, -8.1250,\n",
      "         -8.1875, -8.0625, -8.2500, -8.1250, -8.2500, -5.0312, -0.9609, -8.3125,\n",
      "         -8.1875, -7.9062, -8.1875, -8.2500, -8.1875, -8.1875, -8.2500, -8.0625,\n",
      "         -8.1875, -8.2500, -8.1250, -8.1875, -8.1875, -8.1875, -8.2500, -8.2500,\n",
      "         -8.2500, -8.1875, -8.1875, -8.2500,  2.3750, -1.6328, -2.3594, -5.0625,\n",
      "         -4.0312, -4.0312, -5.2500, -0.7852, -2.3125, -3.8125, -2.9531, -2.8281,\n",
      "          1.3750, -0.9844, -0.2021, -2.3281, -5.5312, -4.5000, -4.1562, -4.2812,\n",
      "         -4.1562, -5.1562, -3.8125, -4.4375, -3.7969, -4.8438, -1.1328, -2.1094,\n",
      "         -5.9062, -4.1250, -3.3906, -5.0312, -5.0938, -4.9375, -4.8438, -2.4062,\n",
      "         -3.9219, -2.9531, -4.7500, -4.4062, -4.0938, -4.4062, -4.2500, -5.4688,\n",
      "          0.3223, -4.4062, -3.7031, -4.2812, -3.4688, -4.5000, -3.7188, -3.5000,\n",
      "         -3.1250, -5.6250, -4.6250, -4.3438, -5.5938, -5.5312, -5.1875, -4.5312,\n",
      "         -5.5938, -5.8125, -5.9062, -3.4375, -5.0312,  0.1426, -3.3750, -0.8633,\n",
      "         -2.0781,  5.0625, -2.1562, -2.4375,  0.5547,  0.8164, -4.1875, -2.9688,\n",
      "         10.6875, -2.1875,  0.1768,  2.0000, -0.8125, -3.2344,  3.8906,  1.2344,\n",
      "          0.4434,  0.9219, -2.9531, -2.0938, -2.9219,  0.1562, -3.2812, -5.9375,\n",
      "         -4.1562, -4.4062, -7.3125, -8.3125, -7.3750, -6.5000, -6.6562, -7.3438,\n",
      "         -7.0312, -7.2500, -6.7812, -7.6875, -5.8438, -5.4688, -6.5938, -6.5000,\n",
      "         -6.3750, -6.1875, -6.7188, -6.1250, -6.6562, -6.6562, -5.9062, -5.4688,\n",
      "         -6.2188, -7.2500, -7.4688, -7.3438, -5.4688, -4.9062, -6.1250, -7.3438,\n",
      "         -6.1562, -6.8438, -7.4062, -7.4062, -7.0312, -6.2812, -6.9062, -7.5312,\n",
      "         -6.8125, -6.3125, -6.5938, -6.9375, -7.2500, -6.2188, -6.6875, -7.4062,\n",
      "         -7.2500, -7.4062, -7.5000, -6.8125, -6.2500, -6.1562, -5.9688, -6.8125,\n",
      "         -7.1562, -7.3125, -7.3438, -6.9688, -6.6875, -7.2500, -7.2812, -6.8125,\n",
      "         -6.5000, -7.4062, -7.2812, -7.1875, -8.1250, -8.1875, -3.7188, -2.0469,\n",
      "         -7.6875, -7.8438, -8.1250, -7.8438, -8.1875, -8.2500, -8.2500, -8.0625,\n",
      "         -7.6562, -8.2500, -7.4062, -6.0312, -7.7188, -7.7500, -8.1875, -8.3125,\n",
      "         -8.3125, -8.1875, -8.2500, -8.1875, -8.2500, -8.2500, -8.1250, -8.1875,\n",
      "         -8.1250, -8.1875, -8.0625, -8.3125, -8.0625, -8.3125, -2.9844, -7.1562,\n",
      "         -7.2812, -7.2812, -7.3125, -7.6250, -7.7812, -7.8125, -7.9688, -8.0000,\n",
      "         -8.1250, -8.1875, -8.1250, -6.9375, -5.9062, -8.3125, -8.1250, -8.1250,\n",
      "         -8.1875, -8.1875, -8.1875, -8.1250, -8.2500, -8.1875, -8.2500, -8.1250,\n",
      "         -8.2500, -8.1250, -8.1875, -8.1875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 107\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  5]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 112, 107])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 107, 256]), Cross output shape: torch.Size([1, 112, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.3828, -0.7539,  0.0137,  ..., -0.5000, -0.2031, -0.4434],\n",
      "        [ 1.9688, -0.0254,  0.9258,  ..., -1.0156,  0.4844, -0.7031],\n",
      "        [ 1.7266,  0.6523,  1.8906,  ..., -1.0469,  1.3906, -0.1260]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 14, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -384.0000,   238.0000,  ...,  1016.0000,\n",
      "            237.0000,  -135.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 14, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  7424.,  -2832., -18944.,  ...,   -458.,   6880., -22400.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 107, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.3828, -0.7539,  0.0137,  ..., -0.5000, -0.2031, -0.4434],\n",
      "        [ 1.9688, -0.0254,  0.9258,  ..., -1.0156,  0.4844, -0.7031],\n",
      "        [ 1.7266,  0.6523,  1.8906,  ..., -1.0469,  1.3906, -0.1260]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 107]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 13]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 107, 112])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.1562, -5.6562, -2.0625, -5.2812, -5.2188, -5.1250, -5.3125, -5.1875,\n",
      "         -5.3125, -5.1562, -5.2812, -5.3125, -5.2188, -2.9375, -1.0469, -5.2500,\n",
      "         -5.2188, -5.0938, -5.1562, -5.1875, -5.2500, -5.2188, -5.1875, -5.1875,\n",
      "         -5.2188, -5.1875, -5.2188, -5.2500, -5.1562, -5.1875, -5.1875, -5.1875,\n",
      "         -5.0938, -5.1250, -5.1562, -5.3438, -0.4980, -2.7812, -1.0156, -4.0312,\n",
      "         -4.2188, -3.1719, -3.1719, -1.6797, -1.3594, -2.8438, -3.3594, -3.2969,\n",
      "         -1.1953, -2.7812, -1.5781, -2.9062, -2.4219, -3.2656, -2.2188, -2.2812,\n",
      "         -2.0312, -2.8594, -3.1250, -2.9219, -2.5625, -3.2500, -2.0938, -2.5938,\n",
      "         -4.0938, -2.3281, -2.6562, -3.5938, -2.9531, -2.9062, -4.3750, -2.8750,\n",
      "         -3.6875,  1.0703, -3.2031, -4.6562, -3.1250, -2.6562, -3.7344, -4.3750,\n",
      "         -3.9688, -3.0938, -3.4062, -4.4062, -3.7812, -4.0625, -3.4844, -3.1562,\n",
      "         -2.7500, -3.4375, -3.3438, -3.8125, -3.4688, -1.9297, -4.0625, -3.0469,\n",
      "         -3.4531, -4.8438, -4.6250, -0.9883, -2.2969,  1.4062, -1.7812, -0.1963,\n",
      "         -0.4395, 11.5000, -1.7109, -2.2969, -0.2236,  3.3594, -3.4062, -0.5820,\n",
      "         -0.9375, -0.6367, -0.6562,  0.5742, -1.2266, -2.5781,  0.8867, -0.6484,\n",
      "          0.6211,  1.0078, -2.3281, -1.4375, -2.5469,  6.2188, -2.4688, -3.3906,\n",
      "         -2.7656, -0.6016, -4.5312, -5.2188, -4.6875, -3.6875, -4.2812, -4.5000,\n",
      "         -4.6562, -4.7500, -4.1250, -4.5938, -4.4688, -3.3281, -4.7188, -4.4688,\n",
      "         -4.0312, -4.4062, -3.9375, -4.1250, -4.0938, -4.5000, -3.6562, -4.0938,\n",
      "         -4.0625, -4.4375, -4.6875, -4.7500, -4.2500, -3.6250, -3.2969, -4.6562,\n",
      "         -3.5312, -3.4219, -4.8750, -4.5625, -3.9375, -4.3125, -4.2500, -4.7188,\n",
      "         -4.8438, -4.3125, -3.9062, -4.1562, -4.2500, -3.8750, -4.2812, -4.7500,\n",
      "         -4.6562, -4.6875, -4.3438, -4.4688, -3.2344, -2.7500, -2.9688, -3.9219,\n",
      "         -4.2812, -4.5000, -4.5000, -4.5000, -4.3438, -4.5938, -4.4688, -4.5625,\n",
      "         -4.1875, -4.6562, -4.6562, -4.5625, -5.1250, -5.2812, -3.2969,  0.4238,\n",
      "         -4.9062, -4.9062, -5.1875, -4.9375, -5.0938, -5.0000, -5.1875, -5.1250,\n",
      "         -4.9688, -5.1875, -4.6875, -3.3906, -4.9062, -4.7500, -5.2500, -5.2188,\n",
      "         -5.3438, -5.2188, -5.1875, -5.1562, -5.2812, -5.3125, -5.2500, -5.0938,\n",
      "         -5.1562, -5.2812, -5.2188, -5.1562, -5.0000, -5.3125, -2.7500, -4.2500,\n",
      "         -4.6250, -4.5312, -4.5000, -4.7812, -4.8125, -5.0312, -5.0312, -5.1250,\n",
      "         -5.0938, -5.1875, -5.1562, -4.3125, -3.3281, -5.2812, -5.2188, -5.1875,\n",
      "         -5.0625, -5.0938, -5.1875, -5.2188, -5.1875, -5.2500, -5.1875, -5.2812,\n",
      "         -5.2188, -5.1875, -5.1875, -5.2812]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 108\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  6]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 112, 108])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 108, 256]), Cross output shape: torch.Size([1, 112, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.9688, -0.0254,  0.9258,  ..., -1.0156,  0.4844, -0.7031],\n",
      "        [ 1.7266,  0.6523,  1.8906,  ..., -1.0469,  1.3906, -0.1260],\n",
      "        [ 0.1914,  0.6250, -0.0215,  ...,  0.6836,  0.7578,  0.5508]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 14, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -82.0000,  -412.0000,   256.0000,  ...,  1040.0000,\n",
      "            243.0000,  -155.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 14, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  7808.,  -2944., -19968.,  ...,   -500.,   7232., -23552.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 108, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.9688, -0.0254,  0.9258,  ..., -1.0156,  0.4844, -0.7031],\n",
      "        [ 1.7266,  0.6523,  1.8906,  ..., -1.0469,  1.3906, -0.1260],\n",
      "        [ 0.1914,  0.6250, -0.0215,  ...,  0.6836,  0.7578,  0.5508]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 108]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 13]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 108, 112])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[ -9.8750,  -9.3750,   2.7812, -10.0000, -10.0625, -10.0000, -10.0625,\n",
      "         -10.0625, -10.1250, -10.0625, -10.0625, -10.1250, -10.0625,  -4.8750,\n",
      "           2.7344, -10.1250, -10.1875,  -9.6250, -10.1250, -10.0000, -10.0000,\n",
      "         -10.0625, -10.0000, -10.0000, -10.0000, -10.0625, -10.0000, -10.0625,\n",
      "         -10.0625, -10.0625, -10.0625, -10.0000, -10.0000, -10.0625, -10.0000,\n",
      "          -9.9375,   6.7188,   0.0190,  -0.3105,  -5.0938,  -3.9688,  -4.3125,\n",
      "          -4.9062,   2.3281,  -1.7891,  -3.7812,  -2.9688,  -3.8750,   7.2812,\n",
      "           2.1562,   2.9531,  -0.6758,  -3.5938,  -2.6250,  -2.5000,  -2.3125,\n",
      "          -3.1250,  -2.3594,  -2.0312,  -2.4531,  -3.3281,  -3.5625,   3.2500,\n",
      "          -1.5391,  -4.3750,  -2.9688,  -4.3438,  -4.3125,  -4.0938,  -2.2969,\n",
      "          -3.5156,  -3.4688,  -2.5000,  -2.3750,  -2.7188,  -2.8125,  -4.8438,\n",
      "          -1.1172,  -3.4531,  -4.0625,  -2.0938,  -2.4375,  -3.0625,  -3.5469,\n",
      "          -3.5312,  -6.0000,  -1.6562,  -2.0000,  -1.2578,  -4.1562,  -5.4062,\n",
      "          -3.5000,  -3.8125,  -3.6406,  -4.1875,  -4.3125,  -4.5312,  -2.5000,\n",
      "          -5.5625,  -1.9297,  -3.9219,  -0.0317,  -1.7734,  -2.2969,   1.5859,\n",
      "          -0.3125,  -1.1172,  -0.4141,  -1.8438,  -0.3945,  -1.5234,  -3.0938,\n",
      "           1.7734,  -0.0703,   0.1338,  -1.5859,  -2.8125,  -3.8594,  -0.1099,\n",
      "           4.8125,   0.3926,  -2.2812,  -3.4688,  -2.5625,  -1.9844,  -0.8750,\n",
      "          -3.3438,  -4.0312,  -2.3438,  -3.7812,  -8.6250, -10.1875,  -4.7500,\n",
      "          -6.3125,  -6.2812,  -7.8125,  -7.9375,  -7.9062,  -7.5312,  -8.9375,\n",
      "          -5.0625,  -5.5938,  -7.4062,  -7.2188,  -6.4375,  -6.5312,  -7.1562,\n",
      "          -7.0625,  -7.5000,  -7.9688,  -6.0938,  -5.2188,  -5.6250,  -8.3750,\n",
      "          -8.7500,  -6.5312,  -5.9375,  -5.6562,  -5.8125,  -8.1250,  -5.3438,\n",
      "          -5.5312,  -9.0000,  -7.0938,  -7.3750,  -6.8125,  -6.6250,  -8.3750,\n",
      "          -7.5938,  -7.1250,  -6.7500,  -7.4062,  -7.3750,  -5.0312,  -7.6875,\n",
      "          -8.6250,  -8.3750,  -8.1250,  -8.3125,  -7.4062,  -5.9375,  -5.9375,\n",
      "          -5.1250,  -6.1875,  -8.1875,  -8.4375,  -8.1875,  -6.7812,  -7.1562,\n",
      "          -8.4375,  -8.1875,  -8.1250,  -6.3750,  -6.7500,  -8.0000,  -7.8750,\n",
      "         -10.0000, -10.0000,  -3.9219,  -3.4219,  -9.1250,  -9.0625, -10.0000,\n",
      "          -9.5625,  -9.8750,  -9.7500, -10.0625,  -9.6875,  -9.1250,  -9.9375,\n",
      "          -7.6250,  -6.2188,  -9.1875,  -9.1250, -10.1875, -10.0000, -10.1250,\n",
      "         -10.0625, -10.1250, -10.0000, -10.0000, -10.1250, -10.0625, -10.0625,\n",
      "         -10.0000, -10.0000, -10.0000, -10.0625,  -9.6875, -10.0625,   0.3105,\n",
      "          -7.8125,  -8.4375,  -8.3750,  -8.5000,  -8.8750,  -9.1875,  -9.3750,\n",
      "          -9.8125,  -9.8750,  -9.8125, -10.0000, -10.1250,  -7.6875,  -5.0938,\n",
      "         -10.1250, -10.0000,  -9.9375, -10.0000, -10.0625,  -9.9375,  -9.8750,\n",
      "         -10.0000, -10.0625, -10.1250, -10.0625,  -9.9375, -10.0000, -10.0625,\n",
      "         -10.1250]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 109\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 112, 109])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 109, 256]), Cross output shape: torch.Size([1, 112, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.7266,  0.6523,  1.8906,  ..., -1.0469,  1.3906, -0.1260],\n",
      "        [ 0.1914,  0.6250, -0.0215,  ...,  0.6836,  0.7578,  0.5508],\n",
      "        [-1.1484,  0.1777,  0.1953,  ...,  0.7461, -0.7070, -2.2031]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 14, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -508.0000,   348.0000,  ...,  1024.0000,\n",
      "             -9.5000,  -233.0000],\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 14, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  6400.,  -6688., -17408.,  ...,   -564.,   6432., -25600.],\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 109, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.7266,  0.6523,  1.8906,  ..., -1.0469,  1.3906, -0.1260],\n",
      "        [ 0.1914,  0.6250, -0.0215,  ...,  0.6836,  0.7578,  0.5508],\n",
      "        [-1.1484,  0.1777,  0.1953,  ...,  0.7461, -0.7070, -2.2031]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 109]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 109, 112])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.5625, -8.8750,  2.4062, -7.4688, -7.5312, -7.5625, -7.4688, -7.4062,\n",
      "         -7.6250, -7.5312, -7.4062, -7.5938, -7.5312, -4.9062,  3.7656, -7.5312,\n",
      "         -7.5000, -7.2812, -7.5625, -7.4062, -7.4062, -7.4062, -7.4688, -7.3750,\n",
      "         -7.5000, -7.5000, -7.4688, -7.6250, -7.3438, -7.4688, -7.4688, -7.4375,\n",
      "         -7.4375, -7.6875, -7.5000, -7.5312,  9.1250, -1.4844, -1.3125, -2.2031,\n",
      "         -4.4375, -5.7500, -4.6875, -1.7969, -2.0625, -4.3750, -2.4062, -4.3125,\n",
      "          0.6484,  0.2197, -1.0859, -0.9375, -4.0938, -2.6719, -2.2188, -2.8281,\n",
      "         -2.8281, -3.4219, -3.9219, -3.5469, -4.3438, -4.2188,  2.0469, -5.0938,\n",
      "         -2.8125, -3.1875, -3.6094, -4.0000, -4.6250, -2.0781, -2.8125, -2.1406,\n",
      "         -2.4219, -3.0000, -2.6875, -4.5312, -2.0156, -1.0781, -2.9688, -3.8281,\n",
      "         -3.4844, -1.9219, -3.4219, -2.2344, -2.0625, -4.4062, -1.4219, -1.6719,\n",
      "         -1.1797, -4.2500, -3.7188, -2.1562, -5.2812, -3.1250, -4.5625, -4.1562,\n",
      "         -5.7812, -4.3125, -4.6562, -2.7656, -5.2500,  0.7852, -1.0547, -0.8711,\n",
      "         -1.0234,  0.0618, -1.2188, -2.3281,  0.0923,  2.4375, -1.7031, -1.6562,\n",
      "         -0.8398, -1.0312, -0.1689, -0.3086, -2.4531, -4.6875, -1.0078, -0.1196,\n",
      "         -1.1250, -1.3594, -4.5000, -1.7344, -4.5938,  0.4707, -3.1094, -4.1250,\n",
      "         -2.7031, -4.6562, -6.7812, -7.5312, -5.0625, -5.8125, -5.8750, -6.3125,\n",
      "         -6.3125, -6.3438, -6.4062, -6.9062, -5.0625, -5.6250, -6.0625, -6.4375,\n",
      "         -5.9375, -5.7188, -6.4062, -5.5000, -6.2500, -6.3750, -5.3125, -5.8750,\n",
      "         -5.8750, -6.0000, -6.6250, -5.4375, -5.0000, -5.9375, -5.5625, -6.3125,\n",
      "         -3.9375, -5.3438, -6.9062, -5.9062, -6.2188, -6.2188, -6.0000, -6.5312,\n",
      "         -6.1875, -5.8125, -5.8438, -5.9062, -6.0625, -4.8750, -6.1875, -6.7500,\n",
      "         -6.6250, -6.5625, -6.5312, -5.8750, -6.1875, -5.0312, -4.8438, -5.4688,\n",
      "         -6.8125, -6.6562, -6.6250, -5.9688, -6.1562, -6.4375, -6.4062, -6.1562,\n",
      "         -5.1562, -5.0312, -6.2500, -6.3125, -7.5000, -7.5000, -3.8750, -2.7188,\n",
      "         -7.0625, -6.9688, -7.5000, -7.2500, -7.4375, -7.4062, -7.5000, -7.4062,\n",
      "         -6.8750, -7.6250, -6.8438, -6.2500, -7.0938, -6.8750, -7.5000, -7.4375,\n",
      "         -7.6562, -7.5000, -7.5938, -7.4688, -7.4375, -7.5312, -7.5000, -7.5000,\n",
      "         -7.4375, -7.4688, -7.5312, -7.4375, -7.4062, -7.5312, -2.7812, -5.8125,\n",
      "         -6.5312, -6.2812, -6.4062, -6.8750, -7.0625, -7.0312, -7.3438, -7.4062,\n",
      "         -7.1875, -7.5312, -7.5625, -6.0625, -4.7500, -7.6250, -7.4062, -7.3750,\n",
      "         -7.4688, -7.4062, -7.5312, -7.4688, -7.5312, -7.5312, -7.5312, -7.5625,\n",
      "         -7.4688, -7.5625, -7.5000, -7.5625]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 110\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 110])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 110, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.1914,  0.6250, -0.0215,  ...,  0.6836,  0.7578,  0.5508],\n",
      "        [-1.1484,  0.1777,  0.1953,  ...,  0.7461, -0.7070, -2.2031],\n",
      "        [-0.8125,  0.5586,  0.5078,  ...,  0.3320, -1.1172, -2.2656]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  104.0000,   -95.0000,   -60.5000,  ...,   318.0000,\n",
      "             33.5000,   -16.2500]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928.0000, -37120.0000,  31488.0000,  ...,   4512.0000,\n",
      "           -9728.0000,  -5952.0000],\n",
      "         [ 21632.0000, -11584.0000, -56576.0000,  ...,  -1032.0000,\n",
      "           18944.0000, -68608.0000],\n",
      "         [  3040.0000, -14208.0000,  -7936.0000,  ...,   -996.0000,\n",
      "            4224.0000, -22656.0000],\n",
      "         ...,\n",
      "         [ 10880.0000,  -3696.0000, -28032.0000,  ...,   -960.0000,\n",
      "            9984.0000, -32384.0000],\n",
      "         [  8448.0000,  -2960.0000, -21760.0000,  ...,   -644.0000,\n",
      "            7872.0000, -25344.0000],\n",
      "         [  2160.0000,  -1656.0000,  -5120.0000,  ...,    114.5000,\n",
      "            2224.0000,  -7392.0000]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 110, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.1914,  0.6250, -0.0215,  ...,  0.6836,  0.7578,  0.5508],\n",
      "        [-1.1484,  0.1777,  0.1953,  ...,  0.7461, -0.7070, -2.2031],\n",
      "        [-0.8125,  0.5586,  0.5078,  ...,  0.3320, -1.1172, -2.2656]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 110]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 110, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[ -9.9375,  -8.3750,  -5.8750,  -9.8750,  -9.8750,  -9.9375,  -9.8125,\n",
      "          -9.7500,  -9.8125,  -9.8750,  -9.9375, -10.0000,  -9.9375,  -6.5625,\n",
      "          -2.2188,  -9.8750,  -9.9375,  -9.5625,  -9.8750,  -9.8750,  -9.8125,\n",
      "          -9.8125,  -9.9375,  -9.8750,  -9.8125,  -9.8750,  -9.9375,  -9.8125,\n",
      "          -9.8125,  -9.8750,  -9.9375,  -9.8750,  -9.8125,  -9.9375,  -9.8125,\n",
      "          -9.8750,  -3.8125,  -5.1562,  -0.3145,  -4.3750,  -5.9375,  -6.5000,\n",
      "          -5.9688,  -2.7969,  -2.3906,  -8.1250,  -5.8125,  -5.1875,  -5.4062,\n",
      "          -2.6562,  -4.4688,  -2.8125,  -6.2500,  -3.3594,  -3.3281,  -4.0938,\n",
      "          -3.8438,  -4.0938,  -5.0312,  -4.6875,  -4.2812,  -5.1562,  -4.8750,\n",
      "          -5.7812,  -4.6875,  -6.3750,  -6.2188,  -8.1250,  -6.2188,  -0.1318,\n",
      "           0.5586,  -1.1094,  -2.7656,  -2.4531,  -2.1562,  -2.6094,  -1.6562,\n",
      "           1.8047,  -0.2891,  -3.4062,  -1.9297,  -0.8008,  -2.7656,  -1.9062,\n",
      "           0.0325,  -4.9062,  -2.2812,   0.0679,  -1.2344,  -2.7188,  -3.1562,\n",
      "          -2.5938,  -5.5000,  -3.4219,  -6.0000,  -2.1875,  -7.3750,  -7.3125,\n",
      "          -6.0625,  -3.6875,  -5.2812,   2.1094,   3.2656,   1.0469,   0.1562,\n",
      "          -0.8086,   0.7656,  -0.8359,   1.5156,  -0.1865,  -0.4863,  -2.5469,\n",
      "           0.3066,   0.9102,  -1.6328,   0.4062,   1.4922,  -1.8281,  -1.4141,\n",
      "           1.3906,   1.5078,  -0.8945,  -1.5391,  -0.0708,  -7.3125,  -0.3320,\n",
      "          -3.8281,  -3.9844,  -4.7812,  -6.2812,  -8.7500,  -9.9375,  -5.5312,\n",
      "          -7.7812,  -7.3438,  -8.6875,  -8.4375,  -8.8125,  -7.8438,  -9.2500,\n",
      "          -6.9375,  -7.4062,  -8.4375,  -8.1250,  -8.0000,  -8.3125,  -8.4375,\n",
      "          -7.6562,  -8.4375,  -8.6250,  -7.3750,  -6.8125,  -7.6875,  -8.6875,\n",
      "          -8.7500,  -7.0312,  -7.4375,  -8.2500,  -7.0312,  -8.6875,  -6.0938,\n",
      "          -6.0312,  -9.0000,  -7.9375,  -7.9062,  -8.0625,  -7.9688,  -8.7500,\n",
      "          -8.3125,  -8.0000,  -7.7812,  -8.3125,  -8.5000,  -6.4688,  -8.3750,\n",
      "          -8.8125,  -9.0000,  -8.9375,  -8.7500,  -8.3125,  -6.5938,  -7.4688,\n",
      "          -7.6875,  -7.7500,  -8.8750,  -8.8750,  -8.9375,  -7.7500,  -8.0000,\n",
      "          -8.8750,  -8.6875,  -8.5000,  -7.3125,  -7.4062,  -8.4375,  -8.5625,\n",
      "          -9.8125,  -9.9375,  -5.1562,  -5.1250,  -9.3125,  -9.3125,  -9.8125,\n",
      "          -9.5625,  -9.6875,  -9.6250,  -9.9375,  -9.5625,  -9.3125, -10.0000,\n",
      "          -8.7500,  -7.7812,  -9.1250,  -9.0000,  -9.9375,  -9.8750, -10.0000,\n",
      "          -9.9375,  -9.8750,  -9.9375,  -9.8750,  -9.9375,  -9.8750,  -9.8750,\n",
      "          -9.8125,  -9.8750,  -9.8750,  -9.9375,  -9.6875,  -9.9375,  -3.5938,\n",
      "          -8.3125,  -8.8750,  -8.6875,  -8.7500,  -9.1875,  -9.2500,  -9.5625,\n",
      "          -9.6250,  -9.8750,  -9.7500,  -9.8125, -10.0000,  -8.3125,  -6.6250,\n",
      "          -9.9375,  -9.9375,  -9.8750,  -9.8125,  -9.8750,  -9.8750,  -9.9375,\n",
      "          -9.8750,  -9.9375,  -9.8750, -10.0000,  -9.8750,  -9.9375,  -9.8125,\n",
      "          -9.9375]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 111\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  2]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 111])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 111, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.1484,  0.1777,  0.1953,  ...,  0.7461, -0.7070, -2.2031],\n",
      "        [-0.8125,  0.5586,  0.5078,  ...,  0.3320, -1.1172, -2.2656],\n",
      "        [ 0.0039, -0.3438, -0.4863,  ...,  0.0879,  1.3281, -0.3262]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [   17.8750,  -249.0000,   114.5000,  ...,   816.0000,\n",
      "            160.0000,   -27.7500]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  5472.,  -2160., -13696.,  ...,   -183.,   5152., -16512.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 111, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.1484,  0.1777,  0.1953,  ...,  0.7461, -0.7070, -2.2031],\n",
      "        [-0.8125,  0.5586,  0.5078,  ...,  0.3320, -1.1172, -2.2656],\n",
      "        [ 0.0039, -0.3438, -0.4863,  ...,  0.0879,  1.3281, -0.3262]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 111]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 111, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.0625, -8.3125, -3.4219, -8.0625, -8.1250, -8.1250, -8.0000, -8.0000,\n",
      "         -8.0625, -8.0000, -8.1250, -8.1250, -8.1250, -5.9062, -1.7500, -8.1250,\n",
      "         -8.1875, -7.9062, -8.0625, -8.0000, -8.0625, -8.0625, -8.1250, -8.0000,\n",
      "         -8.0625, -8.0625, -8.1875, -8.1250, -8.0625, -8.0000, -8.0625, -8.0625,\n",
      "         -7.9375, -8.1875, -8.1250, -8.0625, -0.7578, -4.6250, -2.3906, -4.8125,\n",
      "         -5.4375, -6.5000, -5.1250, -2.7188, -3.3281, -3.0156, -4.0000, -3.7969,\n",
      "         -1.0547, -2.9062, -1.2656, -3.0938, -5.0938, -3.6094, -3.1719, -4.1875,\n",
      "         -3.9531, -4.5625, -4.7812, -5.0938, -4.8750, -5.3125, -1.4531, -4.5312,\n",
      "         -4.9688, -4.5312, -4.4062, -5.0000, -5.9375, -1.3359, -4.0000, -4.7812,\n",
      "         -6.0312, -4.2812, -4.4062, -5.2500, -4.0000, -3.5781, -4.1562, -5.5625,\n",
      "         -3.5000, -4.2500, -5.2188, -3.3594, -4.1250, -5.8750, -5.0000, -3.4375,\n",
      "         -3.8906, -5.0000, -5.5625, -5.6562, -5.8125, -3.8125, -5.6875, -3.9375,\n",
      "         -5.5938, -5.6562, -5.5625, -4.4062, -4.6562,  8.4375, -2.5625, -2.1562,\n",
      "         -2.7812,  4.8125, -2.8281, -2.9531, -0.0981,  3.2500, -2.6562, -2.9375,\n",
      "          3.2188, -2.1719, -1.0703,  4.2188, -2.5781, -3.8906,  4.2188, -2.2969,\n",
      "         -1.2109,  4.6250, -3.6250, -1.9453, -4.4375,  2.7188, -3.7188, -4.2188,\n",
      "         -4.7812, -3.2656, -7.2188, -8.1250, -6.3125, -6.4375, -6.2812, -7.5000,\n",
      "         -6.7188, -7.0938, -6.6875, -7.5625, -5.5938, -6.0312, -6.8125, -6.5625,\n",
      "         -6.4062, -6.6562, -6.9375, -6.6250, -6.8125, -7.0000, -6.2812, -5.4375,\n",
      "         -6.3125, -7.2188, -7.3125, -6.2188, -5.6875, -6.5625, -5.8125, -7.3125,\n",
      "         -4.8750, -6.7188, -7.4375, -6.5938, -6.7812, -6.2500, -7.5625, -6.9688,\n",
      "         -6.9062, -6.6250, -6.2188, -7.1875, -6.9375, -6.3750, -6.9375, -7.5000,\n",
      "         -7.2188, -7.2188, -7.2188, -7.0000, -5.0312, -5.8438, -6.2812, -6.7500,\n",
      "         -7.1875, -7.2188, -7.3125, -6.3438, -6.4688, -7.1875, -6.9375, -6.8750,\n",
      "         -6.2812, -6.4688, -7.1875, -6.8750, -8.0000, -8.0625, -4.8750, -2.5781,\n",
      "         -7.6875, -7.8750, -8.0000, -7.7500, -7.9375, -7.9688, -8.1250, -7.9688,\n",
      "         -7.6562, -8.0625, -7.2812, -6.4688, -7.5625, -7.5625, -8.1875, -8.0625,\n",
      "         -8.0625, -8.1250, -8.1250, -8.0000, -8.1250, -8.0625, -8.1250, -8.0625,\n",
      "         -7.9688, -8.1250, -8.0625, -8.0625, -7.9375, -8.0625, -4.7812, -7.0625,\n",
      "         -7.2812, -7.2188, -7.2188, -7.5625, -7.6250, -7.7188, -7.9062, -8.1250,\n",
      "         -7.9062, -8.0625, -8.1250, -7.0000, -6.0000, -8.1875, -8.0625, -8.0000,\n",
      "         -8.0000, -8.1250, -8.1250, -8.0625, -8.1250, -8.0000, -8.0625, -8.1250,\n",
      "         -8.0625, -8.0625, -8.0625, -8.0625]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 112\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  3]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 112])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 112, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8125,  0.5586,  0.5078,  ...,  0.3320, -1.1172, -2.2656],\n",
      "        [ 0.0039, -0.3438, -0.4863,  ...,  0.0879,  1.3281, -0.3262],\n",
      "        [ 0.6523, -0.8047,  0.9062,  ...,  1.2656,  0.1006, -0.8984]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [   -9.7500,  -360.0000,   210.0000,  ...,   920.0000,\n",
      "            237.0000,  -104.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  7200.,  -2624., -18304.,  ...,   -488.,   6720., -21504.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 112, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8125,  0.5586,  0.5078,  ...,  0.3320, -1.1172, -2.2656],\n",
      "        [ 0.0039, -0.3438, -0.4863,  ...,  0.0879,  1.3281, -0.3262],\n",
      "        [ 0.6523, -0.8047,  0.9062,  ...,  1.2656,  0.1006, -0.8984]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 112]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 112, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.6562, -6.0000, -1.5312, -7.6562, -7.7500, -7.6250, -7.7188, -7.6562,\n",
      "         -7.6562, -7.5312, -7.7188, -7.7188, -7.7812, -4.7812,  0.3086, -7.7188,\n",
      "         -7.7188, -7.5312, -7.7500, -7.6875, -7.5938, -7.6250, -7.6875, -7.5938,\n",
      "         -7.7188, -7.6562, -7.7812, -7.6875, -7.6250, -7.7500, -7.6875, -7.7188,\n",
      "         -7.7188, -7.6875, -7.6875, -7.7812,  0.7070, -2.7812, -1.9844, -5.0938,\n",
      "         -4.7812, -3.2969, -4.1875, -2.8438, -2.9844, -2.8125, -4.6250, -4.5000,\n",
      "         -0.3457, -1.9844, -0.4551, -2.0469, -2.4375, -2.6562, -3.2500, -3.5938,\n",
      "         -2.3125, -3.9531, -4.0312, -3.7812, -5.3438, -3.5781, -1.1641, -4.2500,\n",
      "         -2.6406, -2.9844, -3.7500, -2.2969, -5.4688, -3.7500, -3.4375, -3.0625,\n",
      "         -4.4688, -4.6562, -3.5938, -3.1406, -4.4688, -3.3281, -2.3594, -3.8594,\n",
      "         -5.0000, -3.9844, -0.1177, -3.8438, -2.4375, -2.6250, -4.4062, -3.3906,\n",
      "         -3.0781, -4.5625, -3.5938, -3.8750, -4.6250, -5.1250, -4.4688, -3.6094,\n",
      "         -5.0312, -4.2500, -2.9062, -2.3438, -5.6562, -1.0625,  0.7344,  1.7578,\n",
      "          0.5820, -1.1406, -1.1016,  1.6484, -0.5938,  0.5273, -0.5391,  2.0000,\n",
      "          1.7812, -0.2637, 11.3125, -1.2188,  0.2969, -0.3242,  0.8828,  1.4844,\n",
      "          2.1562, -0.6250,  1.2500, -0.9141, -1.7578, -1.2266, -0.1445, -3.6875,\n",
      "         -2.8125, -2.4375, -6.7188, -7.7188, -4.0938, -5.6562, -5.3750, -6.4375,\n",
      "         -6.4375, -6.6250, -6.0312, -6.8438, -5.0938, -4.9375, -6.2812, -5.9688,\n",
      "         -5.0312, -6.2500, -5.7500, -5.8125, -5.8750, -6.2812, -5.1250, -4.6875,\n",
      "         -4.4375, -6.2812, -6.7812, -4.8438, -4.3750, -4.3438, -5.5938, -6.4375,\n",
      "         -3.4844, -5.0938, -6.9375, -5.0938, -6.2500, -6.0000, -6.3438, -6.5938,\n",
      "         -6.2500, -5.7812, -5.7500, -6.4688, -6.2188, -5.5938, -6.2188, -7.0938,\n",
      "         -6.6875, -6.5000, -6.5312, -6.0625, -5.0938, -4.7812, -4.2188, -5.4062,\n",
      "         -6.4688, -6.7188, -6.5938, -5.4062, -5.8750, -6.6875, -6.4062, -6.0312,\n",
      "         -5.7500, -5.6875, -6.6562, -6.4688, -7.5938, -7.7500, -4.5625, -2.1719,\n",
      "         -7.3438, -7.2188, -7.5938, -7.2188, -7.4688, -7.6250, -7.5938, -7.4375,\n",
      "         -7.0625, -7.6875, -6.3438, -5.3438, -7.1562, -7.2188, -7.6875, -7.7188,\n",
      "         -7.7500, -7.7500, -7.5312, -7.7500, -7.7812, -7.8438, -7.5938, -7.5938,\n",
      "         -7.5625, -7.6562, -7.6562, -7.6562, -7.4688, -7.8125, -4.3438, -6.3125,\n",
      "         -6.8125, -6.6250, -6.5000, -6.8125, -7.0938, -7.3438, -7.5312, -7.7188,\n",
      "         -7.6250, -7.7188, -7.7188, -5.6875, -4.6562, -7.8438, -7.6875, -7.7188,\n",
      "         -7.5000, -7.7188, -7.7812, -7.7188, -7.7188, -7.6562, -7.7188, -7.7812,\n",
      "         -7.6875, -7.6562, -7.6875, -7.5938]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 113\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  4]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 113])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 113, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0039, -0.3438, -0.4863,  ...,  0.0879,  1.3281, -0.3262],\n",
      "        [ 0.6523, -0.8047,  0.9062,  ...,  1.2656,  0.1006, -0.8984],\n",
      "        [ 0.8750,  0.6250, -1.2266,  ...,  0.5391, -1.4688, -0.4102]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -27.8750,  -416.0000,   260.0000,  ...,   992.0000,\n",
      "            236.0000,  -124.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8000.,  -2944., -20352.,  ...,   -544.,   7392., -23808.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 113, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0039, -0.3438, -0.4863,  ...,  0.0879,  1.3281, -0.3262],\n",
      "        [ 0.6523, -0.8047,  0.9062,  ...,  1.2656,  0.1006, -0.8984],\n",
      "        [ 0.8750,  0.6250, -1.2266,  ...,  0.5391, -1.4688, -0.4102]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 113]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 113, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-9.1875e+00, -8.3750e+00, -2.3750e+00, -9.0625e+00, -9.2500e+00,\n",
      "         -9.1250e+00, -9.1250e+00, -9.1250e+00, -9.1250e+00, -9.0000e+00,\n",
      "         -9.1875e+00, -9.1875e+00, -9.1875e+00, -6.0625e+00, -1.1641e+00,\n",
      "         -9.0625e+00, -9.2500e+00, -8.8750e+00, -9.1875e+00, -9.1875e+00,\n",
      "         -9.0625e+00, -9.1250e+00, -9.1875e+00, -8.9375e+00, -9.1250e+00,\n",
      "         -9.1250e+00, -9.1250e+00, -9.2500e+00, -9.0625e+00, -9.0000e+00,\n",
      "         -9.1875e+00, -9.1875e+00, -9.0000e+00, -9.2500e+00, -9.1250e+00,\n",
      "         -9.2500e+00, -1.1816e-01, -3.3438e+00, -2.8750e+00, -4.6250e+00,\n",
      "         -5.7188e+00, -5.8750e+00, -4.8125e+00, -2.6406e+00, -2.1562e+00,\n",
      "         -5.9688e+00, -5.0312e+00, -5.3125e+00, -1.7500e+00, -1.1406e+00,\n",
      "         -2.6250e+00, -3.7031e+00, -3.8750e+00, -2.4531e+00, -3.5156e+00,\n",
      "         -3.4844e+00, -3.5000e+00, -3.1094e+00, -4.0625e+00, -3.9688e+00,\n",
      "         -4.5625e+00, -4.1250e+00, -3.8125e+00, -4.0000e+00, -4.3125e+00,\n",
      "         -3.9219e+00, -4.4062e+00, -5.0000e+00, -6.2812e+00, -1.1172e+00,\n",
      "         -4.1250e+00, -5.1562e+00, -5.9375e+00, -4.4062e+00, -4.7812e+00,\n",
      "         -4.2188e+00, -4.9062e+00, -2.9844e+00, -3.9062e+00, -4.5625e+00,\n",
      "         -4.2812e+00, -5.2188e+00, -3.9062e+00, -3.4375e+00, -5.1250e+00,\n",
      "         -4.4375e+00, -5.1562e+00, -3.8750e+00, -3.5000e+00, -4.4688e+00,\n",
      "         -4.5625e+00, -4.3125e+00, -5.5000e+00, -5.0625e+00, -5.5000e+00,\n",
      "         -4.1250e+00, -5.9062e+00, -6.0312e+00, -5.6250e+00, -3.4688e+00,\n",
      "         -4.7188e+00,  9.4375e+00, -4.5117e-01,  9.8145e-02,  1.3984e+00,\n",
      "          1.1133e-01,  3.4180e-03,  1.8203e+00, -1.8750e+00,  1.4141e+00,\n",
      "         -2.8320e-01,  1.0469e+00, -1.1172e+00, -1.2422e+00,  2.7656e+00,\n",
      "          7.8906e-01, -1.4766e+00, -4.9219e-01, -1.5156e+00,  1.2578e+00,\n",
      "          1.5547e+00,  2.3438e-01, -4.6289e-01,  2.9688e-01, -4.4375e+00,\n",
      "         -1.4160e-01, -3.3281e+00, -4.7500e+00, -3.0312e+00, -4.9375e+00,\n",
      "         -8.1875e+00, -9.1250e+00, -5.0625e+00, -6.8750e+00, -6.5938e+00,\n",
      "         -7.8438e+00, -7.6250e+00, -7.6875e+00, -7.1250e+00, -8.1875e+00,\n",
      "         -5.7500e+00, -6.5938e+00, -7.4375e+00, -7.0625e+00, -6.4688e+00,\n",
      "         -7.4688e+00, -7.3125e+00, -6.9688e+00, -7.4375e+00, -7.9375e+00,\n",
      "         -5.8750e+00, -4.0938e+00, -6.6875e+00, -7.5938e+00, -8.1875e+00,\n",
      "         -6.3750e+00, -6.3438e+00, -6.9688e+00, -5.6250e+00, -8.0625e+00,\n",
      "         -5.4688e+00, -6.5625e+00, -8.5000e+00, -6.0312e+00, -7.1250e+00,\n",
      "         -7.7188e+00, -7.4375e+00, -7.5625e+00, -7.1875e+00, -7.2500e+00,\n",
      "         -6.6562e+00, -7.8750e+00, -7.8125e+00, -7.0938e+00, -7.5938e+00,\n",
      "         -8.2500e+00, -8.1250e+00, -8.3125e+00, -8.0000e+00, -7.4375e+00,\n",
      "         -6.4375e+00, -5.7500e+00, -5.8125e+00, -7.2500e+00, -7.9062e+00,\n",
      "         -7.7812e+00, -8.0625e+00, -6.9688e+00, -6.5000e+00, -8.1250e+00,\n",
      "         -7.8125e+00, -7.7188e+00, -6.5938e+00, -6.4688e+00, -7.5625e+00,\n",
      "         -7.8750e+00, -9.1250e+00, -9.0000e+00, -4.8750e+00, -3.6250e+00,\n",
      "         -8.5000e+00, -8.5625e+00, -9.1250e+00, -8.8125e+00, -9.0625e+00,\n",
      "         -8.8750e+00, -9.1250e+00, -9.0000e+00, -8.1875e+00, -9.1250e+00,\n",
      "         -7.5625e+00, -6.5938e+00, -8.5625e+00, -8.5625e+00, -9.1875e+00,\n",
      "         -9.0625e+00, -9.1875e+00, -9.1875e+00, -9.2500e+00, -9.1250e+00,\n",
      "         -9.1250e+00, -9.1250e+00, -9.1250e+00, -9.1250e+00, -9.1250e+00,\n",
      "         -9.1250e+00, -9.0625e+00, -9.1250e+00, -8.9375e+00, -9.0000e+00,\n",
      "         -2.2656e+00, -7.5312e+00, -8.0000e+00, -7.9375e+00, -8.0625e+00,\n",
      "         -8.3125e+00, -8.5000e+00, -8.5000e+00, -9.0625e+00, -9.1875e+00,\n",
      "         -9.1250e+00, -9.1250e+00, -9.0625e+00, -7.1562e+00, -5.0312e+00,\n",
      "         -9.2500e+00, -9.0625e+00, -9.0625e+00, -9.1250e+00, -9.1250e+00,\n",
      "         -9.1875e+00, -9.1250e+00, -9.1250e+00, -9.1875e+00, -9.1250e+00,\n",
      "         -9.1250e+00, -9.0625e+00, -9.1250e+00, -9.0625e+00, -9.1875e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 114\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  5]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 114])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 114, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.6523, -0.8047,  0.9062,  ...,  1.2656,  0.1006, -0.8984],\n",
      "        [ 0.8750,  0.6250, -1.2266,  ...,  0.5391, -1.4688, -0.4102],\n",
      "        [ 0.8125,  0.7969,  0.1016,  ...,  0.4121,  0.1377,  0.8672]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -27.5000,  -416.0000,   260.0000,  ...,   984.0000,\n",
      "            239.0000,  -126.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8032.,  -2928., -20480.,  ...,   -544.,   7424., -23936.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 114, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.6523, -0.8047,  0.9062,  ...,  1.2656,  0.1006, -0.8984],\n",
      "        [ 0.8750,  0.6250, -1.2266,  ...,  0.5391, -1.4688, -0.4102],\n",
      "        [ 0.8125,  0.7969,  0.1016,  ...,  0.4121,  0.1377,  0.8672]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 114]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 114, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.0625, -4.7188,  0.1650, -5.9688, -6.0625, -6.0312, -6.1562, -6.0000,\n",
      "         -6.0312, -6.0312, -6.0625, -6.0938, -6.0625, -3.9062,  0.6719, -6.0938,\n",
      "         -6.0312, -5.8750, -6.0312, -6.0312, -6.0312, -6.0938, -6.0625, -5.9375,\n",
      "         -6.0312, -6.0000, -6.0312, -6.0938, -6.0312, -6.0312, -6.0938, -6.0000,\n",
      "         -6.0312, -6.0000, -6.0938, -6.0625,  2.3438, -1.6172, -1.8125, -4.8125,\n",
      "         -4.8125, -2.6250, -4.5000, -2.6094, -3.1719, -2.7812, -3.2656, -5.1250,\n",
      "          2.1406,  0.1846,  1.8672, -1.6250, -0.6094, -2.5781, -2.1875, -2.8750,\n",
      "         -1.4609, -2.8750, -2.3281, -1.4766, -3.0625, -2.2344, -0.6875, -3.5938,\n",
      "         -2.7812, -3.2031, -2.1562, -4.3438, -4.2500, -3.0625, -2.7969, -3.5000,\n",
      "         -3.8906, -3.1562, -2.7656, -3.3594, -3.5156, -0.8867, -3.0312, -4.7188,\n",
      "         -4.1250, -2.8281,  1.3281, -3.4531, -2.7500, -3.3125, -4.2188, -1.4531,\n",
      "         -3.5312, -4.6562, -2.2812, -2.5781, -3.2500, -4.5000, -3.9688, -4.1562,\n",
      "         -4.5312, -3.9531, -3.6719, -1.9531, -4.4062, -0.6719, -0.5000, -0.0483,\n",
      "         -0.3789,  1.4453,  0.1748, -1.4844, -0.4570,  0.2656, -1.9609, -1.2422,\n",
      "          0.8828,  0.0771, 11.8125, -0.2930, -1.2578, -0.1172, -2.0625,  0.6562,\n",
      "         -0.6719, -0.8906,  0.9961,  0.2246, -1.5000, -1.0156, -1.5312, -3.4531,\n",
      "         -1.3984, -3.6875, -5.5938, -6.1875, -4.8125, -4.6875, -4.6875, -5.4375,\n",
      "         -5.2188, -5.3750, -4.9375, -5.3125, -4.1250, -4.1875, -5.2188, -4.9375,\n",
      "         -4.7812, -5.2812, -4.6250, -5.1250, -4.7812, -5.1562, -4.5312, -3.8125,\n",
      "         -4.0938, -5.3438, -5.5625, -3.9688, -4.3438, -4.0312, -4.7812, -5.2812,\n",
      "         -2.9531, -4.4062, -5.7812, -4.3438, -5.3438, -5.3438, -5.1250, -5.3750,\n",
      "         -5.1562, -4.8125, -5.0938, -5.4062, -5.0312, -4.7188, -5.0000, -5.6250,\n",
      "         -5.3438, -5.1875, -5.2500, -5.0938, -5.1250, -4.3750, -3.2969, -4.6250,\n",
      "         -5.1875, -5.3125, -5.4062, -4.6875, -4.4688, -5.5312, -5.2188, -5.0312,\n",
      "         -4.8125, -4.8750, -5.2500, -5.1250, -5.9375, -5.9688, -4.5000, -3.4688,\n",
      "         -5.6562, -5.8438, -6.0625, -5.6875, -5.9375, -5.7812, -6.0000, -6.0000,\n",
      "         -5.5312, -6.0000, -5.0938, -4.7188, -5.9375, -5.8125, -6.1250, -5.9688,\n",
      "         -6.0625, -6.0312, -6.0938, -6.0938, -6.1562, -5.9688, -6.0000, -6.0312,\n",
      "         -5.9688, -6.0625, -5.9688, -6.0312, -5.8438, -6.1250, -3.4375, -5.1875,\n",
      "         -5.2812, -5.3438, -5.2188, -5.4688, -5.5625, -5.7812, -5.9375, -6.0000,\n",
      "         -5.9375, -6.0625, -6.0625, -4.6562, -3.1250, -6.0938, -6.0312, -6.0312,\n",
      "         -6.0000, -6.0625, -6.0312, -6.0625, -6.0625, -6.0938, -6.0938, -6.0625,\n",
      "         -6.0938, -6.0938, -6.0625, -6.0000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 115\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  6]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 115])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 115, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.8750,  0.6250, -1.2266,  ...,  0.5391, -1.4688, -0.4102],\n",
      "        [ 0.8125,  0.7969,  0.1016,  ...,  0.4121,  0.1377,  0.8672],\n",
      "        [ 0.4492,  0.4258, -0.4336,  ..., -0.8008,  0.7656,  0.5781]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -27.5000,  -416.0000,   256.0000,  ...,   984.0000,\n",
      "            239.0000,  -126.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8064.,  -2928., -20480.,  ...,   -544.,   7456., -24064.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 115, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.8750,  0.6250, -1.2266,  ...,  0.5391, -1.4688, -0.4102],\n",
      "        [ 0.8125,  0.7969,  0.1016,  ...,  0.4121,  0.1377,  0.8672],\n",
      "        [ 0.4492,  0.4258, -0.4336,  ..., -0.8008,  0.7656,  0.5781]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 115]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 115, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.0625, -6.9375, -1.7578, -7.9062, -8.0625, -8.0625, -7.9375, -7.9062,\n",
      "         -8.0625, -8.0000, -7.9375, -7.9062, -7.9062, -5.9375, -1.1172, -7.9688,\n",
      "         -8.0000, -7.8438, -7.9688, -7.9375, -7.9688, -7.9688, -7.9062, -7.9375,\n",
      "         -7.9688, -7.9688, -8.0000, -7.9375, -7.9375, -7.8750, -8.0000, -7.9062,\n",
      "         -7.9688, -8.0000, -7.9688, -8.0625,  0.8320, -3.2031, -1.8906, -6.3438,\n",
      "         -6.5312, -5.2500, -5.1250, -2.6875, -3.0312, -4.0000, -4.8125, -5.3438,\n",
      "          1.2031, -1.5781, -0.3457, -2.5625, -3.4688, -3.6875, -2.9531, -3.7812,\n",
      "         -3.7656, -3.1875, -4.1875, -3.9219, -4.0938, -4.3750, -2.8438, -3.2031,\n",
      "         -5.4062, -4.0625, -4.6875, -5.1875, -5.4375, -1.1406, -3.7500, -6.0312,\n",
      "         -5.2188, -3.7031, -4.5000, -6.0938, -4.9062, -3.4375, -4.6250, -5.8125,\n",
      "         -3.8906, -4.3750, -4.6562, -4.3750, -6.2188, -6.5625, -6.1250, -4.7812,\n",
      "         -3.8906, -4.8750, -5.4375, -5.3438, -5.0625, -5.7500, -5.6875, -4.3125,\n",
      "         -6.1875, -5.9062, -6.0938, -3.6250, -4.2188,  9.8125, -1.1953, -1.7109,\n",
      "          0.7227,  4.6250, -0.6992, -3.0625, -1.6406,  2.5000, -3.5625, -2.1406,\n",
      "         -1.2422,  0.3242,  1.5625,  0.4551, -2.8750, -2.0625, -0.8906, -0.5078,\n",
      "          2.3594,  0.7852, -1.7031, -1.9375, -1.9062,  0.2236, -3.0781, -4.1875,\n",
      "         -3.6406, -3.7500, -7.5000, -8.0625, -6.6250, -6.5938, -6.1562, -7.1875,\n",
      "         -7.0625, -7.2812, -6.7812, -7.5000, -6.1875, -6.0312, -7.3125, -7.1875,\n",
      "         -6.0625, -6.7812, -6.7188, -6.4688, -6.6875, -7.1562, -6.0312, -5.1250,\n",
      "         -5.9688, -7.0312, -7.4688, -5.7500, -6.6875, -6.5625, -5.5938, -7.2500,\n",
      "         -5.9062, -7.0625, -7.6250, -6.1250, -6.5000, -6.9062, -7.2812, -6.9375,\n",
      "         -7.0938, -7.0000, -5.9688, -7.3438, -7.0000, -6.6250, -6.8750, -7.4688,\n",
      "         -7.3750, -7.2188, -7.1875, -6.8750, -5.8438, -5.3438, -6.0312, -7.0000,\n",
      "         -7.2812, -7.1250, -7.1250, -6.2500, -5.7500, -7.2188, -7.0938, -7.0938,\n",
      "         -6.3438, -6.5312, -6.8750, -7.0312, -7.9375, -7.8438, -5.5938, -2.3438,\n",
      "         -7.5938, -7.7188, -7.9688, -7.8125, -8.0000, -7.7812, -7.9688, -8.0000,\n",
      "         -7.5312, -7.9688, -7.0625, -6.4062, -7.6562, -7.5938, -7.9375, -7.9375,\n",
      "         -7.9375, -8.0625, -8.0000, -7.9688, -8.0000, -7.9062, -7.9062, -7.9688,\n",
      "         -8.0625, -8.0000, -7.8438, -8.0000, -7.7812, -7.9375, -3.8594, -7.1250,\n",
      "         -7.2812, -7.5000, -7.2500, -7.5312, -7.5625, -7.8125, -7.8125, -8.1250,\n",
      "         -7.9062, -7.9375, -8.0000, -6.7812, -4.7188, -7.9062, -7.9688, -7.9062,\n",
      "         -8.0625, -8.0000, -8.0000, -7.9688, -7.9062, -8.0625, -7.9688, -7.9375,\n",
      "         -8.0625, -8.0000, -8.0000, -7.9375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 116\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  7]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 116])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 116, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.8125,  0.7969,  0.1016,  ...,  0.4121,  0.1377,  0.8672],\n",
      "        [ 0.4492,  0.4258, -0.4336,  ..., -0.8008,  0.7656,  0.5781],\n",
      "        [ 0.3477,  0.9102,  0.0156,  ...,  0.1719, -0.2031,  1.6484]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -27.0000,  -416.0000,   256.0000,  ...,   984.0000,\n",
      "            240.0000,  -124.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8064.,  -2928., -20480.,  ...,   -544.,   7456., -24064.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 116, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.8125,  0.7969,  0.1016,  ...,  0.4121,  0.1377,  0.8672],\n",
      "        [ 0.4492,  0.4258, -0.4336,  ..., -0.8008,  0.7656,  0.5781],\n",
      "        [ 0.3477,  0.9102,  0.0156,  ...,  0.1719, -0.2031,  1.6484]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 116]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 116, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.5000, -4.8750,  4.5312, -6.4062, -6.5625, -6.5625, -6.6562, -6.5000,\n",
      "         -6.6250, -6.5625, -6.4062, -6.5312, -6.5312, -3.5156,  4.5938, -6.5312,\n",
      "         -6.4688, -6.2188, -6.5312, -6.5625, -6.4062, -6.6250, -6.4375, -6.4688,\n",
      "         -6.5000, -6.5312, -6.5938, -6.5312, -6.5312, -6.4688, -6.5000, -6.4375,\n",
      "         -6.5625, -6.4688, -6.6250, -6.5000,  5.1250,  1.8203, -0.0664, -3.2031,\n",
      "         -3.0312, -3.4219, -3.9844, -0.5391, -1.5078, -1.1094, -1.7266, -4.4375,\n",
      "          8.1875,  1.1562,  4.0312, -0.7969, -0.8867, -1.0547, -0.8008, -1.3281,\n",
      "         -0.8555, -0.8438, -0.8906, -0.7070, -1.1641, -1.8047, -0.9336,  1.1094,\n",
      "         -2.5625, -1.7969, -2.1719, -2.2656, -2.7812, -2.8125, -2.2969, -3.1562,\n",
      "         -3.4844, -2.5938, -2.9531, -2.4844, -3.1875, -1.3125, -2.8125, -3.9688,\n",
      "         -2.9062, -2.7500, -1.4531, -2.5938, -3.5625, -3.8750, -2.8281, -1.6641,\n",
      "         -3.0469, -3.5781, -2.2969, -2.2812, -3.0000, -3.8750, -3.0781, -3.1875,\n",
      "         -3.7188, -0.8477, -4.0625, -1.4766, -3.2812, -1.6641, -0.6328, -1.4531,\n",
      "          0.0267,  0.1045, -0.5117, -1.3984, -1.0469, -0.8281, -1.2656, -1.7969,\n",
      "          0.8945, -0.5547,  1.0312, -0.7734, -1.7891, -1.7344,  0.0170,  4.9375,\n",
      "         -0.3008, -2.2500, -0.5312, -1.6797, -0.9531, -0.6016, -2.1094, -2.7812,\n",
      "         -1.3906, -1.1016, -6.0000, -6.7188, -3.0000, -4.2188, -4.1875, -5.0938,\n",
      "         -5.0938, -5.0312, -4.9688, -5.6875, -3.6719, -3.8438, -4.6250, -4.8438,\n",
      "         -3.9375, -4.1875, -4.3125, -4.9062, -4.6562, -5.2188, -4.4375, -2.4844,\n",
      "         -3.2031, -5.6875, -5.5938, -3.6406, -3.9219, -3.4375, -4.1250, -5.1250,\n",
      "         -2.9531, -4.9062, -6.0625, -4.4688, -4.9062, -4.8750, -4.6875, -5.6875,\n",
      "         -5.0312, -4.6875, -4.5312, -5.0000, -4.9375, -3.7656, -5.0312, -5.8438,\n",
      "         -5.2188, -5.6250, -5.4375, -4.8125, -4.8125, -4.3438, -4.5000, -4.9375,\n",
      "         -5.4688, -5.3125, -5.3750, -4.5625, -4.1562, -5.6875, -5.2500, -5.1250,\n",
      "         -4.1875, -4.5312, -4.9688, -4.9688, -6.4062, -6.4062, -3.0781, -3.1719,\n",
      "         -5.8438, -6.1250, -6.5625, -6.0938, -6.4375, -6.3438, -6.5312, -6.5000,\n",
      "         -5.9062, -6.3438, -4.8750, -4.3438, -6.0625, -5.9688, -6.5938, -6.5000,\n",
      "         -6.4688, -6.5625, -6.5938, -6.5312, -6.5938, -6.4375, -6.4375, -6.4688,\n",
      "         -6.5625, -6.5000, -6.2812, -6.4062, -6.4375, -6.5312, -0.9414, -5.0000,\n",
      "         -5.3125, -5.2500, -5.2500, -5.5938, -5.7812, -5.9688, -6.4062, -6.4375,\n",
      "         -6.3750, -6.5625, -6.4375, -4.6875, -2.5312, -6.5312, -6.4688, -6.4688,\n",
      "         -6.4688, -6.5312, -6.5000, -6.5312, -6.5312, -6.5938, -6.5625, -6.4375,\n",
      "         -6.5625, -6.5312, -6.5625, -6.5000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 117\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 120, 117])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 117, 256]), Cross output shape: torch.Size([1, 120, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.4492,  0.4258, -0.4336,  ..., -0.8008,  0.7656,  0.5781],\n",
      "        [ 0.3477,  0.9102,  0.0156,  ...,  0.1719, -0.2031,  1.6484],\n",
      "        [-0.3750,  1.1562,  0.0361,  ..., -0.6875, -1.0938, -1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 15, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [ -154.0000,  -568.0000,   456.0000,  ...,  1264.0000,\n",
      "            316.0000,  -216.0000],\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 15, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [ 10880.,  -3696., -28032.,  ...,   -960.,   9984., -32384.],\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 117, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.4492,  0.4258, -0.4336,  ..., -0.8008,  0.7656,  0.5781],\n",
      "        [ 0.3477,  0.9102,  0.0156,  ...,  0.1719, -0.2031,  1.6484],\n",
      "        [-0.3750,  1.1562,  0.0361,  ..., -0.6875, -1.0938, -1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 117]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 117, 120])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.2812, -8.6250,  1.7188, -7.3125, -7.3125, -7.4062, -7.2500, -7.1875,\n",
      "         -7.4062, -7.2812, -7.2500, -7.4062, -7.3438, -4.8438,  4.9688, -7.3750,\n",
      "         -7.2188, -7.1250, -7.3438, -7.2188, -7.2812, -7.2812, -7.2500, -7.2500,\n",
      "         -7.3125, -7.3438, -7.3750, -7.3125, -7.3125, -7.3438, -7.3125, -7.2188,\n",
      "         -7.1562, -7.4375, -7.3438, -7.3438,  9.5625, -0.3203, -0.3027, -2.9688,\n",
      "         -2.9375, -5.9688, -3.1094, -1.7734, -1.6016, -2.0156, -3.0000, -3.5312,\n",
      "          1.3672, -1.0938,  1.0312, -0.4668, -3.3438, -2.3750, -1.4766, -3.5312,\n",
      "         -3.3594, -3.6094, -4.2188, -4.4062, -5.2812, -3.9375, -0.1416, -2.5625,\n",
      "         -2.3594, -1.6641, -2.3281, -2.0781, -5.0000, -3.2656, -2.8594, -1.7188,\n",
      "         -3.0938, -3.5312, -3.5312, -3.9844, -3.4219, -2.1250, -3.2812, -4.0625,\n",
      "         -3.9062, -2.0469, -2.7188, -2.3594, -2.1406, -3.6719, -1.6016, -1.3672,\n",
      "         -2.3281, -2.9062, -1.8047, -2.1406, -4.6875, -3.5312, -3.5469, -2.9062,\n",
      "         -5.3125, -2.5156, -3.7344, -2.7500, -5.8438, -1.1172, -0.9375,  0.1445,\n",
      "         -0.7930, -1.4141, -1.9453, -1.5391, -1.1328, -0.9141, -1.2891, -0.5391,\n",
      "         -0.2832, -0.8398,  0.7578, -0.4453, -0.7070, -3.5625, -1.1562,  1.0859,\n",
      "          0.4766, -0.7188, -2.3750, -0.6406, -4.9375, -0.5664, -2.4375, -4.3750,\n",
      "         -2.3906, -2.6562, -6.5938, -7.3438, -5.3438, -5.8750, -6.0000, -6.2812,\n",
      "         -6.2812, -6.3438, -6.0312, -6.8438, -4.8438, -5.2812, -5.9688, -6.0000,\n",
      "         -5.7500, -5.6875, -6.1875, -5.3438, -6.0000, -6.4375, -4.9688, -4.6875,\n",
      "         -4.9062, -6.1875, -6.4062, -4.7812, -4.6250, -5.1875, -5.0938, -6.3125,\n",
      "         -3.6406, -4.7500, -6.6875, -6.1250, -6.0938, -5.7500, -6.1250, -6.4062,\n",
      "         -5.9062, -6.0312, -5.8438, -5.7188, -5.9688, -4.6875, -6.1250, -6.7188,\n",
      "         -6.5000, -6.4062, -6.1562, -5.9375, -5.4688, -5.0625, -4.8438, -5.6562,\n",
      "         -6.5000, -6.3438, -6.4688, -5.6250, -5.8438, -6.2188, -6.1562, -5.8750,\n",
      "         -5.0938, -4.7188, -6.1562, -6.3125, -7.2188, -7.3438, -2.8594, -2.2656,\n",
      "         -6.7500, -6.9062, -7.2500, -7.0312, -7.2812, -7.2500, -7.3750, -7.1250,\n",
      "         -6.8750, -7.3125, -6.1250, -5.7500, -6.6875, -6.6562, -7.2500, -7.3125,\n",
      "         -7.4375, -7.3750, -7.2188, -7.2812, -7.2500, -7.2500, -7.3125, -7.3125,\n",
      "         -7.2188, -7.3125, -7.2812, -7.2812, -7.2188, -7.2812, -2.1875, -6.1562,\n",
      "         -6.5625, -6.4688, -6.5625, -6.8438, -7.0312, -6.9688, -7.2188, -7.1875,\n",
      "         -7.0312, -7.3750, -7.2812, -6.0312, -4.9688, -7.3750, -7.3125, -7.1875,\n",
      "         -7.2812, -7.2500, -7.3125, -7.2500, -7.3125, -7.3125, -7.3125, -7.3125,\n",
      "         -7.4062, -7.4062, -7.3438, -7.3438]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 118\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 118])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 118, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.3477,  0.9102,  0.0156,  ...,  0.1719, -0.2031,  1.6484],\n",
      "        [-0.3750,  1.1562,  0.0361,  ..., -0.6875, -1.0938, -1.4219],\n",
      "        [-0.4902,  1.0703,  1.7969,  ...,  0.1455, -0.2246, -0.7305]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  103.0000,   -95.0000,   -61.5000,  ...,   322.0000,\n",
      "             33.7500,   -15.7500]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928.0000, -37120.0000,  31488.0000,  ...,   4512.0000,\n",
      "           -9728.0000,  -5952.0000],\n",
      "         [ 21632.0000, -11584.0000, -56576.0000,  ...,  -1032.0000,\n",
      "           18944.0000, -68608.0000],\n",
      "         [  3040.0000, -14208.0000,  -7936.0000,  ...,   -996.0000,\n",
      "            4224.0000, -22656.0000],\n",
      "         ...,\n",
      "         [  8448.0000,  -2960.0000, -21760.0000,  ...,   -644.0000,\n",
      "            7872.0000, -25344.0000],\n",
      "         [  8832.0000,  -3056.0000, -22656.0000,  ...,   -692.0000,\n",
      "            8192.0000, -26240.0000],\n",
      "         [  2208.0000,  -1512.0000,  -5280.0000,  ...,    109.5000,\n",
      "            2272.0000,  -7424.0000]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 118, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.3477,  0.9102,  0.0156,  ...,  0.1719, -0.2031,  1.6484],\n",
      "        [-0.3750,  1.1562,  0.0361,  ..., -0.6875, -1.0938, -1.4219],\n",
      "        [-0.4902,  1.0703,  1.7969,  ...,  0.1455, -0.2246, -0.7305]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 118]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 15]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 118, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.5625,  -9.3125,  -5.3750, -10.5625, -10.5625, -10.6250, -10.5625,\n",
      "         -10.5000, -10.5625, -10.5625, -10.5625, -10.6250, -10.6250,  -6.6875,\n",
      "          -1.6250, -10.6875, -10.6250, -10.1250, -10.5625, -10.6250, -10.5000,\n",
      "         -10.5625, -10.5625, -10.6875, -10.5000, -10.5625, -10.7500, -10.5625,\n",
      "         -10.6250, -10.5000, -10.5625, -10.5625, -10.4375, -10.6250, -10.6250,\n",
      "         -10.6250,  -3.9375,  -6.1875,  -2.2656,  -4.1875,  -6.5938,  -6.3750,\n",
      "          -5.6250,  -4.5938,  -2.9219,  -7.4375,  -6.3438,  -5.0000,  -4.6562,\n",
      "          -2.9688,  -3.7969,  -3.0000,  -5.6875,  -4.0625,  -4.5938,  -5.4062,\n",
      "          -5.0000,  -5.5938,  -6.1250,  -6.0938,  -5.6875,  -5.4062,  -5.1562,\n",
      "          -5.3750,  -5.6562,  -6.0312,  -6.2500,  -8.2500,  -6.9062,  -2.4219,\n",
      "          -2.8594,  -3.1719,  -4.8438,  -3.5000,  -3.4375,  -4.1250,  -3.8906,\n",
      "          -1.2031,  -2.0781,  -4.3750,  -3.3594,  -2.5156,  -4.4688,  -4.0312,\n",
      "          -0.1328,  -5.4688,  -2.9531,  -1.8594,  -3.1719,  -3.8281,  -5.1562,\n",
      "          -4.6875,  -6.2500,  -5.9688,  -6.0000,  -4.2188,  -8.0625,  -6.6875,\n",
      "          -6.2500,  -3.7188,  -7.0625,   2.7812,   2.6719,   1.8672,   0.3301,\n",
      "           0.2188,   0.8477,   0.7734,   1.2188,   0.1426,   0.3105,  -0.2812,\n",
      "           0.6484,   1.3281,  -0.5586,   1.4219,   5.2812,  -1.4688,   0.6250,\n",
      "           1.4922,   1.5312,   0.1118,  -0.3164,   0.0337,  -7.1875,  -0.7500,\n",
      "          -2.3125,  -4.9375,  -5.6562,  -5.5625,  -9.3750, -10.6250,  -6.5312,\n",
      "          -8.4375,  -8.1875,  -9.3750,  -9.1250,  -9.1875,  -8.3750, -10.0000,\n",
      "          -7.5938,  -7.8125,  -9.1250,  -8.6250,  -8.6250,  -9.1250,  -9.0000,\n",
      "          -8.4375,  -9.0625,  -9.3750,  -7.7500,  -6.5938,  -7.7500,  -9.5000,\n",
      "          -9.4375,  -7.9062,  -7.8438,  -8.7500,  -7.3438,  -9.5000,  -6.9062,\n",
      "          -7.1875,  -9.8125,  -8.5625,  -8.0625,  -8.4375,  -8.6875,  -9.3125,\n",
      "          -8.8125,  -8.4375,  -8.0625,  -8.9375,  -9.0625,  -6.4062,  -9.0000,\n",
      "          -9.5625,  -9.6250,  -9.6875,  -9.3125,  -8.8750,  -6.7812,  -7.5625,\n",
      "          -8.1875,  -8.1250,  -9.5625,  -9.4375,  -9.5000,  -8.2500,  -8.4375,\n",
      "          -9.5000,  -9.2500,  -9.0000,  -7.6875,  -7.5312,  -9.0000,  -9.1875,\n",
      "         -10.5000, -10.5625,  -5.9688,  -4.3750,  -9.6875,  -9.8750, -10.5000,\n",
      "         -10.1875, -10.4375, -10.1875, -10.5625, -10.3125, -10.0625, -10.5000,\n",
      "          -9.0625,  -8.3125,  -9.8750,  -9.6250, -10.5625, -10.6250, -10.6875,\n",
      "         -10.5625, -10.5625, -10.6250, -10.5625, -10.5625, -10.5625, -10.5625,\n",
      "         -10.5000, -10.6250, -10.5625, -10.6250, -10.3750, -10.5625,  -4.5625,\n",
      "          -9.0000,  -9.6875,  -9.3750,  -9.4375,  -9.7500,  -9.8750, -10.2500,\n",
      "         -10.3125, -10.5625, -10.4375, -10.5625, -10.6250,  -9.0000,  -7.0938,\n",
      "         -10.5625, -10.6250, -10.5625, -10.5000, -10.5625, -10.5625, -10.6875,\n",
      "         -10.5625, -10.5625, -10.5625, -10.6875, -10.6250, -10.6250, -10.5625,\n",
      "         -10.6250]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 119\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  2]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 119])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 119, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3750,  1.1562,  0.0361,  ..., -0.6875, -1.0938, -1.4219],\n",
      "        [-0.4902,  1.0703,  1.7969,  ...,  0.1455, -0.2246, -0.7305],\n",
      "        [-0.2812, -0.4688,  3.2500,  ...,  0.8906,  1.4453,  0.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [   -9.2500,  -245.0000,    90.5000,  ...,   816.0000,\n",
      "            129.0000,   -54.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  5440.,  -2064., -13632.,  ...,   -239.,   5120., -16320.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 119, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3750,  1.1562,  0.0361,  ..., -0.6875, -1.0938, -1.4219],\n",
      "        [-0.4902,  1.0703,  1.7969,  ...,  0.1455, -0.2246, -0.7305],\n",
      "        [-0.2812, -0.4688,  3.2500,  ...,  0.8906,  1.4453,  0.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 119]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 15]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 119, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.6562, -7.5938, -3.1875, -5.6562, -5.6562, -5.5938, -5.6250, -5.6875,\n",
      "         -5.6562, -5.5312, -5.6250, -5.6250, -5.6562, -4.4062, -2.0625, -5.6250,\n",
      "         -5.5625, -5.4688, -5.6250, -5.5625, -5.5625, -5.6250, -5.4375, -5.6250,\n",
      "         -5.5938, -5.5312, -5.6250, -5.6250, -5.6250, -5.5312, -5.6250, -5.5312,\n",
      "         -5.5312, -5.6250, -5.5312, -5.6250, -1.3984, -4.2500, -3.5781, -3.8438,\n",
      "         -4.3125, -4.2188, -4.4062, -3.3438, -3.1719, -3.3750, -4.3438, -5.0938,\n",
      "         -1.5938, -2.8125, -0.7266, -3.6875, -4.1562, -4.3125, -3.6250, -3.9219,\n",
      "         -3.9375, -5.4688, -3.9531, -5.1250, -4.4688, -5.5000, -3.3125, -2.8906,\n",
      "         -5.1250, -4.1562, -2.0312, -4.4375, -4.3438, -3.7969, -5.7188, -4.8125,\n",
      "         -5.0312, -1.0859, -5.0625, -4.6250, -3.6094, -3.9531, -5.3438, -5.5312,\n",
      "         -4.4375, -5.0000, -4.6875, -4.1250, -3.9844, -6.5000, -4.9688, -4.7500,\n",
      "         -4.2188, -3.4688, -5.3750, -4.5938, -5.1250, -4.5625, -4.8125, -3.9062,\n",
      "         -4.9062, -5.1562, -5.2188, -3.7969, -5.7500,  3.5938, -1.7891, -1.2500,\n",
      "         -1.1172, 10.2500, -2.7812, -1.3750,  2.3906,  4.2188, -3.8125, -2.1406,\n",
      "          2.7344, -1.5000, -0.9062,  4.1875, -0.5859, -3.7031,  2.9531, -0.5273,\n",
      "         -1.2734,  4.1875, -2.5000, -1.5938, -4.0312,  1.3516, -1.3516, -3.9531,\n",
      "         -4.6562, -3.2969, -5.2812, -5.6562, -3.4062, -4.8438, -4.8438, -5.3750,\n",
      "         -5.3438, -5.3438, -5.1250, -5.3438, -5.1250, -5.0312, -5.1250, -5.0312,\n",
      "         -5.1875, -5.0312, -5.0000, -5.3438, -5.0625, -5.0625, -5.0312, -6.0000,\n",
      "         -4.8125, -5.2188, -5.2500, -5.1875, -5.2812, -4.4375, -4.2188, -5.3438,\n",
      "         -4.7188, -3.5938, -5.3125, -5.0000, -4.7500, -5.1562, -5.2188, -5.3438,\n",
      "         -5.2500, -4.7500, -4.5938, -5.4062, -4.9688, -5.0312, -5.1562, -5.3750,\n",
      "         -5.2812, -5.2500, -5.0625, -5.2812, -4.3125, -4.5000, -4.0938, -4.7812,\n",
      "         -5.2812, -5.2188, -5.1250, -4.8750, -5.1562, -5.2812, -5.1562, -5.2188,\n",
      "         -4.7188, -4.8438, -5.0000, -4.9062, -5.5312, -5.4688, -3.9375,  0.0294,\n",
      "         -5.3438, -5.4688, -5.5938, -5.5000, -5.5312, -5.5625, -5.6875, -5.6562,\n",
      "         -5.4688, -5.5938, -5.3438, -4.8438, -5.4062, -5.2500, -5.6250, -5.5938,\n",
      "         -5.6562, -5.6250, -5.6250, -5.6875, -5.6250, -5.6562, -5.6250, -5.5938,\n",
      "         -5.6250, -5.6875, -5.5938, -5.5938, -5.5312, -5.6562, -4.5312, -5.2812,\n",
      "         -5.3438, -5.0938, -5.3438, -5.3438, -5.3125, -5.5312, -5.4375, -5.6562,\n",
      "         -5.5312, -5.6562, -5.6250, -5.1250, -4.1562, -5.5938, -5.6875, -5.5938,\n",
      "         -5.5938, -5.6250, -5.5938, -5.6562, -5.5938, -5.5938, -5.6250, -5.6875,\n",
      "         -5.6250, -5.5625, -5.6875, -5.5938]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 120\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  3]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 120])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 120, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.4902,  1.0703,  1.7969,  ...,  0.1455, -0.2246, -0.7305],\n",
      "        [-0.2812, -0.4688,  3.2500,  ...,  0.8906,  1.4453,  0.1406],\n",
      "        [ 0.0137,  0.5859, -0.2734,  ...,  0.8359, -0.8945, -1.4297]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -49.5000,  -316.0000,   164.0000,  ...,   896.0000,\n",
      "            157.0000,  -110.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  6592.,  -2432., -16768.,  ...,   -362.,   6144., -19712.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 120, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.4902,  1.0703,  1.7969,  ...,  0.1455, -0.2246, -0.7305],\n",
      "        [-0.2812, -0.4688,  3.2500,  ...,  0.8906,  1.4453,  0.1406],\n",
      "        [ 0.0137,  0.5859, -0.2734,  ...,  0.8359, -0.8945, -1.4297]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 120]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 15]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 120, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-9.1250, -9.8125, -4.2188, -9.1250, -9.2500, -9.0625, -9.1250, -9.1250,\n",
      "         -9.2500, -9.1875, -9.2500, -9.2500, -9.1875, -5.8750, -2.0469, -9.1875,\n",
      "         -9.3125, -8.9375, -9.1875, -9.1250, -9.1250, -9.1875, -9.0625, -9.0625,\n",
      "         -9.0625, -9.1875, -9.0625, -9.1250, -9.1250, -9.1250, -9.1250, -9.1250,\n",
      "         -9.0625, -9.1250, -9.1250, -9.3125, -0.6133, -4.9062, -2.3438, -6.0000,\n",
      "         -6.2188, -4.9688, -5.4688, -3.0000, -4.0000, -4.2500, -5.2188, -5.2500,\n",
      "         -0.2656, -1.3281, -0.1680, -3.0312, -3.4844, -3.9062, -4.3750, -4.5000,\n",
      "         -3.7344, -5.3438, -4.8125, -3.6250, -5.7812, -4.9375, -2.6250, -3.7344,\n",
      "         -5.3750, -4.4688, -3.4219, -4.3438, -6.5312, -4.1250, -5.2812, -4.2812,\n",
      "         -5.8750, -5.0938, -5.4375, -4.4375, -5.1562, -4.6875, -4.6875, -5.7500,\n",
      "         -5.5000, -4.4062, -1.1172, -4.7500, -3.3906, -4.8438, -5.4062, -3.4531,\n",
      "         -3.9062, -5.3125, -5.7812, -4.7188, -4.3438, -6.0625, -5.8750, -5.6250,\n",
      "         -6.9688, -5.7188, -4.8438, -4.7188, -5.4688,  3.9375, -0.9297,  1.4766,\n",
      "         -0.1816,  2.4062, -1.8594, -0.0947, -1.3281, -1.0469, -3.1719, -0.8281,\n",
      "          1.5703, -0.6602, 10.0000,  1.5547,  2.7812, -1.0391,  2.9688,  0.4434,\n",
      "          2.0156, -0.2471, -1.0391, -0.5117, -0.7383, -2.7188, -2.1094, -4.1562,\n",
      "         -4.3125, -4.4062, -8.5625, -9.1875, -5.1875, -6.8438, -6.5000, -8.0000,\n",
      "         -7.9688, -8.2500, -7.3438, -8.1250, -6.0312, -6.4375, -7.5625, -7.1875,\n",
      "         -6.9062, -7.2500, -7.1250, -7.1875, -7.3438, -7.8125, -6.2188, -5.8438,\n",
      "         -6.5625, -7.4688, -8.0000, -6.8750, -6.6562, -6.0000, -6.2188, -7.8125,\n",
      "         -6.0938, -7.2812, -8.3750, -6.4375, -6.7812, -7.2812, -7.6562, -7.8125,\n",
      "         -7.3438, -7.1250, -6.8438, -7.8438, -7.2188, -6.3125, -7.4688, -8.2500,\n",
      "         -8.1875, -7.9375, -7.6875, -7.4062, -7.5938, -6.7188, -5.5938, -6.9688,\n",
      "         -7.7500, -7.9688, -8.1875, -7.1562, -6.6562, -8.2500, -7.6875, -7.4375,\n",
      "         -7.0000, -6.8750, -7.5625, -7.6562, -9.1250, -9.1875, -6.0625, -2.8125,\n",
      "         -8.4375, -8.4375, -9.0625, -8.8750, -9.1250, -9.1250, -9.1250, -9.0000,\n",
      "         -8.5000, -9.1250, -8.0625, -6.9375, -8.6875, -8.7500, -9.2500, -9.1875,\n",
      "         -9.3125, -9.1250, -9.1875, -9.1250, -9.0625, -9.1875, -9.1250, -9.1250,\n",
      "         -9.1250, -9.1250, -9.1875, -9.2500, -8.8125, -9.2500, -4.1875, -7.6250,\n",
      "         -8.1250, -7.7500, -8.1250, -8.1250, -8.3125, -8.6875, -9.0000, -9.1875,\n",
      "         -9.1250, -9.1250, -9.1250, -7.3438, -5.7500, -9.1875, -9.1875, -9.1250,\n",
      "         -9.1250, -9.1250, -9.2500, -9.0625, -9.1250, -9.1875, -9.1875, -9.1875,\n",
      "         -9.1875, -9.1875, -9.1250, -9.1250]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 121\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  4]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 121])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 121, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2812, -0.4688,  3.2500,  ...,  0.8906,  1.4453,  0.1406],\n",
      "        [ 0.0137,  0.5859, -0.2734,  ...,  0.8359, -0.8945, -1.4297],\n",
      "        [ 2.5781,  0.1504, -2.1406,  ...,  1.1250, -0.7148, -1.5625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -45.5000,  -372.0000,   230.0000,  ...,   960.0000,\n",
      "            192.0000,  -135.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  7424.,  -2704., -18816.,  ...,   -454.,   6880., -22144.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 121, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2812, -0.4688,  3.2500,  ...,  0.8906,  1.4453,  0.1406],\n",
      "        [ 0.0137,  0.5859, -0.2734,  ...,  0.8359, -0.8945, -1.4297],\n",
      "        [ 2.5781,  0.1504, -2.1406,  ...,  1.1250, -0.7148, -1.5625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 121]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 15]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 121, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.6250, -7.5625,  1.6250, -8.6875, -8.6250, -8.6250, -8.6250, -8.6875,\n",
      "         -8.6875, -8.6875, -8.7500, -8.8125, -8.7500, -5.1562,  2.2500, -8.6875,\n",
      "         -8.6875, -8.5000, -8.7500, -8.7500, -8.6250, -8.6875, -8.6875, -8.6250,\n",
      "         -8.5625, -8.6875, -8.6250, -8.6250, -8.6250, -8.6875, -8.6875, -8.7500,\n",
      "         -8.6250, -8.6250, -8.6250, -8.6875,  4.2500,  0.0376, -1.4453, -3.6094,\n",
      "         -3.8750, -4.3438, -4.3125, -1.2500, -1.4453, -2.2031, -0.4258, -3.7344,\n",
      "          3.9219,  0.2656,  2.0312, -1.7188, -2.0312, -2.1875, -3.4531, -3.1719,\n",
      "         -1.5703, -2.3750, -2.7656, -2.4219, -4.4375, -1.6172, -0.2969, -1.1016,\n",
      "         -1.6094, -2.0312, -4.3750, -1.2188, -4.4062, -2.2656, -2.9688,  0.9102,\n",
      "         -4.0000, -4.7500, -3.6094, -2.5312, -3.3438, -0.6836, -2.4219, -3.9844,\n",
      "         -5.2812, -4.5625, -3.7344, -2.4219, -4.4688, -2.3125, -3.0625, -2.9219,\n",
      "         -2.6875, -2.7031, -3.4219, -3.7969, -3.8438, -3.7500, -4.1562, -3.7500,\n",
      "         -4.4062, -2.9062, -0.6836, -2.6875, -2.4844,  0.0120, -2.2500,  8.5625,\n",
      "          0.3633,  0.2021, -0.8398,  2.1719, -0.0991,  1.0938,  1.5547, -0.2617,\n",
      "         -2.7344, -2.6250,  1.9531, -0.4961, -2.7031,  0.0223, -0.9766,  1.2031,\n",
      "          0.9727, -0.4473, -1.6641, -1.7266, -2.3438, -0.8438,  1.2969, -1.8594,\n",
      "         -2.3750, -2.5781, -7.5312, -8.7500, -4.4062, -5.0625, -5.0938, -6.7812,\n",
      "         -6.3438, -6.8125, -6.0000, -7.7500, -4.6875, -4.1562, -5.9062, -6.0625,\n",
      "         -4.8750, -5.6562, -5.6562, -5.3750, -6.0312, -6.6250, -4.8438, -2.2188,\n",
      "         -4.1562, -6.5312, -7.4062, -4.8750, -3.8594, -3.5000, -4.4688, -6.6875,\n",
      "         -3.2031, -4.2188, -7.6250, -5.5000, -5.9688, -5.7188, -6.3438, -6.5000,\n",
      "         -5.9688, -5.5625, -5.1250, -5.9062, -6.0625, -4.9688, -6.2812, -7.3438,\n",
      "         -7.1875, -6.9375, -7.1562, -5.9688, -5.5312, -4.7188, -3.9531, -5.2500,\n",
      "         -7.2500, -7.1250, -7.2188, -5.6250, -5.2812, -7.0625, -6.9062, -6.5625,\n",
      "         -4.5625, -4.6562, -6.5625, -6.8438, -8.6250, -8.6250, -1.7734, -1.9609,\n",
      "         -7.8125, -8.1250, -8.6250, -8.1250, -8.4375, -8.4375, -8.6250, -8.2500,\n",
      "         -7.5938, -8.7500, -6.5625, -4.8438, -7.7500, -7.8125, -8.6875, -8.6875,\n",
      "         -8.7500, -8.7500, -8.7500, -8.6875, -8.6875, -8.7500, -8.6250, -8.6250,\n",
      "         -8.7500, -8.7500, -8.4375, -8.5625, -8.3125, -8.6875, -2.4219, -6.0625,\n",
      "         -7.2500, -6.5938, -7.1250, -7.1875, -7.8125, -7.8750, -8.4375, -8.5625,\n",
      "         -8.5625, -8.6875, -8.5625, -5.6562, -3.6094, -8.6875, -8.6875, -8.6875,\n",
      "         -8.7500, -8.6875, -8.6250, -8.6875, -8.5625, -8.6875, -8.5625, -8.7500,\n",
      "         -8.7500, -8.6875, -8.6250, -8.7500]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 122\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  5]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 122])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 122, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0137,  0.5859, -0.2734,  ...,  0.8359, -0.8945, -1.4297],\n",
      "        [ 2.5781,  0.1504, -2.1406,  ...,  1.1250, -0.7148, -1.5625],\n",
      "        [-0.1836, -0.3203,  0.8633,  ...,  2.2812,  1.0938,  1.2891]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -38.5000,  -428.0000,   272.0000,  ...,  1040.0000,\n",
      "            229.0000,  -142.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  8256.,  -2992., -21120.,  ...,   -540.,   7648., -24832.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 122, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0137,  0.5859, -0.2734,  ...,  0.8359, -0.8945, -1.4297],\n",
      "        [ 2.5781,  0.1504, -2.1406,  ...,  1.1250, -0.7148, -1.5625],\n",
      "        [-0.1836, -0.3203,  0.8633,  ...,  2.2812,  1.0938,  1.2891]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 122]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 15]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 122, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.0625e+00, -5.8438e+00, -2.0312e+00, -8.0625e+00, -8.0625e+00,\n",
      "         -8.1250e+00, -8.0625e+00, -8.0625e+00, -8.1250e+00, -8.0000e+00,\n",
      "         -8.1875e+00, -8.2500e+00, -8.2500e+00, -4.7812e+00, -1.3516e+00,\n",
      "         -8.1250e+00, -8.0625e+00, -7.8750e+00, -8.1250e+00, -8.0625e+00,\n",
      "         -8.0625e+00, -8.0000e+00, -8.0625e+00, -8.0625e+00, -8.0625e+00,\n",
      "         -8.1250e+00, -8.0625e+00, -8.0625e+00, -8.1250e+00, -8.1875e+00,\n",
      "         -8.1875e+00, -8.1250e+00, -8.0625e+00, -8.0625e+00, -8.0625e+00,\n",
      "         -8.0000e+00,  3.0518e-02, -2.3125e+00, -2.6250e+00, -2.8125e+00,\n",
      "         -3.3594e+00, -4.0938e+00, -5.5000e+00, -2.8906e+00, -3.2031e+00,\n",
      "         -3.9531e+00, -4.2812e+00, -3.6719e+00, -1.9336e-01, -2.1250e+00,\n",
      "          7.1094e-01, -2.4531e+00, -4.1250e+00, -2.4375e+00, -2.8281e+00,\n",
      "         -3.4844e+00, -2.5625e+00, -4.0000e+00, -3.7500e+00, -3.1406e+00,\n",
      "         -3.4062e+00, -4.0312e+00, -1.5859e+00, -3.6562e+00, -3.3594e+00,\n",
      "         -3.2188e+00, -4.0312e+00, -2.7812e+00, -5.0000e+00, -3.5312e+00,\n",
      "         -4.3125e+00, -4.5000e+00, -3.9844e+00, -3.4531e+00, -3.6562e+00,\n",
      "         -4.3750e+00, -3.5312e+00, -5.6839e-04, -3.3906e+00, -5.2188e+00,\n",
      "         -3.3125e+00, -3.8125e+00, -3.6406e+00, -2.3750e+00, -3.8438e+00,\n",
      "         -6.8438e+00, -3.5625e+00, -3.9688e+00, -3.7188e+00, -3.9219e+00,\n",
      "         -4.6875e+00, -5.0625e+00, -5.1250e+00, -3.3281e+00, -4.8750e+00,\n",
      "         -4.4375e+00, -5.8438e+00, -5.8125e+00, -5.6562e+00, -4.4375e+00,\n",
      "         -4.2500e+00,  1.2578e+00, -2.0156e+00, -1.4766e+00, -1.8125e+00,\n",
      "          4.0312e+00, -4.6484e-01, -2.4688e+00,  1.0469e+00,  1.1062e+01,\n",
      "         -2.8281e+00, -1.5391e+00,  2.7188e+00, -1.4297e+00, -1.2891e+00,\n",
      "          1.8984e+00, -1.6484e+00, -4.6250e+00,  8.6328e-01,  2.7344e-01,\n",
      "          5.8350e-02,  7.1484e-01, -3.0625e+00, -2.7344e+00, -5.3750e+00,\n",
      "          4.7812e+00, -1.7188e+00, -4.3125e+00, -3.5938e+00, -4.0312e+00,\n",
      "         -7.0938e+00, -8.1250e+00, -3.9219e+00, -5.7500e+00, -5.5625e+00,\n",
      "         -6.8125e+00, -6.4375e+00, -6.6250e+00, -6.5312e+00, -7.4688e+00,\n",
      "         -5.9375e+00, -5.0938e+00, -6.3125e+00, -6.0312e+00, -5.6875e+00,\n",
      "         -6.0938e+00, -6.5938e+00, -5.7500e+00, -6.3125e+00, -6.2812e+00,\n",
      "         -6.0938e+00, -5.7500e+00, -5.2500e+00, -7.0938e+00, -7.1562e+00,\n",
      "         -6.3125e+00, -5.6250e+00, -5.3125e+00, -6.3438e+00, -6.8438e+00,\n",
      "         -4.2812e+00, -5.3125e+00, -7.2188e+00, -5.6875e+00, -6.3438e+00,\n",
      "         -5.7812e+00, -6.5625e+00, -6.6562e+00, -6.4062e+00, -5.7188e+00,\n",
      "         -6.5938e+00, -6.1875e+00, -6.4688e+00, -5.2812e+00, -6.4062e+00,\n",
      "         -7.0938e+00, -7.0000e+00, -6.8750e+00, -7.0000e+00, -6.0625e+00,\n",
      "         -5.9375e+00, -4.6250e+00, -5.5000e+00, -5.8125e+00, -7.1875e+00,\n",
      "         -6.9375e+00, -7.0312e+00, -6.1250e+00, -5.5938e+00, -7.0312e+00,\n",
      "         -6.7812e+00, -6.4375e+00, -5.4062e+00, -6.1250e+00, -6.7188e+00,\n",
      "         -6.7188e+00, -8.0000e+00, -8.1875e+00, -4.2188e+00, -1.6172e+00,\n",
      "         -7.5625e+00, -7.5938e+00, -8.0000e+00, -7.7188e+00, -8.0000e+00,\n",
      "         -7.5312e+00, -8.0625e+00, -7.6250e+00, -7.4375e+00, -8.0625e+00,\n",
      "         -6.7188e+00, -6.1875e+00, -7.5938e+00, -7.4688e+00, -7.9688e+00,\n",
      "         -8.0625e+00, -8.1250e+00, -8.1875e+00, -8.1250e+00, -8.1250e+00,\n",
      "         -8.0625e+00, -8.0625e+00, -8.1250e+00, -8.1250e+00, -8.0625e+00,\n",
      "         -8.1875e+00, -8.1250e+00, -8.1250e+00, -8.0625e+00, -8.1250e+00,\n",
      "         -4.0000e+00, -6.5312e+00, -7.0312e+00, -6.9375e+00, -6.7500e+00,\n",
      "         -7.3438e+00, -7.4375e+00, -7.7812e+00, -7.8438e+00, -7.9375e+00,\n",
      "         -7.9062e+00, -8.0625e+00, -8.1250e+00, -6.5312e+00, -5.6562e+00,\n",
      "         -8.1250e+00, -8.1875e+00, -8.0625e+00, -8.0625e+00, -8.1250e+00,\n",
      "         -8.0625e+00, -8.1875e+00, -8.1250e+00, -8.1875e+00, -8.0625e+00,\n",
      "         -8.1875e+00, -8.1875e+00, -8.2500e+00, -8.0000e+00, -8.1250e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 123\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  6]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 123])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 123, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.5781,  0.1504, -2.1406,  ...,  1.1250, -0.7148, -1.5625],\n",
      "        [-0.1836, -0.3203,  0.8633,  ...,  2.2812,  1.0938,  1.2891],\n",
      "        [ 1.6016,  0.8672,  0.2793,  ...,  0.2539, -0.3262, -0.1279]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -46.0000,  -456.0000,   296.0000,  ...,  1072.0000,\n",
      "            248.0000,  -160.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  8768.,  -3088., -22528.,  ...,   -636.,   8096., -26240.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 123, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.5781,  0.1504, -2.1406,  ...,  1.1250, -0.7148, -1.5625],\n",
      "        [-0.1836, -0.3203,  0.8633,  ...,  2.2812,  1.0938,  1.2891],\n",
      "        [ 1.6016,  0.8672,  0.2793,  ...,  0.2539, -0.3262, -0.1279]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 123]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 123, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.3750, -6.1875, -0.1484, -5.3438, -5.4688, -5.4688, -5.4375, -5.4375,\n",
      "         -5.3438, -5.4062, -5.5625, -5.5312, -5.4062, -3.8594,  0.4980, -5.5312,\n",
      "         -5.5000, -5.1562, -5.4375, -5.4375, -5.4062, -5.5000, -5.4375, -5.3750,\n",
      "         -5.4062, -5.4375, -5.4062, -5.3750, -5.4062, -5.4688, -5.4375, -5.3438,\n",
      "         -5.3750, -5.4375, -5.4375, -5.4062,  1.1328, -0.2334, -1.8359, -1.9922,\n",
      "         -2.5938, -3.5469, -4.0312, -1.5078, -1.6641, -3.2812, -2.5000, -1.9453,\n",
      "          1.6094,  0.1064,  0.5352, -1.1328, -5.1875, -3.0469, -3.7188, -3.6406,\n",
      "         -2.2969, -3.2344, -1.8359, -1.7656, -2.7812, -3.0312, -1.3047, -0.8984,\n",
      "         -3.5938, -3.6094, -2.3750, -1.7422, -2.8750, -4.2188, -2.8906, -3.2969,\n",
      "         -2.1406, -3.1875, -3.0781, -3.9219, -4.7500, -2.2031, -2.4219, -4.8438,\n",
      "          1.8047, -4.3125, -3.2969, -2.1094, -1.2266, -4.0625, -2.5312, -3.1719,\n",
      "         -2.5938, -4.3125, -4.1562, -2.8125, -3.0156, -2.9688, -3.7500, -3.9375,\n",
      "         -3.2969, -3.5938, -4.7500, -1.9844, -2.9219,  0.2988,  0.0386,  1.2656,\n",
      "          2.5312,  1.0391,  1.6562, -2.7812, -3.2344, -1.1562, -2.1250, -1.7734,\n",
      "         12.0625,  0.6836,  3.2656,  0.4336,  2.1094, -1.9141,  2.5625,  1.8125,\n",
      "         -0.7422, -0.5742, -0.9961, -2.5312, -0.9297, -1.4922, -1.3594, -3.8906,\n",
      "         -2.6094, -3.2500, -4.8125, -5.5625, -3.7188, -3.8281, -3.6250, -4.8438,\n",
      "         -4.3438, -4.8438, -3.9375, -4.9688, -3.2500, -4.0938, -4.3438, -4.4375,\n",
      "         -4.8750, -4.5625, -4.6562, -4.0938, -4.4062, -4.3750, -4.5312, -3.4531,\n",
      "         -3.7656, -4.7188, -4.8438, -4.5312, -3.7500, -3.2188, -4.5000, -4.6562,\n",
      "         -3.3594, -6.2500, -5.0312, -5.3438, -4.5312, -4.3438, -4.9062, -4.9062,\n",
      "         -4.5938, -4.2188, -4.4062, -4.6562, -4.7188, -4.7812, -4.5312, -4.8438,\n",
      "         -4.6875, -4.8750, -4.6875, -4.0000, -3.7969, -4.2500, -3.7188, -4.7500,\n",
      "         -4.7812, -4.8125, -4.5625, -3.9375, -4.6875, -4.7812, -4.6875, -4.3750,\n",
      "         -4.6250, -5.1875, -4.7500, -4.8125, -5.3750, -5.4062, -2.4844, -1.8672,\n",
      "         -5.0000, -5.0312, -5.4375, -5.1250, -5.3125, -5.3750, -5.4062, -5.3125,\n",
      "         -5.0000, -5.4375, -4.4688, -4.3750, -4.9375, -5.0000, -5.4062, -5.3438,\n",
      "         -5.5312, -5.3438, -5.4062, -5.3750, -5.3438, -5.4062, -5.4062, -5.4062,\n",
      "         -5.4062, -5.4375, -5.4062, -5.4688, -5.2812, -5.4688, -2.5312, -4.5625,\n",
      "         -4.6875, -4.6875, -4.8438, -4.8125, -5.0000, -5.3438, -5.2188, -5.3125,\n",
      "         -5.2188, -5.4062, -5.3750, -4.7500, -3.8594, -5.4688, -5.5000, -5.3438,\n",
      "         -5.3750, -5.4375, -5.4375, -5.3438, -5.4062, -5.5000, -5.5000, -5.4375,\n",
      "         -5.4375, -5.5312, -5.3438, -5.4062]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 124\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  7]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 124])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 124, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1836, -0.3203,  0.8633,  ...,  2.2812,  1.0938,  1.2891],\n",
      "        [ 1.6016,  0.8672,  0.2793,  ...,  0.2539, -0.3262, -0.1279],\n",
      "        [ 0.1719,  1.6641, -0.8867,  ...,  1.8828,  0.4883, -0.3496]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -57.5000,  -480.0000,   316.0000,  ...,  1088.0000,\n",
      "            260.0000,  -174.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9088.,  -3136., -23424.,  ...,   -688.,   8384., -27136.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 124, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1836, -0.3203,  0.8633,  ...,  2.2812,  1.0938,  1.2891],\n",
      "        [ 1.6016,  0.8672,  0.2793,  ...,  0.2539, -0.3262, -0.1279],\n",
      "        [ 0.1719,  1.6641, -0.8867,  ...,  1.8828,  0.4883, -0.3496]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 124]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 15]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 124, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.6250, -5.3125,  5.2188, -6.5312, -6.6562, -6.5312, -6.5000, -6.6250,\n",
      "         -6.6562, -6.6562, -6.5000, -6.5938, -6.4688, -3.2031,  5.3438, -6.6562,\n",
      "         -6.5938, -6.1562, -6.6250, -6.5625, -6.4375, -6.5938, -6.6250, -6.5000,\n",
      "         -6.5625, -6.6250, -6.5000, -6.6250, -6.5000, -6.5312, -6.6250, -6.5938,\n",
      "         -6.5312, -6.6250, -6.5625, -6.5625,  5.7188,  4.0938,  0.1963, -1.8438,\n",
      "         -2.7500, -3.0781, -2.9375, -0.7578, -1.0312,  0.6172,  0.3418, -1.7734,\n",
      "          7.5312,  1.6406,  5.3750, -0.4414, -3.1250, -1.9375, -3.3750, -2.1562,\n",
      "         -2.4375, -2.1406, -2.0781, -3.0312, -2.5781, -3.0000,  0.4102,  2.0312,\n",
      "         -2.2031, -0.8633, -1.2656,  0.7266, -2.4062, -2.3906, -3.5000, -2.9531,\n",
      "         -1.4219, -1.4609, -3.0156, -2.7031, -3.8906, -2.0938, -2.3594, -3.0312,\n",
      "         -2.9062, -3.5938, -2.4062, -0.6133, -3.6406, -3.1250, -3.2188, -1.1172,\n",
      "         -2.7344, -2.0000, -2.5469, -2.2656, -2.6094, -2.5781, -2.6094, -2.9531,\n",
      "         -2.8594,  2.1406, -2.3438, -2.1719, -1.6875,  0.7617, -3.2812, -2.9531,\n",
      "          1.3438,  3.0312, -2.3125, -1.8828, -2.2812,  1.6484, -2.1250, -2.8438,\n",
      "          1.3359, -1.7031, -0.6562,  2.3438, -3.1719, -1.5156, -1.7656,  2.4375,\n",
      "         -1.0469,  0.3867, -1.8281, -2.3438, -2.5469, -1.6797, -2.0156, -3.6250,\n",
      "         -2.0312, -1.0703, -5.3125, -6.7188, -2.8750, -3.8750, -3.6250, -4.7500,\n",
      "         -4.7500, -4.6875, -4.8750, -5.7188, -2.3594, -3.5469, -4.6562, -4.4688,\n",
      "         -3.7031, -3.8594, -3.9375, -4.0312, -4.2812, -4.7500, -3.9219, -1.6641,\n",
      "         -2.0156, -5.1250, -5.6562, -3.1875, -3.3594, -0.8164, -3.2188, -4.9375,\n",
      "         -2.7969, -4.6562, -5.8438, -3.8438, -4.6875, -4.1250, -4.0312, -5.0625,\n",
      "         -4.5312, -4.2812, -3.9531, -4.2812, -4.4062, -3.0312, -4.7500, -5.3438,\n",
      "         -5.1875, -5.0625, -5.2500, -4.0000, -4.0625, -3.4688, -3.1875, -4.0625,\n",
      "         -5.1562, -5.2188, -5.0000, -3.7969, -4.2812, -5.2500, -4.7500, -4.7500,\n",
      "         -4.0000, -4.5625, -4.8438, -4.9062, -6.5312, -6.6250, -2.4062, -0.4570,\n",
      "         -5.9062, -5.9375, -6.6250, -6.1875, -6.4375, -6.5938, -6.6250, -6.2500,\n",
      "         -5.9688, -6.6250, -4.4375, -3.8594, -5.7500, -5.7812, -6.5938, -6.5312,\n",
      "         -6.7188, -6.5938, -6.5938, -6.5625, -6.7188, -6.5938, -6.5938, -6.5938,\n",
      "         -6.5625, -6.5000, -6.5312, -6.5625, -6.3125, -6.7188, -1.1016, -4.6875,\n",
      "         -5.1875, -5.0312, -5.1250, -5.8125, -5.8125, -6.0312, -6.4062, -6.5938,\n",
      "         -6.4375, -6.6562, -6.5312, -4.7188, -2.5625, -6.6562, -6.6250, -6.5000,\n",
      "         -6.5938, -6.6875, -6.5938, -6.5312, -6.5312, -6.6250, -6.6562, -6.6250,\n",
      "         -6.6562, -6.6250, -6.5625, -6.6562]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 125\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 128, 125])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 125, 256]), Cross output shape: torch.Size([1, 128, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.6016,  0.8672,  0.2793,  ...,  0.2539, -0.3262, -0.1279],\n",
      "        [ 0.1719,  1.6641, -0.8867,  ...,  1.8828,  0.4883, -0.3496],\n",
      "        [-1.6250,  0.5000, -0.4961,  ..., -0.3477, -0.9883, -1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 16, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -74.0000,  -440.0000,   296.0000,  ...,  1072.0000,\n",
      "            268.0000,  -179.0000],\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 16, 512]), Global transformer output: tensor([[[-12928., -37120.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21632., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -996.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8448.,  -2960., -21760.,  ...,   -644.,   7872., -25344.],\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8896., -28672.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 125, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.6016,  0.8672,  0.2793,  ...,  0.2539, -0.3262, -0.1279],\n",
      "        [ 0.1719,  1.6641, -0.8867,  ...,  1.8828,  0.4883, -0.3496],\n",
      "        [-1.6250,  0.5000, -0.4961,  ..., -0.3477, -0.9883, -1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 125]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 125, 128])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-4.6562, -6.3125,  3.4688, -4.7188, -4.7188, -4.7500, -4.5625, -4.6875,\n",
      "         -4.8125, -4.7188, -4.6250, -4.7812, -4.7188, -2.5469,  6.2188, -4.8125,\n",
      "         -4.5938, -4.5312, -4.7188, -4.6250, -4.6875, -4.6562, -4.6250, -4.7188,\n",
      "         -4.7188, -4.6875, -4.7188, -4.7500, -4.7812, -4.6875, -4.6875, -4.6562,\n",
      "         -4.6875, -4.8125, -4.7188, -4.6875, 10.4375,  0.2578,  1.1719, -1.5078,\n",
      "         -2.5625, -4.2500, -1.1562, -1.1875, -0.9688,  0.3359, -0.7188, -2.3125,\n",
      "          0.8750, -0.1699,  2.1875,  1.3594, -2.0781, -1.4766, -1.6406, -2.6562,\n",
      "         -2.6250, -2.4375, -2.8594, -3.4531, -4.0312, -3.5156,  1.4531, -1.8047,\n",
      "         -0.5703,  0.5156,  0.0610, -0.0806, -3.5156, -2.5000, -2.1406, -1.1406,\n",
      "         -2.0938, -2.8750, -2.1719, -3.1250, -2.4688, -1.7188, -2.5156, -2.7656,\n",
      "         -3.0938, -1.1406, -1.6875, -1.7500, -2.2656, -2.8125, -1.5703, -1.3359,\n",
      "         -2.0312, -2.4688, -0.3320, -1.2578, -3.3281, -2.7656, -2.1719, -1.7031,\n",
      "         -3.4375, -0.2422, -2.5469, -1.8672, -4.5625, -1.3594, -1.3750, -0.5078,\n",
      "         -0.4785, -1.5234, -1.5000, -1.3984, -1.9062, -0.3242, -1.3203, -0.6406,\n",
      "         -0.5352, -0.7695,  0.4141, -1.2812, -1.8359, -3.1875, -1.1875, -0.0869,\n",
      "         -0.4961, -2.0781, -1.2266, -0.7070, -2.7969, -1.8047, -2.1562, -3.1094,\n",
      "         -1.1953, -1.3516, -4.3125, -4.7188, -3.8281, -3.9844, -3.9062, -4.0938,\n",
      "         -4.1562, -4.2500, -3.9375, -4.2812, -3.1250, -3.7812, -4.0000, -3.9688,\n",
      "         -3.8281, -3.7500, -4.0625, -3.4062, -3.7344, -4.1562, -2.7656, -3.4531,\n",
      "         -2.9844, -3.9844, -4.0625, -1.9219, -3.5781, -3.8594, -3.1406, -3.9688,\n",
      "         -2.5469, -2.2344, -4.3750, -4.0000, -3.8125, -3.6719, -3.8750, -3.8750,\n",
      "         -3.7500, -3.7969, -3.8750, -3.3594, -3.8906, -2.9531, -3.7812, -4.3125,\n",
      "         -4.2812, -4.0312, -3.9375, -3.7656, -3.9062, -3.3125, -3.0000, -3.4375,\n",
      "         -4.0312, -4.0625, -4.2500, -3.3438, -3.7031, -4.0000, -3.9375, -3.6562,\n",
      "         -3.0938, -2.8906, -3.7969, -4.1250, -4.6250, -4.7188, -1.6797, -0.7188,\n",
      "         -4.3125, -4.4375, -4.6250, -4.5625, -4.6875, -4.7500, -4.7188, -4.4688,\n",
      "         -4.3750, -4.7188, -4.0312, -3.8750, -4.2188, -4.2188, -4.6562, -4.7500,\n",
      "         -4.8750, -4.7812, -4.6562, -4.7188, -4.6875, -4.6875, -4.6875, -4.6875,\n",
      "         -4.6250, -4.6875, -4.6875, -4.6875, -4.6562, -4.7188, -1.7812, -3.6719,\n",
      "         -4.1875, -4.0938, -4.1875, -4.3750, -4.4375, -4.5938, -4.6250, -4.5938,\n",
      "         -4.4688, -4.7812, -4.6875, -3.8438, -3.5625, -4.7812, -4.7500, -4.5938,\n",
      "         -4.7188, -4.6562, -4.7188, -4.6875, -4.7500, -4.7188, -4.7188, -4.6875,\n",
      "         -4.8125, -4.7500, -4.7500, -4.7188]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 126\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 126])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 126, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.1719,  1.6641, -0.8867,  ...,  1.8828,  0.4883, -0.3496],\n",
      "        [-1.6250,  0.5000, -0.4961,  ..., -0.3477, -0.9883, -1.4219],\n",
      "        [-0.7266,  0.9531,  1.8125,  ...,  0.3027, -0.2617, -0.6484]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  108.0000,   -93.0000,   -63.5000,  ...,   314.0000,\n",
      "             30.2500,   -16.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  2224.,  -1464.,  -5312.,  ...,    111.,   2272.,  -7392.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 126, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.1719,  1.6641, -0.8867,  ...,  1.8828,  0.4883, -0.3496],\n",
      "        [-1.6250,  0.5000, -0.4961,  ..., -0.3477, -0.9883, -1.4219],\n",
      "        [-0.7266,  0.9531,  1.8125,  ...,  0.3027, -0.2617, -0.6484]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 126]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 16]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 126, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.8125, -10.0625,  -5.3438, -10.8125, -10.8125, -10.8750, -10.8750,\n",
      "         -10.8125, -10.8750, -10.8750, -10.8750, -10.8750, -10.8750,  -7.0625,\n",
      "          -1.1406, -10.9375, -10.8750, -10.3750, -10.8750, -10.9375, -10.8125,\n",
      "         -10.8125, -10.8750, -11.0000, -10.8750, -10.8125, -11.0000, -10.8750,\n",
      "         -10.9375, -10.8125, -10.8750, -10.8750, -10.9375, -10.8750, -10.9375,\n",
      "         -10.8750,  -3.9375,  -6.2812,  -1.4219,  -4.5000,  -6.5625,  -6.6562,\n",
      "          -5.5312,  -4.7812,  -2.7969,  -7.5312,  -5.5625,  -5.0312,  -4.8750,\n",
      "          -2.6094,  -3.2656,  -2.7344,  -5.9062,  -4.0312,  -4.7812,  -5.1875,\n",
      "          -5.3438,  -5.6250,  -6.4375,  -6.5312,  -5.8438,  -5.9375,  -5.5625,\n",
      "          -4.9062,  -6.0625,  -5.8125,  -6.0625,  -8.5625,  -7.0938,  -3.3750,\n",
      "          -3.5000,  -4.0312,  -5.5625,  -4.0625,  -3.8125,  -3.9531,  -4.3438,\n",
      "          -1.6406,  -2.2812,  -4.7188,  -4.1562,  -3.3438,  -5.0938,  -5.1250,\n",
      "          -1.8125,  -6.1562,  -3.5938,  -2.7188,  -3.8594,  -4.1250,  -5.6562,\n",
      "          -5.2812,  -6.3125,  -6.3750,  -5.9375,  -4.4375,  -8.1250,  -6.2500,\n",
      "          -6.7812,  -4.2500,  -6.3125,   3.5938,   2.6875,   1.6719,   0.8008,\n",
      "           0.5781,   1.3672,   0.8164,   0.3887,   1.3438,   0.7578,  -0.6328,\n",
      "           0.3730,   1.1641,  -1.2656,   0.8516,   3.7031,  -1.5156,   0.9297,\n",
      "           1.2109,   1.8750,   0.1079,  -0.6445,   0.4258,  -5.8750,  -0.3359,\n",
      "          -3.2031,  -5.4062,  -6.1250,  -5.3125,  -9.6250, -10.8750,  -7.0625,\n",
      "          -8.8750,  -8.6250,  -9.6250,  -9.5000,  -9.4375,  -8.8125, -10.2500,\n",
      "          -7.7812,  -8.2500,  -9.5000,  -8.9375,  -9.0000,  -9.5625,  -9.5000,\n",
      "          -9.0625,  -9.5000,  -9.6875,  -7.8750,  -7.2500,  -8.1250,  -9.9375,\n",
      "          -9.8750,  -8.3125,  -8.3125,  -9.5000,  -7.1875,  -9.8750,  -7.5000,\n",
      "          -7.4688, -10.1250,  -8.6875,  -8.5625,  -9.0000,  -8.8750,  -9.6875,\n",
      "          -9.2500,  -8.8125,  -8.3750,  -9.2500,  -9.4375,  -6.9688,  -9.3125,\n",
      "          -9.8750, -10.0625, -10.0625,  -9.6250,  -9.4375,  -6.8750,  -7.9688,\n",
      "          -8.5625,  -8.4375,  -9.7500,  -9.7500,  -9.8750,  -8.6875,  -8.8125,\n",
      "          -9.8125,  -9.5625,  -9.3750,  -8.1250,  -8.0625,  -9.4375,  -9.5000,\n",
      "         -10.8750, -10.8125,  -6.2500,  -4.4062, -10.1250, -10.1875, -10.8125,\n",
      "         -10.5000, -10.7500, -10.3750, -10.8750, -10.6250, -10.3750, -10.8125,\n",
      "          -9.3750,  -9.0000, -10.1250,  -9.8750, -10.8125, -10.9375, -10.9375,\n",
      "         -10.8750, -10.8750, -10.9375, -10.8750, -10.8125, -10.8750, -10.8750,\n",
      "         -10.7500, -10.9375, -10.9375, -10.9375, -10.6875, -10.8125,  -4.3125,\n",
      "          -9.3750, -10.0000,  -9.6875,  -9.7500, -10.0000, -10.1875, -10.5000,\n",
      "         -10.6250, -10.8125, -10.7500, -10.8750, -10.8750,  -9.3125,  -7.2500,\n",
      "         -10.9375, -10.9375, -10.8125, -10.8125, -10.9375, -10.8750, -11.0000,\n",
      "         -10.9375, -10.8750, -10.8750, -10.9375, -10.8750, -10.8750, -10.8750,\n",
      "         -10.8750]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 127\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  2]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 127])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 127, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6250,  0.5000, -0.4961,  ..., -0.3477, -0.9883, -1.4219],\n",
      "        [-0.7266,  0.9531,  1.8125,  ...,  0.3027, -0.2617, -0.6484],\n",
      "        [-0.3281, -0.8438,  3.2344,  ...,  0.9453,  1.4375,  0.2773]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [   -4.5000,  -243.0000,    88.0000,  ...,   816.0000,\n",
      "            129.0000,   -51.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  5440.,  -2032., -13696.,  ...,   -237.,   5120., -16384.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 127, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6250,  0.5000, -0.4961,  ..., -0.3477, -0.9883, -1.4219],\n",
      "        [-0.7266,  0.9531,  1.8125,  ...,  0.3027, -0.2617, -0.6484],\n",
      "        [-0.3281, -0.8438,  3.2344,  ...,  0.9453,  1.4375,  0.2773]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 127]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         16]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 127, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.1250, -8.0000, -3.0938, -6.1250, -6.1250, -6.0625, -6.0938, -6.1562,\n",
      "         -6.0938, -6.0312, -6.0625, -6.0938, -6.1250, -4.6562, -2.6250, -6.0938,\n",
      "         -6.0312, -5.8750, -6.0625, -6.0312, -6.0625, -6.0625, -5.8750, -6.0938,\n",
      "         -6.0625, -5.9688, -6.0625, -6.0938, -6.0938, -6.0000, -6.0938, -6.0000,\n",
      "         -6.0312, -6.0625, -5.9688, -6.0625, -1.7344, -4.3125, -3.5938, -3.7344,\n",
      "         -4.5000, -4.3750, -4.3125, -3.2500, -3.1094, -3.3125, -3.9688, -5.1250,\n",
      "         -1.6484, -2.9688, -0.8047, -3.6406, -4.1875, -4.3438, -3.8125, -3.8125,\n",
      "         -4.0938, -5.4375, -4.0312, -5.3125, -4.5000, -5.5000, -3.5469, -2.8281,\n",
      "         -5.3750, -4.3438, -2.5625, -4.7188, -4.5000, -4.1875, -5.9688, -4.7500,\n",
      "         -5.0938, -1.1172, -4.9688, -4.3750, -3.5781, -4.1562, -5.1250, -5.3125,\n",
      "         -4.5312, -4.8125, -4.5000, -4.2812, -4.1562, -6.2500, -4.6875, -4.5625,\n",
      "         -4.1562, -3.5625, -5.1250, -4.6562, -4.9375, -4.6250, -4.5312, -3.8594,\n",
      "         -4.9375, -4.9688, -5.2812, -3.8906, -6.0000,  3.5781, -2.0625, -1.2812,\n",
      "         -1.4062, 10.0000, -2.5781, -1.5312,  2.3906,  4.5312, -3.9219, -2.1562,\n",
      "          2.3594, -1.3438, -0.8125,  4.0625, -0.7227, -3.5469,  3.0625, -0.0967,\n",
      "         -0.9141,  4.1875, -2.4219, -1.7500, -3.8750,  1.3906, -1.3359, -3.9688,\n",
      "         -4.6875, -3.4375, -5.6250, -6.0938, -3.7500, -5.0938, -5.1250, -5.6562,\n",
      "         -5.6562, -5.6250, -5.4375, -5.6875, -5.2188, -5.1562, -5.3438, -5.2188,\n",
      "         -5.2188, -5.2812, -5.3750, -5.5000, -5.3125, -5.3438, -5.0312, -6.0000,\n",
      "         -5.0312, -5.5938, -5.6562, -5.3750, -5.3438, -4.5938, -4.3438, -5.7188,\n",
      "         -4.8125, -4.1562, -5.6875, -5.1250, -5.0312, -5.4375, -5.3438, -5.6875,\n",
      "         -5.5625, -5.0312, -4.6875, -5.6562, -5.3125, -5.1875, -5.5000, -5.6875,\n",
      "         -5.6875, -5.5938, -5.4688, -5.5625, -4.5000, -4.5938, -4.3438, -4.9375,\n",
      "         -5.5938, -5.5625, -5.5312, -5.1875, -5.3438, -5.6250, -5.5000, -5.5938,\n",
      "         -4.9688, -5.0938, -5.3438, -5.2812, -6.0312, -5.9062, -4.2500,  0.1143,\n",
      "         -5.7812, -5.8750, -6.0312, -5.9375, -6.0000, -5.9688, -6.1250, -6.0938,\n",
      "         -5.8438, -6.0938, -5.7188, -4.9688, -5.7812, -5.6562, -6.0938, -6.0625,\n",
      "         -6.1562, -6.0625, -6.0938, -6.1562, -6.0625, -6.1250, -6.0938, -6.0625,\n",
      "         -6.0625, -6.1562, -6.0938, -6.0625, -5.9688, -6.1250, -4.4062, -5.5938,\n",
      "         -5.6875, -5.3438, -5.6250, -5.6562, -5.7188, -5.9375, -5.8750, -6.0938,\n",
      "         -5.9688, -6.0938, -6.0938, -5.3125, -4.0625, -6.0938, -6.1250, -6.0312,\n",
      "         -6.0938, -6.1562, -6.0312, -6.0938, -6.0625, -6.0625, -6.0938, -6.1250,\n",
      "         -6.0312, -6.0312, -6.1562, -6.0625]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 128\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  3]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 128])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 128, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.7266,  0.9531,  1.8125,  ...,  0.3027, -0.2617, -0.6484],\n",
      "        [-0.3281, -0.8438,  3.2344,  ...,  0.9453,  1.4375,  0.2773],\n",
      "        [-0.1826,  0.4316, -0.1064,  ...,  0.7773, -0.8203, -1.2344]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -46.5000,  -320.0000,   164.0000,  ...,   904.0000,\n",
      "            161.0000,  -108.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  6656.,  -2400., -16896.,  ...,   -364.,   6176., -19968.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 128, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.7266,  0.9531,  1.8125,  ...,  0.3027, -0.2617, -0.6484],\n",
      "        [-0.3281, -0.8438,  3.2344,  ...,  0.9453,  1.4375,  0.2773],\n",
      "        [-0.1826,  0.4316, -0.1064,  ...,  0.7773, -0.8203, -1.2344]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 128]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 16]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 128, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[ -9.4375, -10.3125,  -4.5625,  -9.3750,  -9.5000,  -9.3125,  -9.4375,\n",
      "          -9.3750,  -9.5000,  -9.4375,  -9.5625,  -9.5000,  -9.4375,  -6.0312,\n",
      "          -2.6562,  -9.4375,  -9.5625,  -9.1875,  -9.4375,  -9.4375,  -9.4375,\n",
      "          -9.4375,  -9.3750,  -9.3125,  -9.3750,  -9.4375,  -9.3125,  -9.4375,\n",
      "          -9.4375,  -9.4375,  -9.4375,  -9.4375,  -9.3125,  -9.4375,  -9.3750,\n",
      "          -9.5625,  -0.7734,  -4.6875,  -2.3125,  -5.8438,  -6.3750,  -4.9375,\n",
      "          -5.2188,  -2.7344,  -4.1875,  -4.2500,  -5.2812,  -5.3750,  -0.6836,\n",
      "          -1.6719,  -0.4473,  -2.9688,  -3.6250,  -4.1875,  -4.4688,  -4.4688,\n",
      "          -3.9062,  -5.3750,  -4.9688,  -3.8438,  -5.8125,  -5.1562,  -2.9062,\n",
      "          -3.5312,  -5.4375,  -4.6250,  -3.7344,  -4.3750,  -6.6562,  -4.3125,\n",
      "          -5.4688,  -4.3438,  -5.9688,  -5.1250,  -5.6250,  -4.4062,  -5.0625,\n",
      "          -4.7812,  -4.6562,  -5.7188,  -5.7188,  -4.4062,  -1.3516,  -4.8750,\n",
      "          -3.6250,  -4.7188,  -5.1562,  -3.4375,  -3.7031,  -5.6562,  -5.5938,\n",
      "          -4.5938,  -4.2188,  -6.1875,  -5.6875,  -5.6875,  -7.0000,  -5.8438,\n",
      "          -4.9375,  -4.8125,  -5.6562,   4.2188,  -1.1094,   1.2109,  -0.0645,\n",
      "           2.4062,  -2.2344,  -0.2100,  -1.2812,  -0.9492,  -3.0938,  -0.8750,\n",
      "           1.6641,  -0.5391,  10.1250,   1.6328,   2.5938,  -0.7734,   3.4375,\n",
      "           0.6914,   2.5781,  -0.6211,  -0.8789,  -0.5430,  -0.5039,  -2.6406,\n",
      "          -2.0469,  -4.0625,  -4.4375,  -4.3125,  -8.8125,  -9.5000,  -5.3438,\n",
      "          -7.0000,  -6.6250,  -8.1250,  -8.1875,  -8.4375,  -7.5625,  -8.3750,\n",
      "          -6.0000,  -6.5312,  -7.6562,  -7.2500,  -6.9062,  -7.2812,  -7.3750,\n",
      "          -7.2500,  -7.5000,  -8.0000,  -6.1875,  -5.9062,  -6.7188,  -7.7188,\n",
      "          -8.2500,  -6.9688,  -6.7188,  -6.0000,  -6.4375,  -8.0625,  -6.1875,\n",
      "          -7.5000,  -8.6250,  -6.5625,  -6.9688,  -7.5000,  -7.5938,  -8.0000,\n",
      "          -7.5000,  -7.3125,  -6.8125,  -8.0000,  -7.4375,  -6.3750,  -7.7188,\n",
      "          -8.4375,  -8.4375,  -8.1250,  -7.9375,  -7.5312,  -7.6562,  -6.7188,\n",
      "          -5.6875,  -7.0000,  -7.9375,  -8.1875,  -8.4375,  -7.3438,  -6.7188,\n",
      "          -8.3750,  -7.9375,  -7.6562,  -7.1250,  -7.1562,  -7.8125,  -7.8750,\n",
      "          -9.3750,  -9.4375,  -6.3125,  -2.9062,  -8.6875,  -8.6875,  -9.3125,\n",
      "          -9.1250,  -9.3750,  -9.3750,  -9.3750,  -9.2500,  -8.7500,  -9.4375,\n",
      "          -8.2500,  -7.1250,  -8.8750,  -8.9375,  -9.5000,  -9.4375,  -9.6250,\n",
      "          -9.3750,  -9.5000,  -9.4375,  -9.3125,  -9.5000,  -9.3750,  -9.4375,\n",
      "          -9.4375,  -9.4375,  -9.5000,  -9.5625,  -9.1250,  -9.5000,  -4.0938,\n",
      "          -7.7812,  -8.3125,  -7.9062,  -8.3125,  -8.3750,  -8.5625,  -8.8750,\n",
      "          -9.2500,  -9.4375,  -9.3750,  -9.4375,  -9.4375,  -7.5000,  -5.6875,\n",
      "          -9.5000,  -9.4375,  -9.3750,  -9.3750,  -9.4375,  -9.5000,  -9.3750,\n",
      "          -9.3750,  -9.4375,  -9.5000,  -9.4375,  -9.4375,  -9.5000,  -9.4375,\n",
      "          -9.3750]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 129\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  4]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 129])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 129, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3281, -0.8438,  3.2344,  ...,  0.9453,  1.4375,  0.2773],\n",
      "        [-0.1826,  0.4316, -0.1064,  ...,  0.7773, -0.8203, -1.2344],\n",
      "        [ 2.4062, -0.0293, -1.9609,  ...,  1.0156, -0.7930, -1.3828]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -43.5000,  -372.0000,   230.0000,  ...,   968.0000,\n",
      "            197.0000,  -131.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  7488.,  -2656., -19072.,  ...,   -462.,   6976., -22272.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 129, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3281, -0.8438,  3.2344,  ...,  0.9453,  1.4375,  0.2773],\n",
      "        [-0.1826,  0.4316, -0.1064,  ...,  0.7773, -0.8203, -1.2344],\n",
      "        [ 2.4062, -0.0293, -1.9609,  ...,  1.0156, -0.7930, -1.3828]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 129]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 129, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.9375, -7.5625,  1.8906, -8.9375, -8.9375, -8.9375, -8.8750, -9.0000,\n",
      "         -9.0000, -8.9375, -9.0625, -9.0625, -9.0000, -5.1250,  2.6094, -9.0000,\n",
      "         -9.0000, -8.7500, -9.0000, -9.0000, -8.8750, -8.9375, -8.9375, -8.8750,\n",
      "         -8.8750, -8.9375, -8.8750, -8.9375, -8.9375, -9.0000, -9.0000, -9.0000,\n",
      "         -8.9375, -8.9375, -8.9375, -8.9375,  4.3750,  0.2832, -1.3594, -3.2188,\n",
      "         -4.0000, -4.3438, -4.1562, -1.3125, -1.6875, -1.7656, -0.0505, -3.9219,\n",
      "          4.0312,  0.1162,  2.5781, -1.4062, -2.0781, -2.4062, -3.5469, -3.1250,\n",
      "         -1.6875, -2.2188, -3.0312, -2.5625, -4.4375, -1.6016, -0.1069, -0.9219,\n",
      "         -1.4375, -1.9219, -4.4062, -1.1484, -4.3750, -2.5312, -3.1562,  0.6875,\n",
      "         -3.9844, -4.5938, -3.6094, -2.2656, -3.5625, -0.8086, -2.4062, -3.9219,\n",
      "         -5.5312, -4.5312, -3.9531, -2.5156, -4.6875, -2.2344, -3.0625, -3.1406,\n",
      "         -2.7344, -2.5781, -3.1875, -3.8125, -3.6250, -3.9375, -4.0625, -3.6719,\n",
      "         -4.3125, -2.6719, -0.8945, -2.8750, -2.6094, -0.3418, -2.2969,  8.3125,\n",
      "          0.6367,  0.3594, -0.7578,  2.2500, -0.2969,  1.3281,  1.3984, -0.0562,\n",
      "         -2.8594, -2.3750,  1.7500, -0.8867, -2.7656, -0.0562, -0.7305,  1.4375,\n",
      "          1.2734, -0.7617, -1.5469, -1.8906, -2.2969, -0.9336,  1.3906, -1.9062,\n",
      "         -2.3750, -2.3750, -7.6875, -9.0000, -4.5312, -5.1875, -5.1562, -6.9375,\n",
      "         -6.4688, -6.9375, -6.1562, -7.9375, -4.6562, -4.1562, -5.9375, -6.1250,\n",
      "         -4.9062, -5.6875, -5.8438, -5.3750, -6.2188, -6.7812, -4.9062, -2.2344,\n",
      "         -4.1875, -6.8125, -7.6875, -5.0000, -4.0000, -3.5156, -4.5000, -6.8750,\n",
      "         -3.3281, -4.3125, -7.8438, -5.7812, -6.0938, -5.8438, -6.3438, -6.6875,\n",
      "         -6.1562, -5.7188, -5.1875, -6.0000, -6.2500, -5.0938, -6.4688, -7.5000,\n",
      "         -7.4062, -7.1562, -7.4062, -6.0625, -5.6875, -4.6875, -4.0625, -5.4062,\n",
      "         -7.4062, -7.2812, -7.4688, -5.7812, -5.2812, -7.2188, -7.1250, -6.7188,\n",
      "         -4.6875, -4.9375, -6.7500, -7.0312, -8.8750, -8.9375, -2.0469, -1.9453,\n",
      "         -8.0625, -8.3750, -8.8750, -8.3750, -8.7500, -8.6875, -8.9375, -8.5625,\n",
      "         -7.8125, -9.0625, -6.7812, -5.0625, -7.9688, -8.0625, -8.9375, -8.9375,\n",
      "         -9.0625, -9.0000, -9.0625, -9.0000, -9.0000, -9.0000, -8.8750, -8.8750,\n",
      "         -9.0000, -9.0625, -8.7500, -8.8750, -8.5625, -8.9375, -2.4688, -6.2188,\n",
      "         -7.4062, -6.7500, -7.2500, -7.3750, -8.0625, -8.1250, -8.6875, -8.8750,\n",
      "         -8.8750, -8.9375, -8.8750, -5.7500, -3.4375, -8.9375, -8.9375, -9.0000,\n",
      "         -9.0000, -9.0000, -8.9375, -9.0000, -8.8750, -9.0000, -8.8750, -9.0000,\n",
      "         -9.0000, -9.0000, -8.8750, -9.0000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 130\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  5]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 130])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 130, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1826,  0.4316, -0.1064,  ...,  0.7773, -0.8203, -1.2344],\n",
      "        [ 2.4062, -0.0293, -1.9609,  ...,  1.0156, -0.7930, -1.3828],\n",
      "        [ 0.0625, -0.7031,  0.7891,  ...,  2.3438,  1.0781,  1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -37.5000,  -440.0000,   272.0000,  ...,  1056.0000,\n",
      "            237.0000,  -144.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  8448.,  -2960., -21632.,  ...,   -564.,   7776., -25216.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 130, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1826,  0.4316, -0.1064,  ...,  0.7773, -0.8203, -1.2344],\n",
      "        [ 2.4062, -0.0293, -1.9609,  ...,  1.0156, -0.7930, -1.3828],\n",
      "        [ 0.0625, -0.7031,  0.7891,  ...,  2.3438,  1.0781,  1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 130]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 16]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 130, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.4375, -6.0312, -2.6875, -8.3750, -8.4375, -8.4375, -8.3750, -8.3750,\n",
      "         -8.4375, -8.3750, -8.5000, -8.5625, -8.5625, -5.0625, -2.5156, -8.5000,\n",
      "         -8.3750, -8.2500, -8.4375, -8.3750, -8.3750, -8.3750, -8.3750, -8.3750,\n",
      "         -8.4375, -8.5000, -8.4375, -8.3750, -8.4375, -8.5000, -8.5000, -8.4375,\n",
      "         -8.3750, -8.4375, -8.4375, -8.3750, -0.4102, -2.8125, -3.1250, -3.0156,\n",
      "         -3.5312, -4.3438, -5.5312, -2.9531, -3.3750, -4.2188, -4.4062, -3.8281,\n",
      "         -0.9570, -2.0781,  0.0962, -2.1719, -4.3125, -2.7812, -3.0469, -3.5156,\n",
      "         -2.6875, -4.0625, -3.8906, -3.3906, -3.4219, -4.0000, -1.8594, -4.2188,\n",
      "         -3.6719, -3.4844, -4.1562, -3.1875, -5.2812, -3.6562, -4.1562, -4.5625,\n",
      "         -4.0000, -3.6250, -3.7969, -4.3750, -3.8281, -0.2266, -3.3125, -5.2188,\n",
      "         -3.3906, -3.9375, -4.0938, -2.4219, -3.8125, -6.9375, -3.6406, -4.2812,\n",
      "         -3.5312, -4.4062, -4.8750, -5.1875, -5.1250, -3.6406, -5.0938, -4.5312,\n",
      "         -5.9062, -6.0625, -5.7812, -4.9375, -4.6562,  2.0625, -1.7812, -1.4531,\n",
      "         -1.8984,  3.9219, -0.8711, -2.2812,  0.7031, 11.5625, -2.9531, -1.2969,\n",
      "          2.8281, -1.7656, -1.5938,  2.6562, -1.7422, -4.5625,  0.8555, -0.0830,\n",
      "          0.4277,  0.7461, -3.1719, -2.5625, -5.2500,  4.1250, -1.7344, -4.4062,\n",
      "         -3.5156, -4.4062, -7.3750, -8.4375, -4.0312, -6.1250, -5.8438, -7.1875,\n",
      "         -6.7188, -6.9688, -6.8438, -7.7812, -6.0938, -5.4375, -6.6250, -6.2812,\n",
      "         -5.9688, -6.3125, -6.9375, -5.9688, -6.6562, -6.5625, -6.2500, -5.8438,\n",
      "         -5.5312, -7.3750, -7.5000, -6.7188, -5.8438, -5.2500, -6.5000, -7.1562,\n",
      "         -4.6875, -5.6250, -7.5312, -5.8438, -6.5938, -6.1250, -6.7500, -6.9375,\n",
      "         -6.6875, -6.0312, -6.7500, -6.5000, -6.7188, -5.5312, -6.7500, -7.4062,\n",
      "         -7.3125, -7.1875, -7.3438, -6.3750, -6.1250, -4.9375, -5.7500, -6.0938,\n",
      "         -7.5000, -7.3125, -7.3125, -6.3750, -5.9062, -7.3438, -7.1250, -6.7812,\n",
      "         -5.6875, -6.4062, -7.0000, -7.0000, -8.3750, -8.5000, -4.7188, -1.6562,\n",
      "         -7.9062, -7.8750, -8.3750, -8.0625, -8.3750, -7.8750, -8.4375, -8.0000,\n",
      "         -7.7500, -8.3750, -7.0000, -6.4688, -7.9375, -7.8438, -8.3125, -8.3750,\n",
      "         -8.4375, -8.5000, -8.5000, -8.5000, -8.4375, -8.3750, -8.4375, -8.4375,\n",
      "         -8.3750, -8.5000, -8.4375, -8.5000, -8.3750, -8.5000, -4.0312, -6.8750,\n",
      "         -7.3438, -7.3125, -7.0625, -7.6875, -7.7500, -8.1250, -8.1875, -8.2500,\n",
      "         -8.2500, -8.3750, -8.4375, -6.8438, -5.8438, -8.4375, -8.5625, -8.3750,\n",
      "         -8.3750, -8.5000, -8.3750, -8.5000, -8.4375, -8.5625, -8.4375, -8.5000,\n",
      "         -8.5000, -8.6250, -8.3750, -8.4375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 131\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  6]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 131])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 131, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4062, -0.0293, -1.9609,  ...,  1.0156, -0.7930, -1.3828],\n",
      "        [ 0.0625, -0.7031,  0.7891,  ...,  2.3438,  1.0781,  1.4219],\n",
      "        [ 1.6094,  0.6953,  0.4473,  ...,  0.3086, -0.2812,  0.0581]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -43.5000,  -460.0000,   296.0000,  ...,  1088.0000,\n",
      "            254.0000,  -160.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  8960.,  -3040., -22912.,  ...,   -652.,   8256., -26496.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 131, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4062, -0.0293, -1.9609,  ...,  1.0156, -0.7930, -1.3828],\n",
      "        [ 0.0625, -0.7031,  0.7891,  ...,  2.3438,  1.0781,  1.4219],\n",
      "        [ 1.6094,  0.6953,  0.4473,  ...,  0.3086, -0.2812,  0.0581]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 131]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 16]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 131, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.4062, -6.3438, -0.4609, -5.3750, -5.5000, -5.5000, -5.5000, -5.4688,\n",
      "         -5.3750, -5.4375, -5.5938, -5.5625, -5.4375, -4.1250,  0.6016, -5.5312,\n",
      "         -5.5312, -5.1562, -5.4375, -5.4688, -5.4375, -5.5312, -5.4688, -5.3750,\n",
      "         -5.4375, -5.4375, -5.4375, -5.4062, -5.4375, -5.5000, -5.4688, -5.3750,\n",
      "         -5.4062, -5.4375, -5.4688, -5.4375,  0.6055, -1.0547, -1.8359, -1.6719,\n",
      "         -2.5625, -3.4688, -4.0312, -1.8359, -1.6328, -3.2500, -2.6094, -2.1406,\n",
      "          0.9258, -0.0544,  0.3320, -1.2812, -5.1562, -3.2500, -3.7812, -3.8438,\n",
      "         -2.4844, -3.4062, -2.0781, -1.8984, -3.0625, -3.0625, -1.5000, -1.4141,\n",
      "         -3.7344, -3.7656, -2.5625, -2.1250, -2.9688, -4.2188, -3.1719, -3.5625,\n",
      "         -2.1406, -3.5000, -3.0156, -4.0938, -4.9062, -2.0312, -2.4062, -4.8438,\n",
      "          1.7969, -4.2500, -3.2500, -1.7266, -1.0781, -4.1562, -2.5469, -3.1719,\n",
      "         -2.5469, -4.5000, -4.0938, -2.8438, -3.0625, -3.0469, -3.8906, -3.8125,\n",
      "         -3.4688, -3.7031, -4.9062, -2.2812, -2.8281,  0.7188, -0.0219,  1.0469,\n",
      "          2.5469,  0.3125,  1.8828, -2.7812, -3.2344, -0.6680, -2.1562, -1.6406,\n",
      "         11.8750,  0.8125,  3.6094,  0.7852,  2.6719, -1.9609,  2.4531,  1.6406,\n",
      "         -0.5234, -0.6328, -0.7695, -2.5469, -1.2422, -1.1562, -1.6328, -4.0625,\n",
      "         -2.7500, -3.5000, -4.7812, -5.5938, -3.9375, -3.9531, -3.7344, -4.8750,\n",
      "         -4.4062, -4.8125, -4.0000, -5.0000, -3.3594, -4.2812, -4.3750, -4.4375,\n",
      "         -5.0312, -4.7188, -4.7188, -4.1250, -4.5000, -4.4375, -4.6250, -3.5938,\n",
      "         -3.9219, -4.7812, -4.8750, -4.7188, -3.9375, -3.2969, -4.6562, -4.7500,\n",
      "         -3.4531, -6.1562, -5.1250, -5.3750, -4.5938, -4.5312, -4.9375, -4.9688,\n",
      "         -4.6875, -4.3750, -4.5312, -4.7188, -4.8125, -4.8125, -4.6250, -4.9062,\n",
      "         -4.7812, -4.9688, -4.7500, -4.0000, -3.8906, -4.2812, -3.9531, -4.8750,\n",
      "         -4.8438, -4.8750, -4.6250, -3.9688, -4.7188, -4.8438, -4.7500, -4.4062,\n",
      "         -4.7500, -5.2188, -4.7812, -4.8438, -5.4062, -5.4375, -2.8438, -2.2656,\n",
      "         -5.0312, -5.0625, -5.4688, -5.1562, -5.3438, -5.4062, -5.4375, -5.3438,\n",
      "         -5.0312, -5.5000, -4.5312, -4.4688, -5.0000, -5.0000, -5.4375, -5.3750,\n",
      "         -5.5625, -5.3750, -5.4375, -5.4062, -5.3750, -5.4375, -5.4375, -5.4375,\n",
      "         -5.4062, -5.4375, -5.4062, -5.5000, -5.2812, -5.5000, -2.7031, -4.5938,\n",
      "         -4.7188, -4.7500, -4.9062, -4.8750, -5.0312, -5.3750, -5.2500, -5.3438,\n",
      "         -5.2500, -5.4375, -5.3750, -4.7812, -3.8906, -5.5000, -5.5312, -5.3750,\n",
      "         -5.3750, -5.4688, -5.4688, -5.3750, -5.4375, -5.5312, -5.5000, -5.4375,\n",
      "         -5.4688, -5.5625, -5.3438, -5.4062]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 132\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  7]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 132])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 132, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0625, -0.7031,  0.7891,  ...,  2.3438,  1.0781,  1.4219],\n",
      "        [ 1.6094,  0.6953,  0.4473,  ...,  0.3086, -0.2812,  0.0581],\n",
      "        [ 0.1299,  1.6250, -0.8203,  ...,  2.0312,  0.4961, -0.2949]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -57.0000,  -484.0000,   318.0000,  ...,  1104.0000,\n",
      "            268.0000,  -176.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9280.,  -3088., -23680.,  ...,   -704.,   8512., -27392.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 132, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0625, -0.7031,  0.7891,  ...,  2.3438,  1.0781,  1.4219],\n",
      "        [ 1.6094,  0.6953,  0.4473,  ...,  0.3086, -0.2812,  0.0581],\n",
      "        [ 0.1299,  1.6250, -0.8203,  ...,  2.0312,  0.4961, -0.2949]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 132]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 16]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 132, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.9062, -5.4375,  5.0312, -6.7812, -6.9375, -6.8125, -6.7812, -6.9375,\n",
      "         -6.9375, -6.9688, -6.7812, -6.8750, -6.7812, -3.3438,  4.9375, -6.9375,\n",
      "         -6.9062, -6.4062, -6.9062, -6.8438, -6.7188, -6.8750, -6.9062, -6.7500,\n",
      "         -6.8438, -6.9062, -6.7812, -6.9062, -6.7500, -6.8438, -6.9375, -6.9062,\n",
      "         -6.8438, -6.9062, -6.8438, -6.8750,  4.8125,  3.1875,  0.0087, -1.7656,\n",
      "         -2.9531, -3.2500, -2.8594, -1.3516, -0.4727,  0.9727,  0.9844, -1.7578,\n",
      "          6.7812,  1.6641,  4.6562, -0.5977, -3.3125, -2.2344, -3.5938, -1.8906,\n",
      "         -2.4531, -2.0938, -2.2500, -3.2344, -2.7031, -3.0312,  0.3320,  1.4531,\n",
      "         -2.0625, -0.7969, -1.4844,  0.0454, -2.5625, -2.3125, -3.7344, -2.9844,\n",
      "         -1.3750, -1.3828, -2.8438, -2.7344, -3.9375, -2.0938, -2.4062, -3.0781,\n",
      "         -3.0469, -3.7188, -2.5000, -0.6211, -3.7500, -3.1719, -3.2656, -1.0391,\n",
      "         -2.7344, -1.9766, -2.6562, -2.2812, -2.6875, -2.6875, -2.8438, -2.6875,\n",
      "         -2.8594,  2.7188, -2.4688, -2.3594, -1.9375,  1.1953, -3.3125, -2.8750,\n",
      "          1.2109,  2.9219, -2.0000, -1.5859, -2.0156,  1.9922, -2.2500, -2.8281,\n",
      "          0.9531, -1.6172, -0.8438,  2.3438, -3.0156, -1.6797, -2.0469,  2.4688,\n",
      "         -1.0234,  0.5078, -1.6719, -1.9375, -2.2969, -1.5391, -2.1094, -3.4844,\n",
      "         -1.9219, -1.0938, -5.5000, -7.0312, -3.1406, -4.1250, -3.8438, -5.0312,\n",
      "         -4.9375, -4.9062, -5.0625, -5.9688, -2.5469, -3.7344, -4.9062, -4.6562,\n",
      "         -3.8438, -4.0625, -4.1250, -4.3125, -4.4688, -4.9688, -3.9844, -1.5078,\n",
      "         -2.2188, -5.3438, -5.9375, -3.4219, -3.6250, -1.0000, -3.2656, -5.1875,\n",
      "         -2.9531, -4.9062, -6.1250, -3.9531, -4.8125, -4.4375, -4.2188, -5.3438,\n",
      "         -4.7812, -4.4688, -4.0938, -4.5312, -4.6250, -3.1875, -5.0000, -5.5938,\n",
      "         -5.4375, -5.3750, -5.5312, -4.1875, -4.3125, -3.6094, -3.4375, -4.3125,\n",
      "         -5.4688, -5.5000, -5.2500, -4.0312, -4.5000, -5.5000, -4.9688, -4.9375,\n",
      "         -4.2812, -4.8125, -5.0938, -5.0938, -6.8438, -6.9062, -2.9844, -0.7500,\n",
      "         -6.2188, -6.1875, -6.9062, -6.4688, -6.6875, -6.8750, -6.9062, -6.5000,\n",
      "         -6.2188, -6.9375, -4.6562, -3.8906, -6.0312, -6.0312, -6.8750, -6.7812,\n",
      "         -7.0000, -6.8750, -6.8750, -6.8438, -7.0000, -6.8750, -6.8750, -6.8750,\n",
      "         -6.8750, -6.7812, -6.8438, -6.8750, -6.5625, -7.0000, -1.4141, -4.9688,\n",
      "         -5.4062, -5.2812, -5.3750, -6.0938, -6.0312, -6.3125, -6.6562, -6.9062,\n",
      "         -6.7188, -6.9688, -6.8125, -5.0000, -2.8125, -6.9062, -6.9062, -6.7812,\n",
      "         -6.8750, -6.9688, -6.9062, -6.8438, -6.8125, -6.9375, -6.9375, -6.9062,\n",
      "         -6.9375, -6.9375, -6.8438, -6.9375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 133\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 136, 133])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 133, 256]), Cross output shape: torch.Size([1, 136, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.6094,  0.6953,  0.4473,  ...,  0.3086, -0.2812,  0.0581],\n",
      "        [ 0.1299,  1.6250, -0.8203,  ...,  2.0312,  0.4961, -0.2949],\n",
      "        [-1.6250,  0.5469, -0.4121,  ..., -0.2344, -0.8203, -1.4297]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 17, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -25.1250,  -456.0000,   304.0000,  ...,  1040.0000,\n",
      "            264.0000,  -168.0000],\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 17, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  8832.,  -3056., -22656.,  ...,   -692.,   8192., -26240.],\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 133, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.6094,  0.6953,  0.4473,  ...,  0.3086, -0.2812,  0.0581],\n",
      "        [ 0.1299,  1.6250, -0.8203,  ...,  2.0312,  0.4961, -0.2949],\n",
      "        [-1.6250,  0.5469, -0.4121,  ..., -0.2344, -0.8203, -1.4297]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 133]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 133, 136])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-4.5938, -6.0938,  2.7031, -4.6562, -4.6562, -4.7188, -4.5312, -4.6250,\n",
      "         -4.7500, -4.6875, -4.5938, -4.6875, -4.6562, -2.3281,  4.9375, -4.7500,\n",
      "         -4.5312, -4.5312, -4.6875, -4.5625, -4.6250, -4.5938, -4.5938, -4.6250,\n",
      "         -4.6562, -4.6250, -4.6875, -4.6875, -4.7500, -4.6562, -4.6250, -4.5938,\n",
      "         -4.6250, -4.7500, -4.7188, -4.6562,  9.5625, -0.7070,  1.0781, -1.1797,\n",
      "         -2.5625, -4.1250, -0.6836, -2.0938, -0.6094,  0.4375,  0.0284, -1.6484,\n",
      "          0.8906,  0.3789,  1.5234,  1.7891, -1.8594, -1.0703, -1.2891, -2.1562,\n",
      "         -2.1875, -2.0469, -2.6875, -3.1562, -3.7500, -3.2031,  1.1797, -2.1250,\n",
      "          0.0713,  1.1328,  0.3906, -1.0391, -3.5938, -2.0938, -2.0156, -1.0703,\n",
      "         -2.0469, -3.0625, -1.8984, -3.1562, -2.3281, -1.3906, -2.3906, -2.7188,\n",
      "         -3.0469, -0.8633, -1.5859, -1.5625, -2.0312, -2.8594, -1.4922, -1.1875,\n",
      "         -2.1250, -2.3281, -0.0559, -1.4609, -3.2031, -2.7656, -2.2969, -1.6953,\n",
      "         -3.6719,  0.1768, -2.4844, -1.6250, -4.8750, -1.2500, -1.3672, -0.4414,\n",
      "         -0.7969, -2.0312, -1.5156, -1.5781, -1.7969, -0.1904, -1.5156, -0.6484,\n",
      "         -0.8555, -0.4180, -0.0515, -1.6484, -1.3438, -3.1875, -0.7930, -0.2275,\n",
      "         -0.4551, -2.0625, -0.8711, -0.6992, -2.3750, -1.7656, -2.2969, -2.7500,\n",
      "         -0.6484, -1.0781, -4.3125, -4.6875, -3.5156, -4.1250, -3.9375, -4.1562,\n",
      "         -4.1875, -4.3125, -3.8125, -4.2500, -3.2344, -3.8750, -3.9531, -4.0625,\n",
      "         -3.7656, -3.8125, -4.0938, -3.5312, -3.6562, -4.1562, -2.5469, -3.1719,\n",
      "         -3.1094, -4.0938, -4.0000, -1.9297, -3.8125, -4.1875, -3.1094, -3.9688,\n",
      "         -2.7500, -2.4375, -4.3125, -3.9375, -3.7344, -3.7344, -3.9844, -3.8906,\n",
      "         -3.7500, -3.7656, -3.8906, -3.3906, -3.9219, -2.9844, -3.7500, -4.2812,\n",
      "         -4.2812, -4.0625, -3.9844, -3.8281, -4.0312, -3.4531, -3.2656, -3.5312,\n",
      "         -4.0625, -4.0938, -4.2500, -3.4531, -3.6875, -3.9688, -3.9531, -3.5469,\n",
      "         -3.1562, -2.8594, -3.7656, -4.0000, -4.5625, -4.6875, -2.0625, -1.2109,\n",
      "         -4.2188, -4.3750, -4.5625, -4.5312, -4.6562, -4.6562, -4.6562, -4.4062,\n",
      "         -4.3125, -4.6875, -4.0938, -3.8438, -4.2188, -4.1875, -4.6250, -4.6875,\n",
      "         -4.8438, -4.7188, -4.5625, -4.6562, -4.6250, -4.6250, -4.5938, -4.6562,\n",
      "         -4.5938, -4.6250, -4.6562, -4.6562, -4.5938, -4.6562, -1.9609, -3.8125,\n",
      "         -4.1562, -4.1562, -4.1875, -4.4062, -4.3438, -4.5938, -4.5938, -4.5312,\n",
      "         -4.4062, -4.7188, -4.6250, -4.0000, -3.7344, -4.7188, -4.7188, -4.5625,\n",
      "         -4.7188, -4.5938, -4.6562, -4.6562, -4.6875, -4.6875, -4.6562, -4.6250,\n",
      "         -4.7812, -4.7188, -4.7188, -4.6562]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 134\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 134])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 134, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.1299,  1.6250, -0.8203,  ...,  2.0312,  0.4961, -0.2949],\n",
      "        [-1.6250,  0.5469, -0.4121,  ..., -0.2344, -0.8203, -1.4297],\n",
      "        [-0.8906,  1.0625,  1.9297,  ...,  0.5664, -0.3379, -0.6602]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  108.0000,   -93.0000,   -65.5000,  ...,   314.0000,\n",
      "             30.1250,   -21.2500]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  2240.,  -1456.,  -5312.,  ...,    104.,   2272.,  -7424.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 134, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.1299,  1.6250, -0.8203,  ...,  2.0312,  0.4961, -0.2949],\n",
      "        [-1.6250,  0.5469, -0.4121,  ..., -0.2344, -0.8203, -1.4297],\n",
      "        [-0.8906,  1.0625,  1.9297,  ...,  0.5664, -0.3379, -0.6602]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 134]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 17]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 134, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.6250,  -9.8750,  -5.1562, -10.6250, -10.6250, -10.6250, -10.6875,\n",
      "         -10.5625, -10.5625, -10.6250, -10.6250, -10.6250, -10.6250,  -7.0938,\n",
      "          -0.3047, -10.7500, -10.6875, -10.1250, -10.6875, -10.7500, -10.6250,\n",
      "         -10.6250, -10.6250, -10.7500, -10.6250, -10.6250, -10.7500, -10.6250,\n",
      "         -10.6875, -10.6250, -10.6875, -10.6250, -10.6875, -10.6250, -10.6875,\n",
      "         -10.6875,  -3.9062,  -6.3750,  -1.4688,  -4.0312,  -6.2812,  -6.5625,\n",
      "          -5.3750,  -5.0625,  -2.6719,  -7.0625,  -5.2188,  -5.1562,  -4.2188,\n",
      "          -2.5625,  -3.2188,  -2.8750,  -5.3125,  -3.6562,  -4.6250,  -4.9062,\n",
      "          -5.3438,  -5.4688,  -6.2500,  -6.3438,  -5.5625,  -5.8125,  -6.0625,\n",
      "          -4.4375,  -5.9375,  -5.7812,  -6.0312,  -8.5000,  -7.0312,  -3.5469,\n",
      "          -4.0312,  -4.2812,  -5.6875,  -4.2188,  -4.0312,  -4.0625,  -4.4375,\n",
      "          -1.8906,  -2.3438,  -4.5000,  -4.4688,  -3.5000,  -5.2188,  -5.3438,\n",
      "          -2.1875,  -6.1250,  -3.7656,  -3.1562,  -4.1250,  -4.1250,  -5.9688,\n",
      "          -5.3438,  -6.1250,  -6.3125,  -5.8125,  -4.3750,  -7.9688,  -5.6875,\n",
      "          -6.6875,  -4.1250,  -5.9375,   3.0156,   2.3750,   1.5547,   0.7734,\n",
      "           0.5820,   1.4922,   1.0781,   0.3828,   1.0547,   1.1719,  -0.1768,\n",
      "           0.2080,   1.1875,  -0.9844,   0.7852,   3.5312,  -1.0469,   0.9609,\n",
      "           1.3281,   1.8594,  -0.0294,  -0.6562,   0.4941,  -5.5312,  -0.0481,\n",
      "          -3.2344,  -5.4375,  -6.1250,  -5.1875,  -9.3750, -10.6875,  -7.0000,\n",
      "          -8.8125,  -8.6250,  -9.4375,  -9.3750,  -9.1875,  -8.6875, -10.1250,\n",
      "          -7.7188,  -8.1250,  -9.2500,  -8.7500,  -8.8125,  -9.4375,  -9.2500,\n",
      "          -9.0000,  -9.3125,  -9.5000,  -7.6250,  -7.1875,  -8.0625,  -9.8125,\n",
      "          -9.7500,  -8.3750,  -8.2500,  -9.6875,  -7.0312,  -9.7500,  -7.4688,\n",
      "          -7.4375, -10.0000,  -8.6250,  -8.5000,  -8.9375,  -8.7500,  -9.5625,\n",
      "          -9.1250,  -8.8125,  -8.3125,  -9.1250,  -9.3125,  -7.0000,  -9.1875,\n",
      "          -9.6875,  -9.9375,  -9.8750,  -9.5000,  -9.3125,  -6.9062,  -7.9062,\n",
      "          -8.6250,  -8.4375,  -9.5000,  -9.5625,  -9.7500,  -8.7500,  -8.7500,\n",
      "          -9.6250,  -9.4375,  -9.2500,  -8.1250,  -8.0625,  -9.3750,  -9.4375,\n",
      "         -10.6875, -10.6250,  -6.4688,  -4.4375,  -9.9375, -10.0000, -10.5625,\n",
      "         -10.3125, -10.5000, -10.1875, -10.6250, -10.4375, -10.1875, -10.6250,\n",
      "          -9.1875,  -8.8750,  -9.9375,  -9.6875, -10.5625, -10.6875, -10.6875,\n",
      "         -10.6875, -10.6875, -10.6875, -10.6250, -10.5625, -10.6250, -10.6875,\n",
      "         -10.5625, -10.6875, -10.6875, -10.6875, -10.4375, -10.6250,  -4.1562,\n",
      "          -9.2500,  -9.8125,  -9.5000,  -9.6250,  -9.8125, -10.0000, -10.3125,\n",
      "         -10.4375, -10.6250, -10.5625, -10.6250, -10.6875,  -9.1875,  -7.0625,\n",
      "         -10.6875, -10.6875, -10.6250, -10.6250, -10.6875, -10.6250, -10.8125,\n",
      "         -10.6875, -10.6875, -10.6250, -10.7500, -10.6875, -10.6875, -10.6875,\n",
      "         -10.6250]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 135\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  2]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 135])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 135, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6250,  0.5469, -0.4121,  ..., -0.2344, -0.8203, -1.4297],\n",
      "        [-0.8906,  1.0625,  1.9297,  ...,  0.5664, -0.3379, -0.6602],\n",
      "        [-0.3945, -0.7578,  3.2188,  ...,  1.1562,  1.4609,  0.2930]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [   -8.2500,  -245.0000,    89.5000,  ...,   816.0000,\n",
      "            129.0000,   -51.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  5440.,  -2032., -13696.,  ...,   -241.,   5120., -16384.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 135, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6250,  0.5469, -0.4121,  ..., -0.2344, -0.8203, -1.4297],\n",
      "        [-0.8906,  1.0625,  1.9297,  ...,  0.5664, -0.3379, -0.6602],\n",
      "        [-0.3945, -0.7578,  3.2188,  ...,  1.1562,  1.4609,  0.2930]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 135]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 17]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 135, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.2812e+00, -8.1250e+00, -3.3125e+00, -6.2500e+00, -6.2500e+00,\n",
      "         -6.1875e+00, -6.2500e+00, -6.2812e+00, -6.2188e+00, -6.1562e+00,\n",
      "         -6.2188e+00, -6.1875e+00, -6.2188e+00, -4.7188e+00, -2.5781e+00,\n",
      "         -6.2188e+00, -6.1875e+00, -5.9688e+00, -6.2188e+00, -6.1875e+00,\n",
      "         -6.1875e+00, -6.2188e+00, -6.0000e+00, -6.2500e+00, -6.1875e+00,\n",
      "         -6.1250e+00, -6.2188e+00, -6.2188e+00, -6.2500e+00, -6.1562e+00,\n",
      "         -6.2188e+00, -6.1250e+00, -6.1562e+00, -6.1875e+00, -6.1250e+00,\n",
      "         -6.1875e+00, -1.8828e+00, -4.5938e+00, -3.6406e+00, -3.9375e+00,\n",
      "         -4.5938e+00, -4.5000e+00, -4.2500e+00, -3.3750e+00, -3.0000e+00,\n",
      "         -3.3594e+00, -3.8281e+00, -5.1875e+00, -1.5547e+00, -2.9531e+00,\n",
      "         -9.5312e-01, -3.6094e+00, -4.1875e+00, -4.3438e+00, -3.8281e+00,\n",
      "         -3.7500e+00, -4.0938e+00, -5.4375e+00, -3.8906e+00, -5.2812e+00,\n",
      "         -4.4375e+00, -5.5312e+00, -3.5625e+00, -2.7969e+00, -5.3125e+00,\n",
      "         -4.5000e+00, -2.8750e+00, -5.0000e+00, -4.6250e+00, -4.1875e+00,\n",
      "         -5.8125e+00, -4.7188e+00, -4.9375e+00, -1.1250e+00, -5.0312e+00,\n",
      "         -4.2812e+00, -3.4688e+00, -4.0625e+00, -5.1250e+00, -5.0312e+00,\n",
      "         -4.2500e+00, -4.6562e+00, -4.4375e+00, -4.2500e+00, -4.1250e+00,\n",
      "         -6.2188e+00, -4.5938e+00, -4.5312e+00, -4.0312e+00, -3.5156e+00,\n",
      "         -5.0625e+00, -4.5938e+00, -4.9375e+00, -4.7500e+00, -4.5312e+00,\n",
      "         -3.9844e+00, -5.0000e+00, -4.8438e+00, -5.3750e+00, -3.8438e+00,\n",
      "         -6.1875e+00,  3.6875e+00, -1.9922e+00, -1.3828e+00, -1.4766e+00,\n",
      "          9.5000e+00, -2.6406e+00, -1.4375e+00,  2.4219e+00,  4.1875e+00,\n",
      "         -3.8594e+00, -2.1094e+00,  2.4531e+00, -1.2578e+00, -8.2812e-01,\n",
      "          4.0625e+00, -7.6172e-01, -3.3438e+00,  3.0000e+00, -6.5918e-02,\n",
      "         -9.6484e-01,  4.1562e+00, -2.3281e+00, -1.7812e+00, -3.8125e+00,\n",
      "          1.1641e+00, -1.5312e+00, -3.9844e+00, -4.7188e+00, -3.4688e+00,\n",
      "         -5.7188e+00, -6.2188e+00, -3.8750e+00, -5.2188e+00, -5.2500e+00,\n",
      "         -5.7188e+00, -5.7812e+00, -5.6875e+00, -5.5312e+00, -5.8438e+00,\n",
      "         -5.2500e+00, -5.2188e+00, -5.4062e+00, -5.2500e+00, -5.2500e+00,\n",
      "         -5.3125e+00, -5.4375e+00, -5.6250e+00, -5.4062e+00, -5.4375e+00,\n",
      "         -5.0625e+00, -6.0000e+00, -5.1562e+00, -5.7188e+00, -5.7812e+00,\n",
      "         -5.5000e+00, -5.4688e+00, -4.8750e+00, -4.3438e+00, -5.8438e+00,\n",
      "         -5.0625e+00, -4.2812e+00, -5.8125e+00, -5.2188e+00, -5.1562e+00,\n",
      "         -5.5625e+00, -5.5000e+00, -5.8438e+00, -5.6875e+00, -5.1562e+00,\n",
      "         -4.7812e+00, -5.8438e+00, -5.4062e+00, -5.2812e+00, -5.6250e+00,\n",
      "         -5.7812e+00, -5.7812e+00, -5.7500e+00, -5.5938e+00, -5.6875e+00,\n",
      "         -4.5625e+00, -4.7500e+00, -4.5312e+00, -5.0312e+00, -5.7188e+00,\n",
      "         -5.6562e+00, -5.6562e+00, -5.3438e+00, -5.4688e+00, -5.7500e+00,\n",
      "         -5.6562e+00, -5.7188e+00, -5.1250e+00, -5.2188e+00, -5.4688e+00,\n",
      "         -5.4062e+00, -6.1562e+00, -6.0625e+00, -4.4688e+00, -2.7771e-03,\n",
      "         -5.9375e+00, -5.9688e+00, -6.1562e+00, -6.0625e+00, -6.1250e+00,\n",
      "         -6.0938e+00, -6.2500e+00, -6.2500e+00, -5.9375e+00, -6.2188e+00,\n",
      "         -5.8125e+00, -5.0312e+00, -5.9062e+00, -5.7500e+00, -6.1875e+00,\n",
      "         -6.2188e+00, -6.3125e+00, -6.2188e+00, -6.2500e+00, -6.2812e+00,\n",
      "         -6.1875e+00, -6.2188e+00, -6.2188e+00, -6.1562e+00, -6.1875e+00,\n",
      "         -6.2812e+00, -6.2188e+00, -6.1875e+00, -6.1250e+00, -6.2500e+00,\n",
      "         -4.3750e+00, -5.7188e+00, -5.7812e+00, -5.4688e+00, -5.7812e+00,\n",
      "         -5.7812e+00, -5.8125e+00, -6.0625e+00, -6.0000e+00, -6.2188e+00,\n",
      "         -6.0938e+00, -6.2500e+00, -6.2188e+00, -5.4062e+00, -4.1250e+00,\n",
      "         -6.2188e+00, -6.2500e+00, -6.1875e+00, -6.2188e+00, -6.2812e+00,\n",
      "         -6.1562e+00, -6.2188e+00, -6.1875e+00, -6.2188e+00, -6.2500e+00,\n",
      "         -6.2812e+00, -6.1562e+00, -6.1562e+00, -6.2812e+00, -6.1875e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 136\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  3]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 136])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 136, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8906,  1.0625,  1.9297,  ...,  0.5664, -0.3379, -0.6602],\n",
      "        [-0.3945, -0.7578,  3.2188,  ...,  1.1562,  1.4609,  0.2930],\n",
      "        [-0.2217,  0.4961,  0.0811,  ...,  0.8359, -0.9219, -1.0391]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -50.0000,  -316.0000,   162.0000,  ...,   904.0000,\n",
      "            161.0000,  -107.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  6656.,  -2384., -17024.,  ...,   -372.,   6208., -19968.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 136, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8906,  1.0625,  1.9297,  ...,  0.5664, -0.3379, -0.6602],\n",
      "        [-0.3945, -0.7578,  3.2188,  ...,  1.1562,  1.4609,  0.2930],\n",
      "        [-0.2217,  0.4961,  0.0811,  ...,  0.8359, -0.9219, -1.0391]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 136]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 17]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 136, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[ -9.5000, -10.6250,  -4.7812,  -9.5000,  -9.6250,  -9.4375,  -9.5000,\n",
      "          -9.5000,  -9.5625,  -9.5625,  -9.6250,  -9.5625,  -9.5000,  -6.0938,\n",
      "          -2.6719,  -9.5625,  -9.6875,  -9.2500,  -9.5625,  -9.5000,  -9.5000,\n",
      "          -9.5625,  -9.4375,  -9.4375,  -9.4375,  -9.5625,  -9.4375,  -9.5000,\n",
      "          -9.5625,  -9.5000,  -9.5625,  -9.5000,  -9.4375,  -9.5000,  -9.4375,\n",
      "          -9.6875,  -0.9102,  -4.8750,  -2.3594,  -6.0312,  -6.5312,  -5.0312,\n",
      "          -5.3750,  -3.0000,  -4.4375,  -4.2188,  -5.3125,  -5.5000,  -0.7891,\n",
      "          -1.7266,  -0.5586,  -3.1406,  -3.8594,  -4.4688,  -4.6250,  -4.5938,\n",
      "          -4.0625,  -5.5938,  -5.1562,  -4.0625,  -6.0000,  -5.3438,  -3.0156,\n",
      "          -3.5312,  -5.6562,  -4.7500,  -4.0000,  -4.5625,  -6.7188,  -4.5938,\n",
      "          -5.7188,  -4.5000,  -6.0625,  -5.0625,  -5.7812,  -4.5625,  -5.2812,\n",
      "          -5.0625,  -4.9375,  -5.8125,  -5.8750,  -4.5312,  -1.7891,  -5.2500,\n",
      "          -3.8125,  -4.8750,  -5.4375,  -3.5938,  -3.9531,  -5.7812,  -5.8438,\n",
      "          -4.7188,  -4.3125,  -6.3125,  -5.7812,  -5.8750,  -7.0312,  -5.9375,\n",
      "          -5.1562,  -5.0312,  -5.8125,   4.1875,  -1.1094,   1.3594,   0.2773,\n",
      "           3.0625,  -2.1562,  -0.0388,  -1.4688,  -1.0156,  -3.1250,  -0.8242,\n",
      "           1.8203,  -0.2949,   9.7500,   1.1641,   3.0781,  -0.8359,   3.4844,\n",
      "           0.9023,   2.8125,  -0.7148,  -0.7930,  -0.5859,  -0.3848,  -2.4219,\n",
      "          -1.8203,  -4.1562,  -4.5312,  -4.2812,  -8.8750,  -9.5625,  -5.5000,\n",
      "          -7.1250,  -6.7500,  -8.1875,  -8.3125,  -8.5000,  -7.6562,  -8.5000,\n",
      "          -6.1250,  -6.5938,  -7.7500,  -7.3438,  -7.0000,  -7.3750,  -7.4688,\n",
      "          -7.3750,  -7.6250,  -8.1250,  -6.3438,  -6.0938,  -6.8438,  -7.8125,\n",
      "          -8.3750,  -7.1250,  -6.9375,  -6.2500,  -6.4688,  -8.1875,  -6.4062,\n",
      "          -7.6562,  -8.7500,  -6.7500,  -7.0312,  -7.6562,  -7.6875,  -8.1250,\n",
      "          -7.6562,  -7.4375,  -6.9062,  -8.1250,  -7.5625,  -6.5000,  -7.8438,\n",
      "          -8.5000,  -8.5625,  -8.2500,  -8.0000,  -7.6562,  -7.7188,  -6.8438,\n",
      "          -5.8750,  -7.1562,  -8.0625,  -8.2500,  -8.5625,  -7.4375,  -6.8438,\n",
      "          -8.5000,  -8.0625,  -7.8125,  -7.2812,  -7.3750,  -7.9375,  -7.9688,\n",
      "          -9.5000,  -9.5625,  -6.4062,  -2.9375,  -8.7500,  -8.8125,  -9.4375,\n",
      "          -9.2500,  -9.5000,  -9.4375,  -9.5000,  -9.3750,  -8.8750,  -9.5000,\n",
      "          -8.3750,  -7.2500,  -9.0000,  -9.0000,  -9.6250,  -9.5625,  -9.7500,\n",
      "          -9.5000,  -9.5625,  -9.5000,  -9.4375,  -9.5625,  -9.5000,  -9.5000,\n",
      "          -9.5000,  -9.5000,  -9.5625,  -9.6875,  -9.2500,  -9.6250,  -4.3438,\n",
      "          -7.9062,  -8.3750,  -8.0000,  -8.4375,  -8.4375,  -8.6250,  -8.9375,\n",
      "          -9.3750,  -9.5625,  -9.5000,  -9.5000,  -9.5000,  -7.5938,  -5.7812,\n",
      "          -9.6250,  -9.5625,  -9.5000,  -9.5000,  -9.5625,  -9.6250,  -9.4375,\n",
      "          -9.5000,  -9.5625,  -9.5625,  -9.5625,  -9.5000,  -9.6250,  -9.5000,\n",
      "          -9.5000]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 137\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  4]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 137])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 137, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3945, -0.7578,  3.2188,  ...,  1.1562,  1.4609,  0.2930],\n",
      "        [-0.2217,  0.4961,  0.0811,  ...,  0.8359, -0.9219, -1.0391],\n",
      "        [ 2.4375, -0.1113, -1.7656,  ...,  1.0938, -0.9062, -1.3594]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -45.5000,  -372.0000,   234.0000,  ...,   968.0000,\n",
      "            197.0000,  -130.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  7520.,  -2640., -19200.,  ...,   -468.,   6976., -22400.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 137, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3945, -0.7578,  3.2188,  ...,  1.1562,  1.4609,  0.2930],\n",
      "        [-0.2217,  0.4961,  0.0811,  ...,  0.8359, -0.9219, -1.0391],\n",
      "        [ 2.4375, -0.1113, -1.7656,  ...,  1.0938, -0.9062, -1.3594]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 137]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 17]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 137, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.4375, -7.1562,  2.1250, -8.5000, -8.4375, -8.4375, -8.4375, -8.5000,\n",
      "         -8.5000, -8.5000, -8.5625, -8.6250, -8.5000, -4.9062,  3.1406, -8.5000,\n",
      "         -8.5625, -8.2500, -8.5625, -8.5625, -8.4375, -8.5000, -8.4375, -8.4375,\n",
      "         -8.3750, -8.5000, -8.4375, -8.4375, -8.4375, -8.5000, -8.5625, -8.5625,\n",
      "         -8.4375, -8.4375, -8.4375, -8.4375,  4.2500,  0.4277, -1.0547, -3.1094,\n",
      "         -3.8750, -4.0625, -3.8906, -1.2734, -1.5703, -1.3438,  0.0413, -3.7656,\n",
      "          4.1875,  0.0913,  3.3750, -1.2344, -1.6875, -2.2656, -3.3906, -2.9844,\n",
      "         -1.4922, -2.0781, -2.9375, -2.3438, -4.2188, -1.2500, -0.2637, -0.4980,\n",
      "         -1.1719, -1.8984, -4.2812, -0.9141, -4.1250, -2.5000, -3.0625,  0.7070,\n",
      "         -3.8594, -4.5312, -3.5156, -2.2188, -3.5156, -0.7930, -2.2500, -3.6562,\n",
      "         -5.3750, -4.2188, -3.8281, -2.4219, -4.6562, -2.0938, -2.9375, -3.1719,\n",
      "         -2.6562, -2.4844, -2.8750, -3.6094, -3.4219, -3.8750, -3.7969, -3.5000,\n",
      "         -4.1250, -2.2344, -0.7852, -2.9688, -2.4062, -0.4141, -2.5000,  8.1250,\n",
      "          0.5703,  0.3301, -0.9102,  2.0312, -0.4570,  1.2812,  1.2734,  0.0089,\n",
      "         -2.8594, -2.2656,  1.7500, -0.9844, -2.8281, -0.0165, -0.6602,  1.1484,\n",
      "          1.3672, -0.9219, -1.5000, -1.8281, -2.1406, -0.8633,  1.5078, -1.7344,\n",
      "         -2.3750, -2.0156, -7.2500, -8.5625, -4.3750, -4.9375, -4.9062, -6.5625,\n",
      "         -6.1250, -6.5625, -5.8125, -7.5312, -4.5000, -3.9219, -5.5625, -5.7500,\n",
      "         -4.5938, -5.3438, -5.4688, -5.0625, -5.8438, -6.4062, -4.5938, -1.9922,\n",
      "         -3.9062, -6.4375, -7.3125, -4.7812, -3.7812, -3.2500, -4.2812, -6.5000,\n",
      "         -3.2031, -4.1562, -7.4062, -5.5312, -5.6875, -5.4688, -5.9688, -6.3125,\n",
      "         -5.7812, -5.4062, -4.9375, -5.6562, -5.8750, -4.8125, -6.0938, -7.0938,\n",
      "         -7.0000, -6.7500, -7.0000, -5.7188, -5.3438, -4.4375, -3.9062, -5.0938,\n",
      "         -7.0312, -6.8438, -7.0625, -5.4688, -5.0000, -6.8125, -6.7500, -6.3438,\n",
      "         -4.4062, -4.6562, -6.4062, -6.6562, -8.4375, -8.4375, -1.9922, -1.7422,\n",
      "         -7.5938, -7.9375, -8.4375, -7.9375, -8.3125, -8.2500, -8.4375, -8.1250,\n",
      "         -7.3750, -8.5625, -6.4375, -4.8125, -7.5000, -7.5938, -8.4375, -8.5000,\n",
      "         -8.5625, -8.5625, -8.6250, -8.5000, -8.5000, -8.5000, -8.4375, -8.4375,\n",
      "         -8.5000, -8.5625, -8.2500, -8.4375, -8.1250, -8.4375, -2.1875, -5.8750,\n",
      "         -7.0312, -6.3750, -6.8750, -7.0000, -7.5938, -7.6875, -8.1875, -8.3750,\n",
      "         -8.3750, -8.5000, -8.3750, -5.3750, -3.1562, -8.5000, -8.5000, -8.5000,\n",
      "         -8.5625, -8.5000, -8.4375, -8.5000, -8.3750, -8.5625, -8.3750, -8.5625,\n",
      "         -8.5000, -8.5000, -8.4375, -8.5000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 138\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  5]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 138])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 138, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2217,  0.4961,  0.0811,  ...,  0.8359, -0.9219, -1.0391],\n",
      "        [ 2.4375, -0.1113, -1.7656,  ...,  1.0938, -0.9062, -1.3594],\n",
      "        [ 0.0000, -0.7188,  0.8828,  ...,  2.2500,  1.0469,  1.4609]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -38.5000,  -440.0000,   280.0000,  ...,  1040.0000,\n",
      "            239.0000,  -144.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  8512.,  -2944., -21760.,  ...,   -576.,   7840., -25344.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 138, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2217,  0.4961,  0.0811,  ...,  0.8359, -0.9219, -1.0391],\n",
      "        [ 2.4375, -0.1113, -1.7656,  ...,  1.0938, -0.9062, -1.3594],\n",
      "        [ 0.0000, -0.7188,  0.8828,  ...,  2.2500,  1.0469,  1.4609]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 138]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 17]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 138, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.5625, -6.0625, -2.6406, -8.5000, -8.5625, -8.5625, -8.5000, -8.5000,\n",
      "         -8.5625, -8.5000, -8.6250, -8.6875, -8.6875, -5.0000, -2.6719, -8.6250,\n",
      "         -8.5000, -8.3125, -8.5625, -8.5000, -8.5000, -8.5000, -8.5000, -8.5000,\n",
      "         -8.5000, -8.5625, -8.5625, -8.5000, -8.5625, -8.6250, -8.6250, -8.5625,\n",
      "         -8.5000, -8.5000, -8.5625, -8.5000, -0.3359, -2.9062, -3.1406, -3.0625,\n",
      "         -3.5469, -4.4688, -5.5312, -2.8906, -3.4219, -4.2188, -4.2500, -3.7188,\n",
      "         -0.8711, -2.1562, -0.0347, -2.1250, -4.3438, -2.8594, -3.0625, -3.4375,\n",
      "         -2.6250, -4.1562, -3.9219, -3.3281, -3.3750, -3.9531, -1.7344, -4.2188,\n",
      "         -3.6875, -3.5156, -4.2812, -3.2812, -5.3125, -3.6406, -4.2500, -4.6250,\n",
      "         -4.0625, -3.6250, -3.8906, -4.5625, -3.7969, -0.3281, -3.3281, -5.0938,\n",
      "         -3.4375, -3.8750, -4.2188, -2.4375, -3.7969, -7.0000, -3.6875, -4.2812,\n",
      "         -3.6094, -4.4062, -4.7812, -5.3438, -5.1875, -3.7344, -5.0312, -4.5625,\n",
      "         -5.9062, -6.0312, -5.8125, -4.9688, -4.7500,  2.2188, -1.8516, -1.5000,\n",
      "         -2.1250,  4.0312, -0.9336, -2.4531,  0.9570, 11.3750, -3.1562, -1.2812,\n",
      "          2.7656, -1.7344, -1.8672,  2.7344, -1.7969, -4.6250,  1.0312, -0.0413,\n",
      "          0.3359,  0.9609, -3.1406, -2.5781, -5.3125,  4.2812, -1.7656, -4.4062,\n",
      "         -3.4688, -4.3750, -7.5000, -8.5625, -4.1562, -6.1875, -5.9062, -7.2812,\n",
      "         -6.8125, -7.0625, -6.9375, -7.9062, -6.1250, -5.5000, -6.7188, -6.3438,\n",
      "         -6.0312, -6.3750, -7.0312, -6.1250, -6.7500, -6.6562, -6.2500, -5.7812,\n",
      "         -5.5938, -7.5000, -7.5938, -6.7188, -5.9375, -5.3125, -6.5312, -7.2500,\n",
      "         -4.8125, -5.7500, -7.6250, -6.0312, -6.6875, -6.2188, -6.7812, -7.0625,\n",
      "         -6.7812, -6.0938, -6.8125, -6.6250, -6.8125, -5.5625, -6.8438, -7.5312,\n",
      "         -7.4062, -7.3125, -7.4375, -6.4688, -6.1875, -5.0312, -5.9375, -6.2188,\n",
      "         -7.6250, -7.3750, -7.4062, -6.5000, -6.0000, -7.4688, -7.2500, -6.8750,\n",
      "         -5.7812, -6.4688, -7.0938, -7.1250, -8.5000, -8.6250, -4.7500, -1.6953,\n",
      "         -8.0000, -8.0000, -8.5000, -8.1875, -8.5000, -8.0000, -8.5625, -8.1250,\n",
      "         -7.8750, -8.5000, -7.1250, -6.5000, -8.0625, -7.9688, -8.4375, -8.5000,\n",
      "         -8.6250, -8.6250, -8.6250, -8.6250, -8.5625, -8.5000, -8.5625, -8.5625,\n",
      "         -8.5000, -8.6250, -8.5625, -8.6250, -8.5000, -8.6250, -4.0312, -6.9688,\n",
      "         -7.4375, -7.4062, -7.1562, -7.7812, -7.8438, -8.1875, -8.3125, -8.3750,\n",
      "         -8.3750, -8.5000, -8.5625, -6.9375, -5.9062, -8.5625, -8.6875, -8.5000,\n",
      "         -8.5625, -8.6250, -8.5000, -8.6250, -8.5625, -8.6875, -8.5000, -8.6250,\n",
      "         -8.6250, -8.7500, -8.5000, -8.5625]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 139\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  6]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 139])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 139, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4375, -0.1113, -1.7656,  ...,  1.0938, -0.9062, -1.3594],\n",
      "        [ 0.0000, -0.7188,  0.8828,  ...,  2.2500,  1.0469,  1.4609],\n",
      "        [ 1.5703,  0.6875,  0.4121,  ...,  0.3047, -0.2500,  0.0186]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -46.0000,  -464.0000,   302.0000,  ...,  1072.0000,\n",
      "            255.0000,  -162.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  8960.,  -3040., -23040.,  ...,   -664.,   8256., -26624.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 139, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4375, -0.1113, -1.7656,  ...,  1.0938, -0.9062, -1.3594],\n",
      "        [ 0.0000, -0.7188,  0.8828,  ...,  2.2500,  1.0469,  1.4609],\n",
      "        [ 1.5703,  0.6875,  0.4121,  ...,  0.3047, -0.2500,  0.0186]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 139]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 139, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.5000, -6.3125, -0.5469, -5.4375, -5.5938, -5.5938, -5.5625, -5.5625,\n",
      "         -5.4688, -5.5312, -5.6562, -5.6250, -5.5312, -4.1250,  0.4004, -5.6250,\n",
      "         -5.5938, -5.2188, -5.5312, -5.5625, -5.5000, -5.5938, -5.5312, -5.4688,\n",
      "         -5.5000, -5.5312, -5.5000, -5.5000, -5.5000, -5.5625, -5.5625, -5.4688,\n",
      "         -5.5000, -5.5312, -5.5312, -5.5000,  0.6523, -1.1797, -2.0312, -1.7969,\n",
      "         -2.4375, -3.5625, -3.9531, -1.8203, -1.6641, -3.2500, -2.5625, -2.0938,\n",
      "          0.5977,  0.0518,  0.1436, -1.3594, -5.2500, -3.3906, -3.9062, -3.8906,\n",
      "         -2.5312, -3.4844, -2.0781, -1.9609, -3.0781, -3.1250, -1.4453, -1.5781,\n",
      "         -3.7500, -3.8594, -2.5625, -2.0938, -3.0000, -4.2500, -3.2031, -3.5781,\n",
      "         -2.1719, -3.4219, -3.0000, -4.0625, -4.9062, -1.9375, -2.3281, -4.7500,\n",
      "          1.8906, -4.1875, -3.1719, -1.6641, -1.0156, -4.1250, -2.5625, -3.1094,\n",
      "         -2.5000, -4.5312, -4.0000, -2.8594, -3.0000, -2.9375, -3.7500, -3.6094,\n",
      "         -3.4531, -3.6562, -4.9062, -2.3281, -2.8281,  0.9023, -0.0742,  0.9453,\n",
      "          2.3594,  0.3008,  1.8672, -2.7656, -3.1094, -0.7812, -2.2031, -1.7344,\n",
      "         11.8750,  0.6562,  3.6875,  0.9414,  2.6094, -1.9922,  2.1250,  1.6016,\n",
      "         -0.6992, -0.4648, -0.5625, -2.5312, -1.2578, -0.9961, -1.4766, -4.1250,\n",
      "         -2.6719, -3.5312, -4.8438, -5.6875, -4.0312, -3.9844, -3.7812, -4.9375,\n",
      "         -4.4688, -4.9062, -4.0625, -5.0625, -3.3594, -4.3438, -4.4375, -4.4375,\n",
      "         -5.0938, -4.7812, -4.8125, -4.2188, -4.5625, -4.5000, -4.6250, -3.5938,\n",
      "         -3.9062, -4.8438, -4.9375, -4.6562, -3.9844, -3.2031, -4.6875, -4.8438,\n",
      "         -3.5312, -6.2188, -5.1875, -5.5000, -4.6562, -4.5938, -5.0000, -5.0625,\n",
      "         -4.7500, -4.4062, -4.5625, -4.8438, -4.8750, -4.7500, -4.6875, -4.9688,\n",
      "         -4.8438, -5.0312, -4.8125, -4.0938, -3.9531, -4.3125, -4.0312, -4.9375,\n",
      "         -4.9375, -4.9688, -4.6875, -4.0312, -4.8125, -4.9375, -4.8438, -4.5000,\n",
      "         -4.8125, -5.2500, -4.8438, -4.9062, -5.4688, -5.5000, -2.8594, -2.2188,\n",
      "         -5.1250, -5.1250, -5.5312, -5.2500, -5.4062, -5.5000, -5.5312, -5.4062,\n",
      "         -5.1250, -5.5625, -4.5625, -4.5312, -5.0625, -5.0625, -5.5000, -5.4375,\n",
      "         -5.6562, -5.4375, -5.5000, -5.5000, -5.4375, -5.5312, -5.5000, -5.5312,\n",
      "         -5.5000, -5.5312, -5.4688, -5.5938, -5.3438, -5.5625, -2.7344, -4.6562,\n",
      "         -4.8125, -4.8438, -4.9688, -4.9688, -5.0938, -5.4688, -5.3125, -5.4375,\n",
      "         -5.3438, -5.5312, -5.4688, -4.8750, -4.0312, -5.5625, -5.6250, -5.4375,\n",
      "         -5.4688, -5.5312, -5.5312, -5.4375, -5.5312, -5.6250, -5.5938, -5.5312,\n",
      "         -5.5625, -5.6562, -5.4375, -5.5000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 140\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  7]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 140])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 140, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0000, -0.7188,  0.8828,  ...,  2.2500,  1.0469,  1.4609],\n",
      "        [ 1.5703,  0.6875,  0.4121,  ...,  0.3047, -0.2500,  0.0186],\n",
      "        [ 0.0879,  1.6016, -0.7969,  ...,  2.1562,  0.5938, -0.2617]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -58.0000,  -488.0000,   326.0000,  ...,  1104.0000,\n",
      "            272.0000,  -178.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9344.,  -3072., -23936.,  ...,   -720.,   8576., -27520.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 140, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0000, -0.7188,  0.8828,  ...,  2.2500,  1.0469,  1.4609],\n",
      "        [ 1.5703,  0.6875,  0.4121,  ...,  0.3047, -0.2500,  0.0186],\n",
      "        [ 0.0879,  1.6016, -0.7969,  ...,  2.1562,  0.5938, -0.2617]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 140]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 17]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 140, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.9062, -5.3125,  4.8438, -6.7812, -6.9375, -6.8438, -6.7812, -6.9375,\n",
      "         -6.9375, -6.9688, -6.8125, -6.8750, -6.7500, -3.3750,  4.2500, -6.9375,\n",
      "         -6.9062, -6.4375, -6.9375, -6.8438, -6.7500, -6.9062, -6.9375, -6.7500,\n",
      "         -6.8750, -6.9062, -6.8125, -6.9062, -6.7812, -6.8438, -6.9375, -6.9062,\n",
      "         -6.8438, -6.9062, -6.8750, -6.8438,  4.7188,  2.9688,  0.3145, -1.8125,\n",
      "         -2.9531, -3.3281, -2.8281, -1.3438, -0.4062,  1.2109,  1.2891, -1.6016,\n",
      "          7.0312,  1.5391,  4.7500, -0.6484, -3.3594, -2.4375, -3.7656, -1.8047,\n",
      "         -2.4062, -2.0469, -2.1719, -3.1719, -2.7344, -2.9688,  0.3203,  1.5000,\n",
      "         -1.9141, -0.8203, -1.6797, -0.4160, -2.5625, -2.2344, -3.7188, -2.8281,\n",
      "         -1.3203, -1.3203, -2.6875, -2.6562, -3.8750, -2.0781, -2.3906, -3.1562,\n",
      "         -3.2188, -3.7656, -2.5469, -0.5781, -3.5938, -3.3125, -3.1875, -0.8828,\n",
      "         -2.7188, -1.9531, -2.7031, -2.4375, -2.7812, -2.6875, -2.8438, -2.7969,\n",
      "         -2.8750,  2.7500, -2.5000, -2.4219, -1.8750,  1.1016, -3.2656, -2.8281,\n",
      "          1.2109,  2.7500, -2.0781, -1.3672, -2.0156,  1.7109, -2.4219, -2.9688,\n",
      "          0.5898, -1.6719, -0.9883,  2.1719, -2.8906, -1.7656, -2.1094,  2.7656,\n",
      "         -1.1016,  0.4219, -1.5938, -2.1250, -2.3125, -1.2031, -2.1875, -3.4375,\n",
      "         -1.8438, -0.9219, -5.5625, -7.0312, -3.2812, -4.1562, -3.8906, -5.0625,\n",
      "         -5.0000, -4.9375, -5.0625, -6.0625, -2.6250, -3.7969, -4.9375, -4.6875,\n",
      "         -3.7812, -4.0938, -4.1562, -4.3438, -4.5000, -5.0000, -4.0000, -1.4141,\n",
      "         -2.2656, -5.4375, -5.9688, -3.5156, -3.7344, -1.1016, -3.2656, -5.2188,\n",
      "         -3.2344, -4.9375, -6.1562, -3.9531, -4.8750, -4.4062, -4.2188, -5.3750,\n",
      "         -4.8438, -4.4688, -4.0625, -4.5938, -4.6250, -3.0625, -5.0312, -5.6250,\n",
      "         -5.5000, -5.3750, -5.6250, -4.2500, -4.3125, -3.5938, -3.5469, -4.3125,\n",
      "         -5.5312, -5.5312, -5.2188, -4.1250, -4.5312, -5.5000, -5.0312, -4.9375,\n",
      "         -4.3125, -4.8438, -5.1250, -5.1562, -6.8750, -6.9375, -2.9844, -0.8750,\n",
      "         -6.2188, -6.1875, -6.9062, -6.4688, -6.6875, -6.9062, -6.9375, -6.5312,\n",
      "         -6.2500, -6.9375, -4.7188, -3.8750, -6.0625, -6.0625, -6.8438, -6.7812,\n",
      "         -7.0000, -6.8750, -6.8750, -6.8438, -7.0000, -6.8438, -6.8750, -6.8750,\n",
      "         -6.8750, -6.7812, -6.8750, -6.8750, -6.5938, -7.0312, -1.2812, -5.0625,\n",
      "         -5.4688, -5.3438, -5.4062, -6.1562, -6.0312, -6.3438, -6.6562, -6.9062,\n",
      "         -6.6875, -7.0000, -6.8125, -5.0625, -2.8906, -6.9062, -6.9375, -6.7812,\n",
      "         -6.9062, -6.9688, -6.9062, -6.8438, -6.8125, -6.9688, -6.9375, -6.9375,\n",
      "         -6.9688, -6.9375, -6.8750, -6.9688]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 141\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 144, 141])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 141, 256]), Cross output shape: torch.Size([1, 144, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.5703,  0.6875,  0.4121,  ...,  0.3047, -0.2500,  0.0186],\n",
      "        [ 0.0879,  1.6016, -0.7969,  ...,  2.1562,  0.5938, -0.2617],\n",
      "        [-1.6875,  0.5078, -0.3457,  ..., -0.1089, -0.7344, -1.3438]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 18, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -59.0000,  -504.0000,   338.0000,  ...,  1136.0000,\n",
      "            284.0000,  -208.0000],\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 18, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9664.,  -3168., -24832.,  ...,   -788.,   8960., -28672.],\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 141, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.5703,  0.6875,  0.4121,  ...,  0.3047, -0.2500,  0.0186],\n",
      "        [ 0.0879,  1.6016, -0.7969,  ...,  2.1562,  0.5938, -0.2617],\n",
      "        [-1.6875,  0.5078, -0.3457,  ..., -0.1089, -0.7344, -1.3438]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 141]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 141, 144])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-4.6562, -6.0625,  2.4062, -4.6875, -4.6875, -4.7812, -4.5625, -4.6875,\n",
      "         -4.8125, -4.7188, -4.6562, -4.7500, -4.6875, -2.4531,  4.3125, -4.8125,\n",
      "         -4.5938, -4.5938, -4.7500, -4.5938, -4.6875, -4.6562, -4.6562, -4.6875,\n",
      "         -4.7188, -4.6562, -4.7188, -4.7188, -4.7812, -4.7188, -4.6562, -4.6562,\n",
      "         -4.6875, -4.7812, -4.7500, -4.6875,  9.6250, -1.5312,  1.2109, -1.2812,\n",
      "         -2.7344, -4.2500, -0.7539, -2.0625, -0.9883,  0.3457, -0.2109, -1.5000,\n",
      "          1.1094,  0.4219,  1.0234,  1.4219, -1.9141, -1.2031, -1.4766, -2.1875,\n",
      "         -2.2656, -2.0781, -2.6094, -3.2188, -3.5781, -3.0781,  0.2910, -2.0938,\n",
      "          0.0649,  0.6016,  0.0239, -1.7578, -3.6875, -1.9297, -1.9297, -0.9453,\n",
      "         -2.0625, -2.9219, -1.6484, -2.8750, -2.2031, -1.1641, -1.9297, -2.6562,\n",
      "         -2.7969, -0.7188, -1.5625, -1.5938, -2.0000, -2.8594, -1.4531, -1.1172,\n",
      "         -2.0312, -2.4531, -0.0444, -1.4219, -2.9531, -2.6406, -2.3125, -1.8281,\n",
      "         -3.7656,  0.0957, -2.6406, -1.6328, -5.0938, -1.3516, -1.4922, -0.4082,\n",
      "         -0.9961, -1.9844, -1.6875, -1.3906, -1.6719, -0.1973, -1.5703, -0.8320,\n",
      "         -0.9336, -0.5547, -0.4297, -1.7734, -1.3906, -3.2031, -1.0938, -0.3574,\n",
      "         -0.6680, -2.2031, -0.9102, -0.9727, -2.1250, -1.6484, -2.5312, -2.8906,\n",
      "         -0.3750, -0.9219, -4.4062, -4.7500, -3.7188, -4.2812, -4.0312, -4.2188,\n",
      "         -4.2500, -4.3438, -3.8594, -4.4062, -3.3906, -4.0312, -4.0000, -4.2188,\n",
      "         -3.7656, -3.8750, -4.2188, -3.5312, -3.6875, -4.2188, -2.5938, -3.1406,\n",
      "         -3.2500, -4.2500, -4.1250, -2.3125, -3.9375, -4.6562, -3.2188, -4.0312,\n",
      "         -3.0156, -2.5469, -4.3750, -4.0312, -3.9219, -3.8125, -4.1562, -4.0000,\n",
      "         -3.8750, -3.8750, -3.9219, -3.4531, -4.0000, -3.0469, -3.8906, -4.3125,\n",
      "         -4.3750, -4.0938, -4.0938, -4.0000, -4.2812, -3.5781, -3.4688, -3.6406,\n",
      "         -4.1250, -4.1562, -4.3125, -3.6875, -3.8594, -4.0312, -4.0312, -3.6562,\n",
      "         -3.2656, -3.1094, -3.8750, -4.1250, -4.6250, -4.7500, -2.2031, -1.4688,\n",
      "         -4.2812, -4.4688, -4.6562, -4.5625, -4.7188, -4.7188, -4.7188, -4.4688,\n",
      "         -4.3750, -4.7500, -4.2188, -3.9844, -4.2500, -4.2500, -4.6562, -4.7188,\n",
      "         -4.9062, -4.7500, -4.6250, -4.7188, -4.6875, -4.6875, -4.6562, -4.6875,\n",
      "         -4.6562, -4.6875, -4.7188, -4.7188, -4.6562, -4.7188, -1.7188, -3.9375,\n",
      "         -4.2500, -4.2812, -4.2812, -4.5312, -4.4688, -4.6562, -4.6250, -4.5625,\n",
      "         -4.4375, -4.7812, -4.6562, -4.0312, -3.7656, -4.7500, -4.7812, -4.6250,\n",
      "         -4.7500, -4.6250, -4.6875, -4.7188, -4.7500, -4.7812, -4.6875, -4.6562,\n",
      "         -4.8438, -4.7500, -4.7812, -4.6875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 142\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 142])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 142, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0879,  1.6016, -0.7969,  ...,  2.1562,  0.5938, -0.2617],\n",
      "        [-1.6875,  0.5078, -0.3457,  ..., -0.1089, -0.7344, -1.3438],\n",
      "        [-0.9492,  0.9062,  1.9688,  ...,  0.7148, -0.3926, -0.7266]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  108.0000,   -93.0000,   -68.0000,  ...,   310.0000,\n",
      "             29.1250,   -21.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  2240.,  -1432.,  -5312.,  ...,    105.,   2272.,  -7392.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 142, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0879,  1.6016, -0.7969,  ...,  2.1562,  0.5938, -0.2617],\n",
      "        [-1.6875,  0.5078, -0.3457,  ..., -0.1089, -0.7344, -1.3438],\n",
      "        [-0.9492,  0.9062,  1.9688,  ...,  0.7148, -0.3926, -0.7266]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 142]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 18]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 142, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-1.0938e+01, -9.8750e+00, -5.3750e+00, -1.0938e+01, -1.0938e+01,\n",
      "         -1.0938e+01, -1.1000e+01, -1.0875e+01, -1.0875e+01, -1.0938e+01,\n",
      "         -1.0938e+01, -1.0938e+01, -1.0938e+01, -7.4375e+00, -6.6406e-01,\n",
      "         -1.1062e+01, -1.1000e+01, -1.0438e+01, -1.1000e+01, -1.1062e+01,\n",
      "         -1.0938e+01, -1.0938e+01, -1.0938e+01, -1.1062e+01, -1.0938e+01,\n",
      "         -1.0938e+01, -1.1062e+01, -1.0938e+01, -1.1000e+01, -1.0938e+01,\n",
      "         -1.1000e+01, -1.0938e+01, -1.1062e+01, -1.0938e+01, -1.1000e+01,\n",
      "         -1.0938e+01, -4.0000e+00, -6.6875e+00, -1.5938e+00, -4.3125e+00,\n",
      "         -6.4375e+00, -7.0312e+00, -5.6875e+00, -5.3750e+00, -3.0312e+00,\n",
      "         -6.8750e+00, -5.1875e+00, -5.3750e+00, -3.5938e+00, -2.5781e+00,\n",
      "         -3.5156e+00, -3.1719e+00, -5.5000e+00, -3.8438e+00, -4.8438e+00,\n",
      "         -4.9375e+00, -5.5625e+00, -5.6562e+00, -6.3125e+00, -6.3438e+00,\n",
      "         -5.5625e+00, -5.8438e+00, -6.7188e+00, -4.1250e+00, -6.0000e+00,\n",
      "         -6.1562e+00, -6.3125e+00, -8.9375e+00, -7.3438e+00, -3.8750e+00,\n",
      "         -4.3750e+00, -4.6250e+00, -5.9688e+00, -4.3125e+00, -4.3750e+00,\n",
      "         -4.1250e+00, -4.4688e+00, -2.1875e+00, -2.5156e+00, -4.5625e+00,\n",
      "         -4.7188e+00, -3.8281e+00, -5.3438e+00, -5.5625e+00, -2.7500e+00,\n",
      "         -6.4062e+00, -4.0000e+00, -3.5781e+00, -4.4062e+00, -4.3125e+00,\n",
      "         -6.2500e+00, -5.4062e+00, -6.2812e+00, -6.3750e+00, -6.0000e+00,\n",
      "         -4.5625e+00, -8.1250e+00, -5.4688e+00, -7.0000e+00, -4.4062e+00,\n",
      "         -5.6875e+00,  2.8438e+00,  2.3906e+00,  1.5703e+00,  9.6875e-01,\n",
      "          7.8125e-01,  1.5547e+00,  1.3828e+00,  3.8867e-01,  8.7500e-01,\n",
      "          1.2734e+00,  1.7383e-01,  2.7344e-01,  1.2891e+00, -7.8125e-01,\n",
      "          7.6953e-01,  3.2812e+00, -9.9609e-01,  1.1797e+00,  1.6250e+00,\n",
      "          1.8984e+00, -5.9814e-03, -5.5859e-01,  6.6797e-01, -5.3125e+00,\n",
      "          1.2891e-01, -3.3125e+00, -5.6875e+00, -6.1562e+00, -5.0938e+00,\n",
      "         -9.6875e+00, -1.1000e+01, -7.3125e+00, -9.0000e+00, -8.8750e+00,\n",
      "         -9.6875e+00, -9.6250e+00, -9.3750e+00, -9.0000e+00, -1.0438e+01,\n",
      "         -7.9062e+00, -8.3125e+00, -9.4375e+00, -9.0000e+00, -8.9375e+00,\n",
      "         -9.5625e+00, -9.5625e+00, -9.2500e+00, -9.5000e+00, -9.8125e+00,\n",
      "         -7.7812e+00, -7.1875e+00, -8.1875e+00, -1.0125e+01, -1.0062e+01,\n",
      "         -8.8750e+00, -8.5000e+00, -9.8125e+00, -7.1562e+00, -1.0000e+01,\n",
      "         -7.9062e+00, -7.6875e+00, -1.0250e+01, -8.8125e+00, -8.9375e+00,\n",
      "         -9.2500e+00, -9.0000e+00, -9.8125e+00, -9.4375e+00, -9.1250e+00,\n",
      "         -8.5000e+00, -9.4375e+00, -9.6250e+00, -7.3750e+00, -9.5000e+00,\n",
      "         -9.9375e+00, -1.0188e+01, -1.0125e+01, -9.8750e+00, -9.6250e+00,\n",
      "         -7.3125e+00, -8.1875e+00, -8.9375e+00, -8.7500e+00, -9.7500e+00,\n",
      "         -9.8125e+00, -1.0062e+01, -9.1875e+00, -9.0000e+00, -9.9375e+00,\n",
      "         -9.7500e+00, -9.5625e+00, -8.4375e+00, -8.5000e+00, -9.6875e+00,\n",
      "         -9.7500e+00, -1.1000e+01, -1.0875e+01, -6.9062e+00, -4.5938e+00,\n",
      "         -1.0188e+01, -1.0312e+01, -1.0875e+01, -1.0562e+01, -1.0812e+01,\n",
      "         -1.0438e+01, -1.0938e+01, -1.0812e+01, -1.0500e+01, -1.0938e+01,\n",
      "         -9.4375e+00, -9.1875e+00, -1.0188e+01, -9.9375e+00, -1.0875e+01,\n",
      "         -1.1000e+01, -1.1000e+01, -1.0938e+01, -1.1000e+01, -1.1000e+01,\n",
      "         -1.0938e+01, -1.0875e+01, -1.0938e+01, -1.0938e+01, -1.0875e+01,\n",
      "         -1.1062e+01, -1.1000e+01, -1.1000e+01, -1.0750e+01, -1.0938e+01,\n",
      "         -4.4062e+00, -9.5625e+00, -1.0125e+01, -9.7500e+00, -9.8750e+00,\n",
      "         -1.0062e+01, -1.0312e+01, -1.0625e+01, -1.0688e+01, -1.0875e+01,\n",
      "         -1.0812e+01, -1.0938e+01, -1.0938e+01, -9.4375e+00, -7.2188e+00,\n",
      "         -1.1000e+01, -1.1000e+01, -1.0938e+01, -1.0938e+01, -1.1000e+01,\n",
      "         -1.0938e+01, -1.1125e+01, -1.0938e+01, -1.1000e+01, -1.0938e+01,\n",
      "         -1.1000e+01, -1.1000e+01, -1.1000e+01, -1.1000e+01, -1.0938e+01]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 143\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 143])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 143, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6875,  0.5078, -0.3457,  ..., -0.1089, -0.7344, -1.3438],\n",
      "        [-0.9492,  0.9062,  1.9688,  ...,  0.7148, -0.3926, -0.7266],\n",
      "        [-0.3926, -0.6445,  3.2344,  ...,  1.1953,  1.4609,  0.2227]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -10.0000,  -243.0000,    89.5000,  ...,   816.0000,\n",
      "            129.0000,   -47.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  5440.,  -2016., -13760.,  ...,   -248.,   5152., -16384.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 143, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.6875,  0.5078, -0.3457,  ..., -0.1089, -0.7344, -1.3438],\n",
      "        [-0.9492,  0.9062,  1.9688,  ...,  0.7148, -0.3926, -0.7266],\n",
      "        [-0.3926, -0.6445,  3.2344,  ...,  1.1953,  1.4609,  0.2227]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 143]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 18]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 143, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.7812, -8.5625, -3.8125, -6.8125, -6.7812, -6.7188, -6.7812, -6.8125,\n",
      "         -6.7500, -6.6875, -6.7500, -6.7188, -6.7500, -5.0938, -3.0625, -6.7500,\n",
      "         -6.7188, -6.5000, -6.7500, -6.7188, -6.7500, -6.7500, -6.5312, -6.7812,\n",
      "         -6.7188, -6.6562, -6.7500, -6.7500, -6.7812, -6.6875, -6.7500, -6.6250,\n",
      "         -6.6875, -6.6875, -6.6250, -6.7188, -1.9219, -5.0000, -3.8125, -4.3750,\n",
      "         -4.9375, -4.8750, -4.5000, -3.4062, -3.2812, -3.7500, -3.9688, -5.2812,\n",
      "         -1.4844, -2.7812, -1.4609, -3.8281, -4.4688, -4.6562, -4.2812, -4.0312,\n",
      "         -4.4375, -5.7500, -4.1875, -5.5938, -4.7812, -5.8438, -3.7969, -2.9688,\n",
      "         -5.5312, -4.8438, -3.3438, -5.5938, -5.0938, -4.5000, -6.0312, -4.9688,\n",
      "         -5.1562, -1.6328, -5.3125, -4.5625, -3.7812, -4.0938, -5.2812, -5.2188,\n",
      "         -4.5312, -4.9688, -4.7188, -4.5938, -4.4688, -6.5000, -4.8750, -4.8750,\n",
      "         -4.3125, -3.8438, -5.4062, -4.9062, -5.1875, -5.0625, -4.8125, -4.3438,\n",
      "         -5.3438, -5.1250, -5.5938, -4.1875, -6.4375,  3.7656, -1.8125, -1.1406,\n",
      "         -1.2656,  9.1875, -2.4844, -1.4688,  2.5000,  4.4688, -3.7031, -1.9766,\n",
      "          2.4219, -1.2422, -0.6406,  4.1250, -0.7109, -3.2969,  3.0469,  0.0194,\n",
      "         -0.7578,  4.2812, -2.2031, -1.6250, -3.9219,  1.2656, -1.7266, -4.2188,\n",
      "         -4.9062, -3.8906, -6.1875, -6.7500, -4.2812, -5.6250, -5.6562, -6.1562,\n",
      "         -6.2188, -6.1250, -5.9688, -6.3438, -5.6562, -5.6250, -5.8125, -5.6562,\n",
      "         -5.6562, -5.7812, -5.8750, -6.0000, -5.8438, -5.9062, -5.4062, -6.3438,\n",
      "         -5.5312, -6.1875, -6.3125, -5.9688, -5.8438, -5.2812, -4.6875, -6.3125,\n",
      "         -5.4375, -4.6562, -6.3125, -5.6250, -5.6562, -6.0938, -5.9062, -6.2812,\n",
      "         -6.1562, -5.6250, -5.1875, -6.3125, -5.8750, -5.6875, -6.0938, -6.2500,\n",
      "         -6.2812, -6.2188, -6.0938, -6.1875, -5.0312, -5.2188, -4.9688, -5.4688,\n",
      "         -6.1562, -6.1250, -6.1875, -5.8438, -5.8750, -6.2188, -6.1250, -6.1562,\n",
      "         -5.5312, -5.6562, -5.9688, -5.9062, -6.7188, -6.5938, -4.9688, -0.4062,\n",
      "         -6.4375, -6.5000, -6.6875, -6.5938, -6.6250, -6.5938, -6.7812, -6.7500,\n",
      "         -6.4375, -6.7812, -6.2500, -5.4688, -6.3750, -6.2188, -6.7188, -6.7500,\n",
      "         -6.8438, -6.7188, -6.7812, -6.8438, -6.7188, -6.7812, -6.7500, -6.6875,\n",
      "         -6.7188, -6.8125, -6.7500, -6.7188, -6.6562, -6.7812, -4.3125, -6.1562,\n",
      "         -6.2812, -5.9375, -6.2500, -6.2812, -6.3125, -6.5625, -6.5312, -6.7500,\n",
      "         -6.5938, -6.7812, -6.7500, -5.8438, -4.4375, -6.7500, -6.7812, -6.6875,\n",
      "         -6.7812, -6.8125, -6.6875, -6.7188, -6.6875, -6.7500, -6.7812, -6.8125,\n",
      "         -6.6875, -6.6875, -6.8125, -6.7188]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 144\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 144])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 144, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.9492,  0.9062,  1.9688,  ...,  0.7148, -0.3926, -0.7266],\n",
      "        [-0.3926, -0.6445,  3.2344,  ...,  1.1953,  1.4609,  0.2227],\n",
      "        [-0.2891,  0.5391,  0.1143,  ...,  0.8711, -0.9688, -1.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -50.0000,  -320.0000,   164.0000,  ...,   904.0000,\n",
      "            161.0000,  -105.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  6720.,  -2352., -17024.,  ...,   -378.,   6208., -19968.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 144, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.9492,  0.9062,  1.9688,  ...,  0.7148, -0.3926, -0.7266],\n",
      "        [-0.3926, -0.6445,  3.2344,  ...,  1.1953,  1.4609,  0.2227],\n",
      "        [-0.2891,  0.5391,  0.1143,  ...,  0.8711, -0.9688, -1.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 144]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 18]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 144, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[ -9.6875, -10.8125,  -5.0938,  -9.6875,  -9.7500,  -9.5625,  -9.6875,\n",
      "          -9.6875,  -9.7500,  -9.7500,  -9.8125,  -9.7500,  -9.6875,  -6.2812,\n",
      "          -3.1406,  -9.7500,  -9.8750,  -9.4375,  -9.7500,  -9.6875,  -9.6875,\n",
      "          -9.7500,  -9.6250,  -9.6250,  -9.6250,  -9.6875,  -9.6250,  -9.6875,\n",
      "          -9.7500,  -9.6875,  -9.7500,  -9.6875,  -9.6250,  -9.6875,  -9.6250,\n",
      "          -9.8750,  -1.0703,  -4.9688,  -2.5312,  -6.2188,  -6.6250,  -5.1562,\n",
      "          -5.4062,  -3.1250,  -4.6562,  -4.4375,  -5.5000,  -5.5312,  -0.8047,\n",
      "          -1.6562,  -0.8945,  -3.2656,  -4.1250,  -4.6562,  -4.7812,  -4.7188,\n",
      "          -4.2812,  -5.7500,  -5.3438,  -4.3125,  -6.2188,  -5.5938,  -3.4375,\n",
      "          -3.5781,  -5.8125,  -4.8125,  -4.2188,  -4.8438,  -6.8438,  -4.7812,\n",
      "          -5.7500,  -4.6250,  -6.2188,  -5.1250,  -5.9062,  -4.5938,  -5.4688,\n",
      "          -5.2812,  -5.0938,  -5.9375,  -5.8750,  -4.6250,  -2.1562,  -5.4062,\n",
      "          -3.9688,  -5.0625,  -5.6562,  -3.7500,  -4.1250,  -5.8750,  -6.0312,\n",
      "          -4.8125,  -4.4375,  -6.3750,  -5.8750,  -6.0625,  -7.0938,  -6.1875,\n",
      "          -5.2812,  -5.3438,  -6.0000,   4.3750,  -0.9492,   1.4609,   0.4531,\n",
      "           3.2812,  -2.1250,   0.0742,  -1.5156,  -0.9844,  -3.0469,  -0.8359,\n",
      "           1.8750,  -0.1660,   9.3125,   1.2422,   3.2344,  -0.9023,   3.5469,\n",
      "           1.0156,   2.9844,  -0.6523,  -0.5820,  -0.5938,  -0.4297,  -2.2188,\n",
      "          -1.7422,  -4.3125,  -4.7188,  -4.4375,  -9.0000,  -9.7500,  -5.8125,\n",
      "          -7.2500,  -6.8750,  -8.3750,  -8.5000,  -8.6875,  -7.8438,  -8.6875,\n",
      "          -6.2812,  -6.7500,  -7.8750,  -7.5000,  -7.1562,  -7.5000,  -7.6562,\n",
      "          -7.4688,  -7.8125,  -8.3125,  -6.5000,  -6.1562,  -7.0000,  -8.0000,\n",
      "          -8.5625,  -7.2500,  -7.1250,  -6.4062,  -6.5938,  -8.3125,  -6.6250,\n",
      "          -7.8125,  -8.8750,  -6.9375,  -7.2188,  -7.8438,  -7.8125,  -8.3125,\n",
      "          -7.8438,  -7.6250,  -6.9688,  -8.2500,  -7.6875,  -6.6250,  -8.0000,\n",
      "          -8.6875,  -8.7500,  -8.4375,  -8.1875,  -7.8125,  -7.8125,  -6.9688,\n",
      "          -6.0938,  -7.3438,  -8.1875,  -8.3750,  -8.6875,  -7.5938,  -7.0312,\n",
      "          -8.6875,  -8.2500,  -8.0000,  -7.4062,  -7.5312,  -8.1250,  -8.1250,\n",
      "          -9.6875,  -9.6875,  -6.5000,  -2.9531,  -8.9375,  -8.9375,  -9.6250,\n",
      "          -9.4375,  -9.6875,  -9.6250,  -9.6875,  -9.5625,  -9.0625,  -9.6875,\n",
      "          -8.5000,  -7.4062,  -9.1250,  -9.1875,  -9.7500,  -9.7500,  -9.9375,\n",
      "          -9.6875,  -9.7500,  -9.6875,  -9.6250,  -9.7500,  -9.6875,  -9.6875,\n",
      "          -9.6875,  -9.6875,  -9.7500,  -9.8750,  -9.3750,  -9.8125,  -4.4062,\n",
      "          -8.0625,  -8.5625,  -8.1875,  -8.6250,  -8.6250,  -8.8125,  -9.1250,\n",
      "          -9.5625,  -9.6875,  -9.6875,  -9.6875,  -9.6875,  -7.7500,  -5.9062,\n",
      "          -9.7500,  -9.6875,  -9.6250,  -9.6875,  -9.7500,  -9.8125,  -9.6250,\n",
      "          -9.6875,  -9.7500,  -9.7500,  -9.7500,  -9.6875,  -9.7500,  -9.6875,\n",
      "          -9.6875]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 145\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 145])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 145, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3926, -0.6445,  3.2344,  ...,  1.1953,  1.4609,  0.2227],\n",
      "        [-0.2891,  0.5391,  0.1143,  ...,  0.8711, -0.9688, -1.1406],\n",
      "        [ 2.4062, -0.0664, -1.6797,  ...,  1.1406, -0.8594, -1.4453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -47.5000,  -372.0000,   234.0000,  ...,   968.0000,\n",
      "            199.0000,  -128.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  7584.,  -2608., -19328.,  ...,   -474.,   6976., -22400.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 145, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3926, -0.6445,  3.2344,  ...,  1.1953,  1.4609,  0.2227],\n",
      "        [-0.2891,  0.5391,  0.1143,  ...,  0.8711, -0.9688, -1.1406],\n",
      "        [ 2.4062, -0.0664, -1.6797,  ...,  1.1406, -0.8594, -1.4453]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 145]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         18]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 145, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.5000, -7.0625,  1.9141, -8.5000, -8.4375, -8.4375, -8.4375, -8.5625,\n",
      "         -8.5000, -8.5000, -8.5625, -8.6250, -8.5625, -4.9688,  2.9688, -8.5000,\n",
      "         -8.5625, -8.2500, -8.5625, -8.5625, -8.4375, -8.5000, -8.5000, -8.4375,\n",
      "         -8.4375, -8.5000, -8.4375, -8.4375, -8.4375, -8.5625, -8.5625, -8.5625,\n",
      "         -8.5000, -8.4375, -8.5000, -8.5000,  4.4062,  0.3633, -1.0391, -3.1406,\n",
      "         -3.8438, -3.9844, -3.8594, -1.1875, -1.6406, -1.4766,  0.0366, -3.7188,\n",
      "          4.2500,  0.2354,  3.4062, -1.2344, -1.7109, -2.4375, -3.4375, -2.9531,\n",
      "         -1.5000, -2.1875, -3.0156, -2.2969, -4.3125, -1.2656, -0.2578, -0.5195,\n",
      "         -1.0781, -1.8672, -4.2812, -1.2188, -4.2188, -2.6094, -3.1250,  0.5195,\n",
      "         -3.8281, -4.4062, -3.5781, -2.1406, -3.5625, -0.7812, -2.3438, -3.5625,\n",
      "         -5.3750, -4.2500, -3.7344, -2.4531, -4.7188, -2.1406, -3.1094, -3.1875,\n",
      "         -2.6250, -2.5469, -2.9688, -3.7031, -3.4375, -3.9219, -3.8750, -3.5625,\n",
      "         -4.2188, -2.3594, -0.9062, -2.9688, -2.5469, -0.4609, -2.4688,  8.0625,\n",
      "          0.6953,  0.5039, -0.9727,  2.2188, -0.5117,  1.1094,  1.1094,  0.2578,\n",
      "         -2.9219, -2.2500,  2.1094, -1.1953, -2.8906,  0.0864, -0.9688,  1.1797,\n",
      "          1.4141, -0.9961, -1.5469, -1.8516, -2.0469, -0.7734,  1.5938, -1.8047,\n",
      "         -2.4531, -2.2344, -7.2500, -8.5625, -4.5938, -4.9688, -4.9688, -6.5312,\n",
      "         -6.1562, -6.5625, -5.8438, -7.5938, -4.5625, -3.9844, -5.5938, -5.7812,\n",
      "         -4.5938, -5.3438, -5.5312, -5.0938, -5.8750, -6.4375, -4.5312, -2.0469,\n",
      "         -3.9531, -6.5000, -7.3438, -4.8750, -3.8281, -3.2969, -4.2812, -6.5312,\n",
      "         -3.2812, -4.0938, -7.4375, -5.5312, -5.7812, -5.5625, -5.9688, -6.3438,\n",
      "         -5.8750, -5.4688, -4.9688, -5.7188, -5.9375, -4.9688, -6.1562, -7.1250,\n",
      "         -7.0625, -6.7812, -7.0312, -5.8125, -5.3438, -4.4375, -3.9531, -5.1250,\n",
      "         -7.0312, -6.8750, -7.1250, -5.5312, -5.0312, -6.8750, -6.7500, -6.4062,\n",
      "         -4.4688, -4.7500, -6.4375, -6.6875, -8.4375, -8.5000, -2.1250, -1.8438,\n",
      "         -7.6562, -7.9688, -8.4375, -7.9688, -8.3125, -8.3125, -8.5000, -8.1250,\n",
      "         -7.4062, -8.6250, -6.5000, -4.9688, -7.5312, -7.5938, -8.5000, -8.5000,\n",
      "         -8.6250, -8.5625, -8.6250, -8.5000, -8.5625, -8.5625, -8.4375, -8.4375,\n",
      "         -8.5625, -8.6250, -8.3125, -8.4375, -8.1875, -8.5000, -2.0781, -5.8750,\n",
      "         -7.0312, -6.4062, -6.8750, -7.0000, -7.6250, -7.6875, -8.1875, -8.3750,\n",
      "         -8.4375, -8.5000, -8.4375, -5.3750, -3.1250, -8.5000, -8.5000, -8.5000,\n",
      "         -8.5625, -8.5000, -8.4375, -8.5000, -8.4375, -8.5625, -8.4375, -8.5625,\n",
      "         -8.5625, -8.5625, -8.4375, -8.5625]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 146\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 146])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 146, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2891,  0.5391,  0.1143,  ...,  0.8711, -0.9688, -1.1406],\n",
      "        [ 2.4062, -0.0664, -1.6797,  ...,  1.1406, -0.8594, -1.4453],\n",
      "        [-0.0801, -0.7969,  0.8633,  ...,  2.3438,  1.0703,  1.4688]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -40.0000,  -440.0000,   278.0000,  ...,  1056.0000,\n",
      "            241.0000,  -142.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  8512.,  -2912., -21888.,  ...,   -576.,   7872., -25344.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 146, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.2891,  0.5391,  0.1143,  ...,  0.8711, -0.9688, -1.1406],\n",
      "        [ 2.4062, -0.0664, -1.6797,  ...,  1.1406, -0.8594, -1.4453],\n",
      "        [-0.0801, -0.7969,  0.8633,  ...,  2.3438,  1.0703,  1.4688]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 146]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 18]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 146, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.6875, -6.2500, -2.8281, -8.6250, -8.6875, -8.7500, -8.6250, -8.6250,\n",
      "         -8.6875, -8.6250, -8.7500, -8.8125, -8.8125, -5.0938, -2.9844, -8.7500,\n",
      "         -8.6250, -8.4375, -8.6875, -8.6250, -8.6250, -8.6250, -8.6875, -8.6250,\n",
      "         -8.6875, -8.6875, -8.6875, -8.6250, -8.6875, -8.7500, -8.7500, -8.6875,\n",
      "         -8.6250, -8.6250, -8.6875, -8.6250, -0.2852, -3.0000, -3.3281, -3.1406,\n",
      "         -3.5469, -4.5625, -5.6250, -2.9844, -3.6875, -4.3438, -4.3438, -3.6875,\n",
      "         -1.0391, -2.0469, -0.2061, -2.0469, -4.4688, -2.9844, -3.2031, -3.5469,\n",
      "         -2.7969, -4.3125, -4.0312, -3.4219, -3.4531, -4.1875, -1.9609, -4.3750,\n",
      "         -3.7969, -3.5000, -4.3125, -3.3906, -5.4375, -3.7500, -4.3438, -4.7500,\n",
      "         -4.0312, -3.6094, -4.0312, -4.6250, -3.6875, -0.3613, -3.3750, -5.0938,\n",
      "         -3.5156, -3.8594, -4.3750, -2.3594, -3.7656, -7.1875, -3.6719, -4.3438,\n",
      "         -3.6094, -4.4375, -4.8125, -5.4375, -5.3125, -3.8906, -5.1562, -4.7188,\n",
      "         -6.0000, -6.1562, -5.8125, -5.0938, -4.9375,  2.4375, -1.8672, -1.6953,\n",
      "         -2.1094,  4.2500, -1.0234, -2.4844,  1.2266, 11.0625, -3.2969, -1.3750,\n",
      "          2.6094, -1.8125, -1.9453,  2.9844, -1.8047, -4.6875,  1.3516, -0.0391,\n",
      "          0.2158,  1.0859, -3.0469, -2.6406, -5.5312,  3.9844, -1.7891, -4.5312,\n",
      "         -3.4688, -4.5625, -7.5938, -8.6875, -4.2500, -6.2812, -6.0312, -7.3750,\n",
      "         -6.9375, -7.2188, -7.0312, -8.0625, -6.2500, -5.6250, -6.8125, -6.4375,\n",
      "         -6.1562, -6.4688, -7.1562, -6.2500, -6.8750, -6.7500, -6.3125, -5.8125,\n",
      "         -5.6875, -7.6250, -7.7188, -6.7188, -6.1250, -5.3125, -6.6562, -7.3750,\n",
      "         -5.0312, -5.8438, -7.7812, -6.1562, -6.7812, -6.3125, -6.8750, -7.1875,\n",
      "         -6.9062, -6.2188, -6.8438, -6.7500, -6.9062, -5.5625, -7.0000, -7.6562,\n",
      "         -7.5312, -7.4062, -7.5625, -6.5938, -6.2188, -5.1562, -6.1250, -6.3438,\n",
      "         -7.7188, -7.4688, -7.5312, -6.6250, -6.1250, -7.5938, -7.3750, -7.0312,\n",
      "         -5.8125, -6.5312, -7.1875, -7.2188, -8.6250, -8.7500, -4.9375, -1.7422,\n",
      "         -8.1250, -8.1250, -8.6250, -8.3125, -8.6250, -8.1250, -8.6875, -8.2500,\n",
      "         -8.0000, -8.6250, -7.2812, -6.6875, -8.1875, -8.0625, -8.5625, -8.6250,\n",
      "         -8.7500, -8.7500, -8.7500, -8.7500, -8.6875, -8.6250, -8.6875, -8.6875,\n",
      "         -8.6250, -8.7500, -8.6875, -8.7500, -8.6250, -8.7500, -4.0938, -7.0625,\n",
      "         -7.5625, -7.5312, -7.2812, -7.9062, -7.9688, -8.3125, -8.4375, -8.5000,\n",
      "         -8.5000, -8.6250, -8.6875, -7.0625, -6.0312, -8.6875, -8.8125, -8.6250,\n",
      "         -8.6875, -8.7500, -8.6250, -8.7500, -8.6875, -8.8125, -8.6250, -8.7500,\n",
      "         -8.8125, -8.8750, -8.6250, -8.6875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 147\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          6]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 147])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 147, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4062, -0.0664, -1.6797,  ...,  1.1406, -0.8594, -1.4453],\n",
      "        [-0.0801, -0.7969,  0.8633,  ...,  2.3438,  1.0703,  1.4688],\n",
      "        [ 1.5156,  0.6797,  0.3887,  ...,  0.3008, -0.2412, -0.0059]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -46.5000,  -464.0000,   302.0000,  ...,  1088.0000,\n",
      "            260.0000,  -160.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9024.,  -3008., -23168.,  ...,   -664.,   8320., -26752.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 147, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4062, -0.0664, -1.6797,  ...,  1.1406, -0.8594, -1.4453],\n",
      "        [-0.0801, -0.7969,  0.8633,  ...,  2.3438,  1.0703,  1.4688],\n",
      "        [ 1.5156,  0.6797,  0.3887,  ...,  0.3008, -0.2412, -0.0059]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 147]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 18]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 147, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.4375, -6.5312, -0.7305, -5.3750, -5.5312, -5.5312, -5.5000, -5.5000,\n",
      "         -5.3750, -5.4688, -5.5938, -5.5938, -5.4688, -4.1875,  0.3691, -5.5625,\n",
      "         -5.5312, -5.1562, -5.4688, -5.5000, -5.4375, -5.5312, -5.4688, -5.3750,\n",
      "         -5.4375, -5.4688, -5.4375, -5.4375, -5.4375, -5.5000, -5.5000, -5.3750,\n",
      "         -5.4375, -5.4688, -5.4688, -5.4375,  0.7383, -1.1406, -2.1406, -1.7812,\n",
      "         -2.3125, -3.6719, -3.9844, -1.8359, -1.6641, -3.2344, -2.5625, -2.0625,\n",
      "          0.7148,  0.3359,  0.4863, -1.3672, -5.3125, -3.4062, -3.9688, -3.9375,\n",
      "         -2.6250, -3.5938, -2.2188, -2.0156, -3.1250, -3.1875, -1.2812, -1.5156,\n",
      "         -3.7344, -3.8594, -2.7031, -2.0000, -2.9844, -4.2812, -3.2031, -3.5938,\n",
      "         -2.2344, -3.4219, -2.9688, -4.0312, -4.9688, -2.0312, -2.2344, -4.7812,\n",
      "          1.7031, -4.2500, -3.2969, -1.7969, -0.9141, -4.1562, -2.5469, -3.1562,\n",
      "         -2.5156, -4.6250, -4.0625, -2.9219, -2.9688, -2.9062, -3.8281, -3.5781,\n",
      "         -3.4531, -3.6250, -4.9062, -2.3281, -2.7656,  0.8008,  0.0197,  0.9609,\n",
      "          2.2500,  0.4941,  1.8359, -2.7188, -3.2969, -0.8008, -2.1562, -1.7578,\n",
      "         11.5000,  0.6602,  3.6562,  0.6797,  2.7344, -2.0156,  2.3281,  1.7812,\n",
      "         -0.7656, -0.5977, -0.5703, -2.5156, -1.2422, -0.9180, -1.4219, -4.1250,\n",
      "         -2.7969, -3.5312, -4.7812, -5.5938, -4.0938, -3.9531, -3.7656, -4.9062,\n",
      "         -4.4375, -4.8750, -4.0312, -5.0625, -3.4688, -4.3438, -4.3750, -4.4375,\n",
      "         -5.0625, -4.7500, -4.7812, -4.1562, -4.5000, -4.4688, -4.6250, -3.5938,\n",
      "         -3.9688, -4.8125, -4.8750, -4.6875, -4.0625, -3.2188, -4.6875, -4.7812,\n",
      "         -3.5469, -6.2188, -5.1250, -5.4375, -4.5938, -4.5938, -4.9688, -5.0312,\n",
      "         -4.7188, -4.3438, -4.5000, -4.8125, -4.8438, -4.7812, -4.6562, -4.9375,\n",
      "         -4.8125, -5.0312, -4.7812, -4.0312, -3.9219, -4.2812, -4.0938, -4.9375,\n",
      "         -4.8750, -4.9062, -4.6875, -4.0000, -4.8125, -4.9062, -4.8125, -4.4688,\n",
      "         -4.8125, -5.2500, -4.8125, -4.8750, -5.4062, -5.4375, -2.9531, -2.2500,\n",
      "         -5.0625, -5.0625, -5.4688, -5.1875, -5.3438, -5.4375, -5.4688, -5.3750,\n",
      "         -5.0625, -5.5000, -4.5625, -4.5312, -5.0000, -5.0312, -5.4375, -5.3750,\n",
      "         -5.5938, -5.3750, -5.4375, -5.4062, -5.3750, -5.4688, -5.4375, -5.4375,\n",
      "         -5.4375, -5.4375, -5.4062, -5.5312, -5.2812, -5.5000, -2.7812, -4.5938,\n",
      "         -4.7500, -4.7812, -4.9062, -4.9062, -5.0312, -5.4062, -5.2500, -5.3750,\n",
      "         -5.2500, -5.4688, -5.4062, -4.8125, -4.0312, -5.5000, -5.5625, -5.3750,\n",
      "         -5.4062, -5.4688, -5.4688, -5.3750, -5.4375, -5.5625, -5.5312, -5.4375,\n",
      "         -5.4688, -5.5938, -5.3750, -5.4375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 148\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          7]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 148])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 148, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0801, -0.7969,  0.8633,  ...,  2.3438,  1.0703,  1.4688],\n",
      "        [ 1.5156,  0.6797,  0.3887,  ...,  0.3008, -0.2412, -0.0059],\n",
      "        [ 0.0605,  1.5234, -0.7812,  ...,  2.1875,  0.5586, -0.3945]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -59.0000,  -488.0000,   324.0000,  ...,  1104.0000,\n",
      "            276.0000,  -176.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9344.,  -3040., -24064.,  ...,   -724.,   8576., -27648.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 148, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0801, -0.7969,  0.8633,  ...,  2.3438,  1.0703,  1.4688],\n",
      "        [ 1.5156,  0.6797,  0.3887,  ...,  0.3008, -0.2412, -0.0059],\n",
      "        [ 0.0605,  1.5234, -0.7812,  ...,  2.1875,  0.5586, -0.3945]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 148]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 18]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 148, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.9375, -5.4688,  4.6562, -6.7812, -6.9375, -6.8438, -6.7812, -6.9375,\n",
      "         -6.9375, -6.9688, -6.8125, -6.8750, -6.7500, -3.3594,  4.3750, -6.9375,\n",
      "         -6.9062, -6.4062, -6.9375, -6.8438, -6.7500, -6.9062, -6.9375, -6.7500,\n",
      "         -6.8438, -6.8750, -6.8125, -6.9062, -6.7812, -6.8438, -6.9375, -6.8750,\n",
      "         -6.8438, -6.8750, -6.8438, -6.8438,  4.6875,  2.7344,  0.3457, -1.7109,\n",
      "         -2.7656, -3.2188, -2.6562, -1.3516, -0.2910,  1.4375,  1.4375, -1.3984,\n",
      "          6.7812,  1.6719,  5.1250, -0.5820, -3.4219, -2.5938, -3.8594, -1.9297,\n",
      "         -2.5000, -2.1719, -2.1875, -3.1250, -2.7500, -2.9062,  0.6172,  1.3672,\n",
      "         -1.8125, -0.8086, -1.6719, -0.1543, -2.4844, -2.1406, -3.5938, -2.7656,\n",
      "         -1.1172, -1.2422, -2.5625, -2.6250, -3.8125, -1.9922, -2.2969, -3.0156,\n",
      "         -3.2344, -3.6562, -2.5312, -0.5273, -3.4531, -3.2500, -3.1250, -0.6641,\n",
      "         -2.6406, -1.9062, -2.5938, -2.4688, -2.6250, -2.6250, -2.8281, -2.6250,\n",
      "         -2.7344,  2.7656, -2.4219, -2.2969, -1.8594,  0.9961, -3.1094, -2.8281,\n",
      "          1.4219,  2.6250, -2.1562, -1.1953, -2.0312,  1.5312, -2.5156, -2.7344,\n",
      "          0.5820, -1.5859, -0.8906,  2.1562, -2.8438, -1.7188, -2.2344,  2.9531,\n",
      "         -1.3672,  0.4824, -1.4844, -2.2031, -2.2656, -1.0781, -2.2031, -3.3750,\n",
      "         -1.7578, -0.9570, -5.5000, -7.0312, -3.3125, -4.0625, -3.8125, -5.0000,\n",
      "         -4.9688, -4.8750, -5.0000, -6.0938, -2.5625, -3.7656, -4.8438, -4.5938,\n",
      "         -3.7188, -3.9688, -4.1562, -4.2812, -4.4688, -4.9688, -3.9375, -1.3672,\n",
      "         -2.2031, -5.4375, -5.9375, -3.5000, -3.6719, -0.9609, -3.1875, -5.1875,\n",
      "         -3.2188, -4.8125, -6.1562, -3.8438, -4.7812, -4.3125, -4.1875, -5.3750,\n",
      "         -4.8125, -4.4062, -3.9688, -4.5312, -4.5312, -2.8906, -5.0312, -5.5938,\n",
      "         -5.5000, -5.3438, -5.5938, -4.1875, -4.1875, -3.5156, -3.4219, -4.1562,\n",
      "         -5.4688, -5.5000, -5.1875, -4.0938, -4.5312, -5.4688, -4.9688, -4.9062,\n",
      "         -4.2500, -4.8125, -5.0938, -5.0938, -6.8750, -6.9375, -2.8906, -0.8555,\n",
      "         -6.2188, -6.1562, -6.9375, -6.4688, -6.6875, -6.9062, -6.9375, -6.5312,\n",
      "         -6.2500, -6.9688, -4.6875, -3.7969, -6.0312, -6.0312, -6.8438, -6.7500,\n",
      "         -7.0000, -6.8750, -6.8750, -6.8438, -7.0000, -6.8438, -6.8750, -6.8750,\n",
      "         -6.8750, -6.7812, -6.8750, -6.8750, -6.5938, -7.0312, -1.2969, -4.9688,\n",
      "         -5.4375, -5.3125, -5.3750, -6.1250, -6.0000, -6.3125, -6.6562, -6.9062,\n",
      "         -6.6875, -7.0000, -6.8125, -4.9688, -2.8438, -6.9062, -6.9375, -6.7812,\n",
      "         -6.9062, -6.9688, -6.9062, -6.8438, -6.8125, -6.9688, -6.9688, -6.9062,\n",
      "         -6.9375, -6.9375, -6.8438, -6.9375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 149\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 152, 149])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 149, 256]), Cross output shape: torch.Size([1, 152, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.5156,  0.6797,  0.3887,  ...,  0.3008, -0.2412, -0.0059],\n",
      "        [ 0.0605,  1.5234, -0.7812,  ...,  2.1875,  0.5586, -0.3945],\n",
      "        [-1.7344,  0.4766, -0.3633,  ...,  0.0215, -0.7305, -1.3281]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 19, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -212.0000],\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 19, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3168., -25088.,  ...,   -808.,   9024., -28928.],\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 149, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.5156,  0.6797,  0.3887,  ...,  0.3008, -0.2412, -0.0059],\n",
      "        [ 0.0605,  1.5234, -0.7812,  ...,  2.1875,  0.5586, -0.3945],\n",
      "        [-1.7344,  0.4766, -0.3633,  ...,  0.0215, -0.7305, -1.3281]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 149]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 149, 152])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-4.7500, -6.1875,  2.7031, -4.8125, -4.8125, -4.8750, -4.6875, -4.8125,\n",
      "         -4.9062, -4.8125, -4.7812, -4.8750, -4.8125, -2.4375,  4.4375, -4.9375,\n",
      "         -4.7500, -4.6562, -4.8750, -4.7188, -4.8125, -4.7812, -4.7812, -4.8125,\n",
      "         -4.8438, -4.7500, -4.8125, -4.8125, -4.8750, -4.8125, -4.7812, -4.7812,\n",
      "         -4.8125, -4.8750, -4.8438, -4.8125,  9.8750, -2.0938,  0.3867, -1.2266,\n",
      "         -2.4688, -4.2812, -0.8672, -2.2344, -1.1250,  0.3203, -0.4180, -1.3125,\n",
      "          0.7930,  0.6484,  0.7070,  0.9453, -1.8438, -0.9844, -1.4219, -1.9922,\n",
      "         -2.1875, -1.7422, -2.3281, -3.1250, -3.2812, -2.9375,  0.0400, -2.2969,\n",
      "         -0.0732,  0.4219, -0.1777, -1.7266, -3.7344, -1.7734, -1.8203, -0.8438,\n",
      "         -1.8984, -2.6562, -1.3750, -2.6094, -1.8984, -1.0000, -1.4922, -2.4531,\n",
      "         -2.5625, -0.5664, -1.4844, -1.5469, -1.8984, -2.5781, -1.3594, -0.8672,\n",
      "         -1.7500, -2.4375,  0.1113, -1.2969, -2.6562, -2.2500, -2.1406, -1.7344,\n",
      "         -3.7812, -0.0684, -2.5312, -1.6250, -5.3125, -1.4453, -1.6719, -0.5938,\n",
      "         -1.2500, -1.8516, -1.8594, -1.2734, -1.6719, -0.1074, -1.4922, -0.9102,\n",
      "         -1.0703, -0.6641, -0.8125, -1.8203, -1.5000, -3.2188, -1.2812, -0.3887,\n",
      "         -0.7812, -2.4062, -0.9062, -1.1797, -1.8438, -1.4688, -2.4062, -3.0625,\n",
      "          0.0131, -1.1562, -4.4688, -4.8750, -3.6562, -4.2500, -3.9688, -4.2188,\n",
      "         -4.2500, -4.3125, -3.8906, -4.5000, -3.2656, -4.0000, -3.9688, -4.1562,\n",
      "         -3.6719, -3.7656, -4.2812, -3.4688, -3.7031, -4.2500, -2.5469, -3.0781,\n",
      "         -3.2344, -4.2812, -4.2188, -2.4531, -3.8281, -4.5938, -3.1875, -4.0938,\n",
      "         -2.9219, -2.6406, -4.4688, -3.8750, -3.9688, -3.7812, -4.1875, -4.0625,\n",
      "         -3.9062, -3.9062, -3.7656, -3.4531, -4.0000, -2.9531, -3.9844, -4.3750,\n",
      "         -4.4688, -4.1250, -4.1562, -4.0625, -4.4688, -3.5625, -3.5469, -3.6094,\n",
      "         -4.1250, -4.2188, -4.4062, -3.7656, -3.9219, -4.1250, -4.0938, -3.7344,\n",
      "         -3.2969, -3.2344, -3.9531, -4.2188, -4.7500, -4.8750, -1.9766, -1.4922,\n",
      "         -4.4062, -4.6250, -4.7812, -4.6875, -4.8125, -4.8438, -4.8125, -4.5938,\n",
      "         -4.4688, -4.8750, -4.2812, -3.9688, -4.3125, -4.3438, -4.7500, -4.8125,\n",
      "         -5.0312, -4.8750, -4.7500, -4.8750, -4.8438, -4.8125, -4.7812, -4.8125,\n",
      "         -4.7812, -4.8438, -4.8125, -4.8125, -4.7812, -4.8438, -1.6562, -3.9531,\n",
      "         -4.3750, -4.3750, -4.4062, -4.6250, -4.5938, -4.7812, -4.7188, -4.6562,\n",
      "         -4.5625, -4.8750, -4.7812, -4.0312, -3.8438, -4.9062, -4.9062, -4.7500,\n",
      "         -4.8438, -4.7500, -4.7812, -4.8438, -4.8750, -4.9062, -4.7812, -4.7500,\n",
      "         -4.9375, -4.8750, -4.8750, -4.8125]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 150\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 150])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 150, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0605,  1.5234, -0.7812,  ...,  2.1875,  0.5586, -0.3945],\n",
      "        [-1.7344,  0.4766, -0.3633,  ...,  0.0215, -0.7305, -1.3281],\n",
      "        [-1.0234,  0.9531,  1.9219,  ...,  0.7109, -0.4531, -0.7148]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  108.0000,   -96.0000,   -68.5000,  ...,   314.0000,\n",
      "             28.7500,   -25.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  2256.,  -1424.,  -5376.,  ...,    101.,   2304.,  -7456.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 150, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0605,  1.5234, -0.7812,  ...,  2.1875,  0.5586, -0.3945],\n",
      "        [-1.7344,  0.4766, -0.3633,  ...,  0.0215, -0.7305, -1.3281],\n",
      "        [-1.0234,  0.9531,  1.9219,  ...,  0.7109, -0.4531, -0.7148]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 150]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 150, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.9375,  -9.6875,  -5.4062, -10.9375, -10.9375, -11.0000, -11.0625,\n",
      "         -10.9375, -10.9375, -11.0000, -11.0000, -11.0000, -11.0000,  -7.4688,\n",
      "          -1.2891, -11.0625, -11.0625, -10.5000, -11.0625, -11.0625, -11.0000,\n",
      "         -11.0000, -11.0000, -11.0625, -11.0000, -11.0000, -11.1250, -11.0000,\n",
      "         -11.0625, -11.0000, -11.0625, -11.0000, -11.1250, -11.0000, -11.0625,\n",
      "         -11.0000,  -4.2812,  -6.9062,  -1.6562,  -4.2188,  -6.4375,  -7.2188,\n",
      "          -5.6562,  -5.5312,  -3.1875,  -6.6250,  -5.1562,  -5.4062,  -3.6406,\n",
      "          -2.7344,  -3.5625,  -3.3125,  -5.5625,  -3.7188,  -4.8438,  -4.8750,\n",
      "          -5.5625,  -5.6562,  -6.3125,  -6.2812,  -5.5000,  -5.6250,  -7.1250,\n",
      "          -4.2188,  -6.0938,  -6.3125,  -6.5000,  -9.1250,  -7.3438,  -3.9531,\n",
      "          -4.4375,  -4.6562,  -5.9688,  -4.2188,  -4.4062,  -4.0000,  -4.3750,\n",
      "          -2.3438,  -2.5000,  -4.5000,  -4.6562,  -3.8594,  -5.2812,  -5.5312,\n",
      "          -2.7188,  -6.3125,  -4.0625,  -3.5625,  -4.4688,  -4.3125,  -6.1875,\n",
      "          -5.3750,  -6.2188,  -6.2812,  -5.9688,  -4.5938,  -8.1875,  -5.3438,\n",
      "          -7.0625,  -4.5000,  -5.5312,   3.0469,   2.2656,   1.6094,   0.9180,\n",
      "           0.7148,   1.5938,   1.4609,   0.3555,   0.8828,   1.1953,   0.2520,\n",
      "           0.3223,   1.4141,  -0.6719,   0.8945,   3.3438,  -0.9141,   1.2344,\n",
      "           1.6406,   1.9609,   0.1338,  -0.3516,   0.8281,  -5.1250,   0.3066,\n",
      "          -3.4062,  -5.8750,  -6.1875,  -5.1250,  -9.6875, -11.0000,  -7.4375,\n",
      "          -9.0625,  -8.8750,  -9.6875,  -9.6875,  -9.3750,  -9.0000, -10.5000,\n",
      "          -7.8750,  -8.3750,  -9.4375,  -8.9375,  -8.9375,  -9.6250,  -9.6250,\n",
      "          -9.3125,  -9.5625,  -9.8125,  -7.7812,  -7.1250,  -8.1875, -10.1875,\n",
      "         -10.1250,  -8.9375,  -8.5000,  -9.7500,  -7.0938, -10.1250,  -7.9375,\n",
      "          -7.6875, -10.3125,  -8.7500,  -8.9375,  -9.3125,  -9.0000,  -9.8750,\n",
      "          -9.5000,  -9.1250,  -8.3750,  -9.5000,  -9.6250,  -7.3750,  -9.5625,\n",
      "         -10.0000, -10.2500, -10.1875,  -9.9375,  -9.6875,  -7.3438,  -8.1250,\n",
      "          -9.0000,  -8.8125,  -9.8125,  -9.8750, -10.0625,  -9.2500,  -9.0625,\n",
      "         -10.0000,  -9.7500,  -9.5625,  -8.5000,  -8.5000,  -9.7500,  -9.8125,\n",
      "         -11.0625, -10.9375,  -7.0000,  -4.5938, -10.2500, -10.3750, -10.9375,\n",
      "         -10.6250, -10.8750, -10.5000, -11.0000, -10.8750, -10.5000, -11.0000,\n",
      "          -9.5000,  -9.1875, -10.2500, -10.0000, -10.9375, -11.0625, -11.0625,\n",
      "         -11.0000, -11.0000, -11.0625, -11.0000, -10.9375, -11.0000, -11.0000,\n",
      "         -10.9375, -11.0625, -11.0625, -11.0000, -10.8125, -11.0000,  -4.4375,\n",
      "          -9.6250, -10.1875,  -9.8125,  -9.9375, -10.1250, -10.3750, -10.6250,\n",
      "         -10.7500, -10.9375, -10.8750, -11.0000, -11.0000,  -9.4375,  -7.1875,\n",
      "         -11.0625, -11.0625, -10.9375, -11.0000, -11.0625, -11.0000, -11.1250,\n",
      "         -11.0000, -11.0625, -11.0000, -11.0625, -11.0000, -11.0625, -11.0625,\n",
      "         -11.0000]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 151\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 151])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 151, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.7344,  0.4766, -0.3633,  ...,  0.0215, -0.7305, -1.3281],\n",
      "        [-1.0234,  0.9531,  1.9219,  ...,  0.7109, -0.4531, -0.7148],\n",
      "        [-0.3691, -0.5938,  3.2344,  ...,  1.1875,  1.4844,  0.1660]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -10.6250,  -241.0000,    87.0000,  ...,   808.0000,\n",
      "            131.0000,   -47.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  5440.,  -1992., -13696.,  ...,   -251.,   5152., -16320.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 151, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.7344,  0.4766, -0.3633,  ...,  0.0215, -0.7305, -1.3281],\n",
      "        [-1.0234,  0.9531,  1.9219,  ...,  0.7109, -0.4531, -0.7148],\n",
      "        [-0.3691, -0.5938,  3.2344,  ...,  1.1875,  1.4844,  0.1660]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 151]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 151, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.9688, -8.6875, -3.8438, -7.0000, -6.9688, -6.9062, -6.9688, -7.0000,\n",
      "         -6.9375, -6.8750, -6.9375, -6.9062, -6.9375, -5.1875, -3.3750, -6.9375,\n",
      "         -6.9062, -6.6875, -6.9375, -6.9062, -6.9375, -6.9375, -6.7500, -6.9688,\n",
      "         -6.9062, -6.8438, -6.9375, -6.9062, -6.9688, -6.8750, -6.9375, -6.8438,\n",
      "         -6.8750, -6.8750, -6.8438, -6.9375, -2.0469, -5.0625, -3.9062, -4.2812,\n",
      "         -4.9062, -5.0000, -4.5312, -3.5625, -3.3281, -3.8750, -3.9688, -5.2812,\n",
      "         -1.7109, -2.8125, -1.4922, -3.8906, -4.5625, -4.7812, -4.5000, -4.1250,\n",
      "         -4.5625, -5.8125, -4.2812, -5.6875, -4.8750, -5.8750, -4.0312, -3.0781,\n",
      "         -5.6250, -4.9062, -3.4844, -5.6250, -5.1250, -4.6250, -6.1250, -5.0938,\n",
      "         -5.1875, -1.8750, -5.4062, -4.6875, -3.8125, -4.1250, -5.2812, -5.1875,\n",
      "         -4.4375, -4.9688, -4.7500, -4.6562, -4.5312, -6.5625, -4.7812, -4.8438,\n",
      "         -4.2812, -3.8906, -5.3438, -4.9688, -5.1562, -5.0938, -4.8125, -4.2500,\n",
      "         -5.4062, -5.1250, -5.7188, -4.2500, -6.4375,  3.8906, -1.8359, -1.1172,\n",
      "         -1.2031,  8.8750, -2.4688, -1.5391,  2.5781,  4.5000, -3.6875, -1.9453,\n",
      "          2.5000, -1.2422, -0.7383,  4.2188, -0.6914, -3.2344,  3.2344,  0.2080,\n",
      "         -0.7383,  4.4375, -2.0000, -1.6094, -3.9844,  1.3281, -1.8281, -4.2812,\n",
      "         -4.9688, -4.0625, -6.3438, -6.9375, -4.4375, -5.7812, -5.7500, -6.2812,\n",
      "         -6.3750, -6.2500, -6.1250, -6.5312, -5.7812, -5.7500, -5.9062, -5.7500,\n",
      "         -5.7812, -5.8750, -6.0625, -6.1562, -6.0000, -6.0312, -5.5000, -6.4375,\n",
      "         -5.6250, -6.3438, -6.4688, -6.0312, -5.9375, -5.2812, -4.7500, -6.4688,\n",
      "         -5.5938, -4.7500, -6.4688, -5.7812, -5.7812, -6.2188, -6.0000, -6.4375,\n",
      "         -6.2812, -5.7500, -5.2500, -6.4688, -6.0312, -5.7188, -6.2500, -6.4062,\n",
      "         -6.4375, -6.3750, -6.2500, -6.3125, -5.1562, -5.3438, -5.0938, -5.5938,\n",
      "         -6.3125, -6.2500, -6.3438, -5.9375, -6.0000, -6.4062, -6.2812, -6.3125,\n",
      "         -5.5938, -5.6875, -6.0625, -6.0625, -6.8750, -6.7812, -5.0938, -0.5352,\n",
      "         -6.5938, -6.6562, -6.8750, -6.7812, -6.8125, -6.7500, -7.0000, -6.9375,\n",
      "         -6.6250, -6.9688, -6.3750, -5.5625, -6.5625, -6.3750, -6.9062, -6.9375,\n",
      "         -7.0312, -6.9062, -6.9688, -7.0312, -6.9062, -6.9688, -6.9375, -6.8750,\n",
      "         -6.9062, -7.0000, -6.9375, -6.9062, -6.8438, -6.9688, -4.3438, -6.2812,\n",
      "         -6.4375, -6.1250, -6.3750, -6.4375, -6.5000, -6.7500, -6.7188, -6.9375,\n",
      "         -6.7812, -6.9688, -6.9062, -5.9688, -4.4688, -6.9375, -6.9688, -6.8750,\n",
      "         -6.9688, -7.0000, -6.8750, -6.9062, -6.8750, -6.9375, -6.9688, -7.0000,\n",
      "         -6.8750, -6.8750, -7.0000, -6.9062]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 152\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 152])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 152, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0234,  0.9531,  1.9219,  ...,  0.7109, -0.4531, -0.7148],\n",
      "        [-0.3691, -0.5938,  3.2344,  ...,  1.1875,  1.4844,  0.1660],\n",
      "        [-0.3086,  0.5820,  0.1973,  ...,  0.9453, -0.9648, -1.1094]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -50.5000,  -316.0000,   160.0000,  ...,   896.0000,\n",
      "            163.0000,  -106.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  6720.,  -2304., -17024.,  ...,   -380.,   6240., -19968.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 152, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0234,  0.9531,  1.9219,  ...,  0.7109, -0.4531, -0.7148],\n",
      "        [-0.3691, -0.5938,  3.2344,  ...,  1.1875,  1.4844,  0.1660],\n",
      "        [-0.3086,  0.5820,  0.1973,  ...,  0.9453, -0.9648, -1.1094]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 152]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 152, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.1250, -11.1250,  -5.1562, -10.1250, -10.1875, -10.0000, -10.1250,\n",
      "         -10.1250, -10.1875, -10.1875, -10.1875, -10.1875, -10.1250,  -6.4062,\n",
      "          -3.3438, -10.1250, -10.2500,  -9.8125, -10.1875, -10.1250, -10.1250,\n",
      "         -10.1875, -10.0625, -10.0625, -10.0625, -10.1250, -10.0625, -10.1250,\n",
      "         -10.1250, -10.1250, -10.1250, -10.1250, -10.0000, -10.0625, -10.0625,\n",
      "         -10.2500,  -1.2031,  -5.0938,  -2.8750,  -6.3750,  -6.6562,  -5.2812,\n",
      "          -5.5000,  -3.3281,  -4.7812,  -4.6250,  -5.6562,  -5.6250,  -1.0000,\n",
      "          -1.6875,  -0.9453,  -3.3125,  -4.2812,  -4.8125,  -4.9688,  -4.8750,\n",
      "          -4.4062,  -5.8438,  -5.4375,  -4.4375,  -6.3750,  -5.7812,  -3.4688,\n",
      "          -3.6094,  -6.0000,  -4.9688,  -4.3438,  -4.9688,  -6.9688,  -4.8750,\n",
      "          -5.8438,  -4.7188,  -6.1875,  -5.2500,  -5.9688,  -4.6562,  -5.5312,\n",
      "          -5.3125,  -5.1250,  -6.0625,  -5.8750,  -4.6250,  -2.3281,  -5.5312,\n",
      "          -4.0938,  -5.1875,  -5.5938,  -3.7500,  -4.1875,  -6.0000,  -6.1250,\n",
      "          -4.8750,  -4.5000,  -6.3750,  -5.9375,  -6.1250,  -7.2500,  -6.3125,\n",
      "          -5.4688,  -5.5000,  -6.1562,   4.3125,  -0.9102,   1.5078,   0.7383,\n",
      "           3.2344,  -2.1875,   0.1504,  -1.5547,  -1.0000,  -2.9688,  -0.8672,\n",
      "           2.0938,  -0.0459,   9.2500,   1.3281,   3.1406,  -0.9453,   3.7188,\n",
      "           1.3594,   3.1094,  -0.7969,  -0.4727,  -0.7305,  -0.4121,  -2.1562,\n",
      "          -1.7344,  -4.3750,  -4.8125,  -4.7188,  -9.3125, -10.1250,  -5.9688,\n",
      "          -7.4688,  -7.0938,  -8.6250,  -8.8125,  -8.9375,  -8.1250,  -9.1250,\n",
      "          -6.3750,  -6.9062,  -8.1250,  -7.6875,  -7.3438,  -7.7500,  -7.9375,\n",
      "          -7.7188,  -8.1250,  -8.6250,  -6.6562,  -6.2812,  -7.1875,  -8.3750,\n",
      "          -8.9375,  -7.5000,  -7.3438,  -6.5312,  -6.7500,  -8.6875,  -6.8125,\n",
      "          -8.0000,  -9.2500,  -7.2812,  -7.4688,  -8.0625,  -8.0000,  -8.6250,\n",
      "          -8.1250,  -7.9062,  -7.1562,  -8.5000,  -7.9688,  -6.7500,  -8.3125,\n",
      "          -9.0000,  -9.1250,  -8.7500,  -8.5000,  -8.1250,  -8.0625,  -7.1875,\n",
      "          -6.2812,  -7.5312,  -8.5000,  -8.6875,  -9.0625,  -7.8438,  -7.3438,\n",
      "          -9.0000,  -8.5625,  -8.3125,  -7.6562,  -7.8125,  -8.4375,  -8.5000,\n",
      "         -10.1250, -10.1250,  -6.5625,  -3.1406,  -9.3125,  -9.3125, -10.0625,\n",
      "          -9.8125, -10.0625, -10.0000, -10.0625, -10.0000,  -9.4375, -10.1250,\n",
      "          -8.8125,  -7.6562,  -9.5000,  -9.5000, -10.1875, -10.1875, -10.3750,\n",
      "         -10.0625, -10.1875, -10.1250, -10.0000, -10.1875, -10.1250, -10.0625,\n",
      "         -10.1250, -10.1250, -10.1875, -10.2500,  -9.8125, -10.1875,  -4.6250,\n",
      "          -8.3750,  -8.9375,  -8.5625,  -8.9375,  -9.0000,  -9.1875,  -9.5625,\n",
      "         -10.0000, -10.1250, -10.0625, -10.1250, -10.1250,  -8.0625,  -6.0938,\n",
      "         -10.1875, -10.1250, -10.0625, -10.0625, -10.1250, -10.1875, -10.0625,\n",
      "         -10.0625, -10.1250, -10.1875, -10.1250, -10.1250, -10.1875, -10.1250,\n",
      "         -10.1250]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 153\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 153])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 153, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3691, -0.5938,  3.2344,  ...,  1.1875,  1.4844,  0.1660],\n",
      "        [-0.3086,  0.5820,  0.1973,  ...,  0.9453, -0.9648, -1.1094],\n",
      "        [ 2.3281, -0.0703, -1.5781,  ...,  1.0859, -0.9141, -1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -48.5000,  -372.0000,   232.0000,  ...,   952.0000,\n",
      "            203.0000,  -127.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  7584.,  -2560., -19328.,  ...,   -476.,   7008., -22400.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 153, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3691, -0.5938,  3.2344,  ...,  1.1875,  1.4844,  0.1660],\n",
      "        [-0.3086,  0.5820,  0.1973,  ...,  0.9453, -0.9648, -1.1094],\n",
      "        [ 2.3281, -0.0703, -1.5781,  ...,  1.0859, -0.9141, -1.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 153]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 153, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.5000e+00, -7.3438e+00,  1.5469e+00, -8.5625e+00, -8.5000e+00,\n",
      "         -8.5000e+00, -8.5000e+00, -8.5625e+00, -8.5625e+00, -8.5625e+00,\n",
      "         -8.6250e+00, -8.6875e+00, -8.5625e+00, -5.0625e+00,  2.7344e+00,\n",
      "         -8.5625e+00, -8.6250e+00, -8.3125e+00, -8.6250e+00, -8.6250e+00,\n",
      "         -8.5000e+00, -8.5625e+00, -8.5000e+00, -8.5000e+00, -8.4375e+00,\n",
      "         -8.5625e+00, -8.5000e+00, -8.5000e+00, -8.5000e+00, -8.6250e+00,\n",
      "         -8.6250e+00, -8.6250e+00, -8.5000e+00, -8.5000e+00, -8.5000e+00,\n",
      "         -8.5000e+00,  4.4688e+00,  1.0889e-01, -1.3125e+00, -3.1094e+00,\n",
      "         -3.8281e+00, -4.1562e+00, -3.8906e+00, -1.3594e+00, -1.6641e+00,\n",
      "         -1.5703e+00,  4.5898e-02, -3.7969e+00,  4.0625e+00,  2.6367e-01,\n",
      "          3.3125e+00, -1.2891e+00, -1.8359e+00, -2.5938e+00, -3.5781e+00,\n",
      "         -3.1094e+00, -1.6719e+00, -2.3281e+00, -3.1875e+00, -2.4219e+00,\n",
      "         -4.4062e+00, -1.4297e+00, -1.4877e-03, -7.6953e-01, -1.1641e+00,\n",
      "         -1.9688e+00, -4.4062e+00, -1.2266e+00, -4.3438e+00, -2.6719e+00,\n",
      "         -3.1875e+00,  5.5078e-01, -3.7812e+00, -4.3438e+00, -3.6406e+00,\n",
      "         -2.2344e+00, -3.6094e+00, -8.8672e-01, -2.3750e+00, -3.5781e+00,\n",
      "         -5.4062e+00, -4.1875e+00, -3.9219e+00, -2.5312e+00, -4.7500e+00,\n",
      "         -2.1250e+00, -3.0781e+00, -3.1719e+00, -2.5781e+00, -2.6719e+00,\n",
      "         -2.9531e+00, -3.7656e+00, -3.4844e+00, -3.9844e+00, -3.9219e+00,\n",
      "         -3.5312e+00, -4.2812e+00, -2.4219e+00, -1.0000e+00, -3.0625e+00,\n",
      "         -2.7188e+00, -4.6484e-01, -2.5469e+00,  8.2500e+00,  6.3281e-01,\n",
      "          6.3672e-01, -9.8438e-01,  2.1562e+00, -3.8281e-01,  1.0859e+00,\n",
      "          1.0625e+00,  2.9688e-01, -2.9531e+00, -2.0938e+00,  1.9375e+00,\n",
      "         -1.2812e+00, -2.8906e+00,  9.6191e-02, -7.7344e-01,  1.3203e+00,\n",
      "          1.5000e+00, -1.0938e+00, -1.5391e+00, -1.9297e+00, -2.1406e+00,\n",
      "         -9.0625e-01,  1.6406e+00, -1.8125e+00, -2.5625e+00, -2.3438e+00,\n",
      "         -7.3125e+00, -8.6250e+00, -4.7188e+00, -5.0312e+00, -5.0625e+00,\n",
      "         -6.5938e+00, -6.1875e+00, -6.6250e+00, -5.9375e+00, -7.6562e+00,\n",
      "         -4.6562e+00, -4.0625e+00, -5.6250e+00, -5.8125e+00, -4.6562e+00,\n",
      "         -5.3750e+00, -5.6250e+00, -5.0938e+00, -5.9375e+00, -6.5000e+00,\n",
      "         -4.5625e+00, -2.1719e+00, -4.0625e+00, -6.5938e+00, -7.4062e+00,\n",
      "         -4.9375e+00, -3.9844e+00, -3.3125e+00, -4.3438e+00, -6.5938e+00,\n",
      "         -3.4375e+00, -4.1250e+00, -7.5000e+00, -5.6562e+00, -5.8438e+00,\n",
      "         -5.6250e+00, -6.0625e+00, -6.4062e+00, -5.9688e+00, -5.5312e+00,\n",
      "         -5.0000e+00, -5.7812e+00, -6.0000e+00, -5.0938e+00, -6.1875e+00,\n",
      "         -7.1875e+00, -7.1250e+00, -6.8438e+00, -7.0938e+00, -5.8438e+00,\n",
      "         -5.5000e+00, -4.5312e+00, -4.0938e+00, -5.1875e+00, -7.0625e+00,\n",
      "         -6.9062e+00, -7.2188e+00, -5.5938e+00, -5.1250e+00, -6.9062e+00,\n",
      "         -6.8438e+00, -6.4688e+00, -4.5312e+00, -4.8438e+00, -6.5000e+00,\n",
      "         -6.7812e+00, -8.5000e+00, -8.5000e+00, -2.3125e+00, -1.9922e+00,\n",
      "         -7.7188e+00, -8.0000e+00, -8.5000e+00, -8.0000e+00, -8.3750e+00,\n",
      "         -8.3750e+00, -8.5000e+00, -8.1875e+00, -7.4688e+00, -8.6250e+00,\n",
      "         -6.5938e+00, -5.0000e+00, -7.5625e+00, -7.6562e+00, -8.5000e+00,\n",
      "         -8.5625e+00, -8.6250e+00, -8.6250e+00, -8.6875e+00, -8.5625e+00,\n",
      "         -8.5625e+00, -8.5625e+00, -8.5000e+00, -8.5000e+00, -8.5625e+00,\n",
      "         -8.6875e+00, -8.3125e+00, -8.5000e+00, -8.1875e+00, -8.5000e+00,\n",
      "         -2.2188e+00, -5.9375e+00, -7.1250e+00, -6.4688e+00, -6.9375e+00,\n",
      "         -7.0938e+00, -7.6875e+00, -7.7500e+00, -8.2500e+00, -8.4375e+00,\n",
      "         -8.4375e+00, -8.5625e+00, -8.5000e+00, -5.4688e+00, -3.2344e+00,\n",
      "         -8.5625e+00, -8.5625e+00, -8.5625e+00, -8.6250e+00, -8.5625e+00,\n",
      "         -8.5000e+00, -8.5625e+00, -8.4375e+00, -8.6250e+00, -8.4375e+00,\n",
      "         -8.6250e+00, -8.6250e+00, -8.6250e+00, -8.5000e+00, -8.5625e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 154\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 154])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 154, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3086,  0.5820,  0.1973,  ...,  0.9453, -0.9648, -1.1094],\n",
      "        [ 2.3281, -0.0703, -1.5781,  ...,  1.0859, -0.9141, -1.4219],\n",
      "        [-0.0801, -0.8398,  0.9648,  ...,  2.3594,  1.0156,  1.5469]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -41.0000,  -440.0000,   278.0000,  ...,  1056.0000,\n",
      "            243.0000,  -142.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  8576.,  -2880., -22016.,  ...,   -584.,   7904., -25472.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 154, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3086,  0.5820,  0.1973,  ...,  0.9453, -0.9648, -1.1094],\n",
      "        [ 2.3281, -0.0703, -1.5781,  ...,  1.0859, -0.9141, -1.4219],\n",
      "        [-0.0801, -0.8398,  0.9648,  ...,  2.3594,  1.0156,  1.5469]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 154]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 154, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.5625, -6.4062, -3.0469, -8.5000, -8.5625, -8.6250, -8.5625, -8.5625,\n",
      "         -8.5625, -8.5000, -8.6250, -8.6875, -8.7500, -5.1250, -3.2344, -8.6250,\n",
      "         -8.5625, -8.3750, -8.5625, -8.5625, -8.5000, -8.5000, -8.5625, -8.5625,\n",
      "         -8.5625, -8.6250, -8.5625, -8.5000, -8.6250, -8.6875, -8.6875, -8.5625,\n",
      "         -8.5625, -8.5625, -8.5625, -8.5000, -0.1807, -3.0469, -3.3750, -3.2656,\n",
      "         -3.6406, -4.6250, -5.6562, -3.0000, -3.8281, -4.3438, -4.4375, -3.7031,\n",
      "         -0.9727, -1.9844, -0.1963, -2.1250, -4.4688, -3.0000, -3.2188, -3.5625,\n",
      "         -2.9219, -4.4375, -4.1250, -3.5312, -3.5156, -4.2812, -1.8672, -4.3438,\n",
      "         -3.9375, -3.5156, -4.3438, -3.5000, -5.5312, -3.9062, -4.3750, -4.7500,\n",
      "         -4.1250, -3.6094, -4.1250, -4.6875, -3.7344, -0.5234, -3.3906, -5.0938,\n",
      "         -3.5781, -3.8906, -4.5938, -2.4062, -3.7656, -7.1562, -3.7031, -4.3438,\n",
      "         -3.6250, -4.5312, -4.8750, -5.5000, -5.3438, -4.0312, -5.0938, -4.8438,\n",
      "         -6.0000, -6.2500, -5.7812, -5.2188, -4.9688,  2.3438, -1.8672, -1.7734,\n",
      "         -2.1250,  4.3750, -1.2109, -2.5781,  1.1484, 11.1250, -3.3906, -1.3047,\n",
      "          2.5312, -1.8203, -2.1094,  3.0469, -1.7422, -4.6562,  1.3359,  0.0991,\n",
      "          0.3691,  1.2109, -3.0000, -2.7969, -5.5625,  3.7344, -1.8203, -4.5000,\n",
      "         -3.5000, -4.6562, -7.5312, -8.6250, -4.2812, -6.3125, -6.0312, -7.3438,\n",
      "         -6.9062, -7.1562, -7.0312, -8.0000, -6.3125, -5.6250, -6.8125, -6.4062,\n",
      "         -6.1875, -6.4688, -7.1562, -6.2188, -6.8750, -6.7188, -6.3438, -5.8750,\n",
      "         -5.7188, -7.6250, -7.6562, -6.8125, -6.2188, -5.2812, -6.6875, -7.3125,\n",
      "         -5.2188, -5.9688, -7.7188, -6.1562, -6.7812, -6.3125, -6.8125, -7.1562,\n",
      "         -6.9062, -6.2188, -6.8438, -6.7188, -6.8750, -5.6250, -6.9375, -7.5938,\n",
      "         -7.5000, -7.3438, -7.5000, -6.5938, -6.3125, -5.1875, -6.1875, -6.3438,\n",
      "         -7.6562, -7.4375, -7.5000, -6.6562, -6.1875, -7.5312, -7.3125, -6.9688,\n",
      "         -5.7812, -6.5312, -7.1562, -7.2188, -8.5000, -8.6250, -5.0000, -1.7422,\n",
      "         -8.0625, -8.0625, -8.5000, -8.2500, -8.5625, -8.0625, -8.5625, -8.1875,\n",
      "         -7.9375, -8.5000, -7.2500, -6.6875, -8.0625, -8.0000, -8.4375, -8.5625,\n",
      "         -8.6250, -8.6875, -8.6250, -8.6250, -8.5625, -8.5625, -8.5625, -8.6250,\n",
      "         -8.5000, -8.6875, -8.5625, -8.6250, -8.5000, -8.6875, -4.0938, -7.0312,\n",
      "         -7.5000, -7.5000, -7.2500, -7.8438, -7.9062, -8.2500, -8.3750, -8.4375,\n",
      "         -8.4375, -8.5000, -8.5625, -7.0312, -6.0312, -8.6250, -8.6875, -8.5625,\n",
      "         -8.5625, -8.6250, -8.5625, -8.6875, -8.6250, -8.6875, -8.5625, -8.6250,\n",
      "         -8.6875, -8.7500, -8.5000, -8.6250]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 155\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  6]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 155])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 155, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.3281, -0.0703, -1.5781,  ...,  1.0859, -0.9141, -1.4219],\n",
      "        [-0.0801, -0.8398,  0.9648,  ...,  2.3594,  1.0156,  1.5469],\n",
      "        [ 1.4531,  0.6797,  0.4316,  ...,  0.2422, -0.2236,  0.0566]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -47.0000,  -460.0000,   302.0000,  ...,  1088.0000,\n",
      "            260.0000,  -158.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9088.,  -2960., -23296.,  ...,   -672.,   8320., -26752.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 155, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.3281, -0.0703, -1.5781,  ...,  1.0859, -0.9141, -1.4219],\n",
      "        [-0.0801, -0.8398,  0.9648,  ...,  2.3594,  1.0156,  1.5469],\n",
      "        [ 1.4531,  0.6797,  0.4316,  ...,  0.2422, -0.2236,  0.0566]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 155]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 155, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.4375, -6.5938, -0.7461, -5.3750, -5.5312, -5.5312, -5.5312, -5.5000,\n",
      "         -5.4062, -5.4688, -5.5938, -5.5938, -5.4688, -4.1875,  0.4863, -5.5625,\n",
      "         -5.5625, -5.1562, -5.4688, -5.5000, -5.4375, -5.5625, -5.5000, -5.4062,\n",
      "         -5.4375, -5.4688, -5.4688, -5.4375, -5.4375, -5.5312, -5.5000, -5.4062,\n",
      "         -5.4375, -5.4688, -5.5000, -5.4375,  0.7617, -1.3750, -2.2031, -1.6953,\n",
      "         -2.2656, -3.7344, -3.9844, -1.9375, -1.6016, -3.2344, -2.7031, -2.0938,\n",
      "          0.4844,  0.4258,  0.5391, -1.4609, -5.2812, -3.3281, -4.0312, -3.9688,\n",
      "         -2.6719, -3.6406, -2.2188, -1.9844, -3.0938, -3.1406, -1.2656, -1.6016,\n",
      "         -3.6719, -3.9062, -2.6719, -2.1250, -2.9844, -4.2500, -3.2344, -3.5312,\n",
      "         -2.1406, -3.3594, -2.9219, -3.9844, -4.9062, -2.0469, -2.1094, -4.6562,\n",
      "          1.6641, -4.1250, -3.2812, -1.7578, -0.7891, -4.1562, -2.4062, -3.0938,\n",
      "         -2.4844, -4.6562, -3.9688, -2.9062, -2.9062, -2.8281, -3.7500, -3.5469,\n",
      "         -3.4844, -3.6562, -4.9062, -2.2812, -2.7188,  0.6484,  0.0306,  1.0078,\n",
      "          2.3594,  0.4590,  1.8828, -2.6094, -3.3125, -0.8867, -2.0312, -1.5781,\n",
      "         11.2500,  0.6328,  3.5938,  0.5977,  2.9688, -2.0625,  2.4062,  1.9531,\n",
      "         -0.8320, -0.8008, -0.3945, -2.4844, -1.2812, -0.8984, -1.4375, -4.1250,\n",
      "         -2.7500, -3.5156, -4.7812, -5.6250, -4.1250, -3.9375, -3.7969, -4.9062,\n",
      "         -4.4688, -4.8750, -4.0312, -5.0625, -3.4844, -4.3438, -4.3750, -4.4062,\n",
      "         -5.0938, -4.7812, -4.7812, -4.1875, -4.5312, -4.4688, -4.5938, -3.5938,\n",
      "         -4.0000, -4.8438, -4.9062, -4.7188, -4.0938, -3.2344, -4.6875, -4.7812,\n",
      "         -3.5469, -6.2500, -5.1250, -5.4062, -4.5625, -4.5938, -4.9688, -5.0625,\n",
      "         -4.7500, -4.3750, -4.5000, -4.8125, -4.8750, -4.7188, -4.6875, -4.9688,\n",
      "         -4.8438, -5.0625, -4.7812, -4.0312, -3.8438, -4.2812, -4.1250, -4.9062,\n",
      "         -4.8750, -4.9062, -4.7188, -4.0312, -4.8750, -4.9375, -4.8125, -4.5000,\n",
      "         -4.8438, -5.2812, -4.8125, -4.9062, -5.4375, -5.4375, -3.0469, -2.3750,\n",
      "         -5.0625, -5.0625, -5.4688, -5.2188, -5.3750, -5.4688, -5.4688, -5.3750,\n",
      "         -5.0938, -5.5312, -4.5938, -4.5312, -5.0312, -5.0312, -5.4688, -5.3750,\n",
      "         -5.6250, -5.3750, -5.4688, -5.4375, -5.3750, -5.4688, -5.4688, -5.4688,\n",
      "         -5.4375, -5.4375, -5.4062, -5.5312, -5.2812, -5.5312, -2.8438, -4.5938,\n",
      "         -4.7812, -4.7812, -4.9375, -4.9062, -5.0312, -5.4375, -5.2500, -5.3750,\n",
      "         -5.2500, -5.4688, -5.4062, -4.8438, -4.0938, -5.5000, -5.5625, -5.3750,\n",
      "         -5.4062, -5.4688, -5.5000, -5.4062, -5.4375, -5.5625, -5.5312, -5.4688,\n",
      "         -5.5000, -5.6250, -5.3750, -5.4375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 156\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  7]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 156])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 156, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0801, -0.8398,  0.9648,  ...,  2.3594,  1.0156,  1.5469],\n",
      "        [ 1.4531,  0.6797,  0.4316,  ...,  0.2422, -0.2236,  0.0566],\n",
      "        [ 0.0312,  1.4688, -0.7969,  ...,  2.1562,  0.5430, -0.4121]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -59.5000,  -484.0000,   326.0000,  ...,  1104.0000,\n",
      "            276.0000,  -174.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9344.,  -3008., -24064.,  ...,   -724.,   8640., -27648.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 156, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0801, -0.8398,  0.9648,  ...,  2.3594,  1.0156,  1.5469],\n",
      "        [ 1.4531,  0.6797,  0.4316,  ...,  0.2422, -0.2236,  0.0566],\n",
      "        [ 0.0312,  1.4688, -0.7969,  ...,  2.1562,  0.5430, -0.4121]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 156]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 156, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.0000, -5.4375,  4.6875, -6.8438, -7.0000, -6.9062, -6.8438, -7.0000,\n",
      "         -7.0000, -7.0625, -6.9062, -6.9375, -6.8125, -3.3281,  4.3125, -7.0312,\n",
      "         -6.9688, -6.4688, -7.0000, -6.9062, -6.8125, -6.9688, -7.0000, -6.7812,\n",
      "         -6.9062, -6.9375, -6.8750, -6.9688, -6.8438, -6.9062, -7.0000, -6.9688,\n",
      "         -6.9062, -6.9375, -6.9375, -6.9062,  4.7188,  2.6250,  0.2891, -1.4844,\n",
      "         -2.5938, -3.1719, -2.5625, -1.4141, -0.0698,  1.7578,  1.5781, -1.3203,\n",
      "          6.7188,  1.5156,  5.2500, -0.6133, -3.3906, -2.6250, -3.9531, -2.0000,\n",
      "         -2.5469, -2.1562, -2.1875, -3.0938, -2.7344, -2.7812,  0.7344,  1.2266,\n",
      "         -1.6797, -0.6797, -1.6250,  0.0322, -2.4219, -2.0938, -3.5469, -2.5938,\n",
      "         -0.9141, -1.2266, -2.5000, -2.4844, -3.6875, -2.0156, -2.1875, -2.9062,\n",
      "         -3.2344, -3.5469, -2.4375, -0.4629, -3.3594, -3.1250, -2.9844, -0.4805,\n",
      "         -2.5625, -1.8594, -2.5000, -2.5156, -2.5156, -2.5312, -2.7656, -2.4531,\n",
      "         -2.7188,  2.8438, -2.2812, -2.2031, -1.8516,  0.8789, -3.0625, -2.7656,\n",
      "          1.4844,  2.5469, -2.1875, -0.9727, -2.0156,  1.3906, -2.4844, -2.5469,\n",
      "          0.3965, -1.6328, -0.8984,  2.0938, -2.7656, -1.6719, -2.2500,  3.1719,\n",
      "         -1.4375,  0.4766, -1.3906, -2.3125, -2.2031, -0.9570, -2.1719, -3.3125,\n",
      "         -1.5703, -0.9219, -5.5312, -7.1250, -3.2812, -4.0312, -3.7656, -5.0000,\n",
      "         -4.9688, -4.8750, -5.0000, -6.1562, -2.4531, -3.7188, -4.7812, -4.5312,\n",
      "         -3.6406, -3.8906, -4.1250, -4.2188, -4.4688, -4.9688, -3.8750, -1.2500,\n",
      "         -2.1406, -5.4375, -5.9688, -3.4531, -3.5781, -0.8789, -3.1250, -5.2188,\n",
      "         -3.1406, -4.8438, -6.2188, -3.7812, -4.7188, -4.3125, -4.1875, -5.4062,\n",
      "         -4.8125, -4.3438, -3.8906, -4.5000, -4.5000, -2.7812, -5.0312, -5.6250,\n",
      "         -5.5312, -5.3750, -5.6562, -4.1562, -4.0625, -3.4688, -3.3281, -4.0625,\n",
      "         -5.5000, -5.5000, -5.2188, -4.0625, -4.5000, -5.4688, -5.0000, -4.9375,\n",
      "         -4.2188, -4.7500, -5.1250, -5.1250, -6.9375, -7.0000, -2.8281, -0.8438,\n",
      "         -6.2812, -6.1875, -7.0000, -6.5312, -6.7500, -6.9688, -7.0000, -6.5938,\n",
      "         -6.3125, -7.0312, -4.6875, -3.7188, -6.0625, -6.0625, -6.9062, -6.8125,\n",
      "         -7.0625, -6.9375, -6.9375, -6.9062, -7.0625, -6.9062, -6.9375, -6.9375,\n",
      "         -6.9375, -6.8438, -6.9375, -6.9375, -6.6562, -7.0938, -1.2812, -4.9688,\n",
      "         -5.4688, -5.3438, -5.4062, -6.1875, -6.0312, -6.3750, -6.7188, -6.9688,\n",
      "         -6.7500, -7.0625, -6.8750, -4.9688, -2.7500, -6.9688, -7.0000, -6.8750,\n",
      "         -6.9688, -7.0312, -6.9688, -6.9062, -6.8750, -7.0625, -7.0312, -6.9688,\n",
      "         -7.0312, -7.0312, -6.9375, -7.0312]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 157\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 160, 157])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 157, 256]), Cross output shape: torch.Size([1, 160, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.4531,  0.6797,  0.4316,  ...,  0.2422, -0.2236,  0.0566],\n",
      "        [ 0.0312,  1.4688, -0.7969,  ...,  2.1562,  0.5430, -0.4121],\n",
      "        [-1.7578,  0.5352, -0.2969,  ...,  0.0347, -0.6914, -1.1875]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 20, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -63.0000,  -512.0000,   350.0000,  ...,  1136.0000,\n",
      "            292.0000,  -214.0000],\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 20, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3136., -25344.,  ...,   -824.,   9088., -29184.],\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 157, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.4531,  0.6797,  0.4316,  ...,  0.2422, -0.2236,  0.0566],\n",
      "        [ 0.0312,  1.4688, -0.7969,  ...,  2.1562,  0.5430, -0.4121],\n",
      "        [-1.7578,  0.5352, -0.2969,  ...,  0.0347, -0.6914, -1.1875]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 157]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 157, 160])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.3438, -6.9062,  2.1875, -5.3750, -5.4062, -5.4688, -5.2812, -5.3750,\n",
      "         -5.4688, -5.3750, -5.3750, -5.4375, -5.3750, -2.7500,  3.7969, -5.5312,\n",
      "         -5.3125, -5.2188, -5.4375, -5.3125, -5.3750, -5.3750, -5.3750, -5.3750,\n",
      "         -5.4375, -5.3125, -5.3750, -5.4062, -5.4688, -5.4062, -5.3438, -5.3438,\n",
      "         -5.4062, -5.4062, -5.4375, -5.3750, 10.2500, -2.6094,  0.0522, -1.4844,\n",
      "         -2.6094, -4.5625, -1.1562, -2.2969, -1.4141,  0.0942, -0.7305, -1.4141,\n",
      "          0.9766,  0.4883,  0.5703,  0.6719, -2.1250, -1.0781, -1.5312, -2.1875,\n",
      "         -2.3438, -1.9844, -2.4062, -3.2500, -3.4219, -3.0938,  0.1582, -2.4844,\n",
      "         -0.5742,  0.1035, -0.6289, -2.0156, -4.1250, -1.9141, -2.0156, -0.9062,\n",
      "         -1.8828, -2.7656, -1.5469, -2.6562, -1.9922, -1.2109, -1.5469, -2.7500,\n",
      "         -2.4844, -0.7266, -1.6016, -1.7031, -2.0156, -2.9375, -1.5547, -0.8008,\n",
      "         -1.9062, -2.5781, -0.1211, -1.6172, -2.8594, -2.2969, -2.2656, -2.1094,\n",
      "         -4.2188, -0.3496, -2.7344, -1.6953, -5.7188, -1.3438, -1.9141, -0.5625,\n",
      "         -1.0859, -1.7734, -1.9609, -1.2578, -1.6172, -0.2969, -1.7578, -1.2344,\n",
      "         -1.0859, -0.7617, -0.9492, -1.7500, -1.3828, -3.3906, -1.3359, -0.1699,\n",
      "         -0.7461, -2.3281, -0.9336, -1.3438, -2.0156, -1.4766, -2.7031, -3.4219,\n",
      "         -0.1504, -1.3984, -4.9688, -5.4375, -4.0312, -4.6562, -4.3438, -4.6875,\n",
      "         -4.7500, -4.7812, -4.3438, -5.0625, -3.5312, -4.3750, -4.4062, -4.5625,\n",
      "         -4.0625, -4.2500, -4.7812, -3.8906, -4.1562, -4.6875, -2.9062, -3.4844,\n",
      "         -3.5625, -4.8125, -4.7500, -2.8438, -4.1562, -4.9375, -3.5469, -4.6250,\n",
      "         -3.2812, -2.9688, -5.0000, -4.3125, -4.4375, -4.1875, -4.7188, -4.5625,\n",
      "         -4.3750, -4.3750, -4.0938, -3.9219, -4.4062, -3.2344, -4.4688, -4.8750,\n",
      "         -4.9688, -4.5938, -4.6562, -4.5312, -4.7812, -3.9844, -3.9688, -3.9844,\n",
      "         -4.5938, -4.7500, -4.9062, -4.1562, -4.3750, -4.6250, -4.5938, -4.2188,\n",
      "         -3.7188, -3.6406, -4.4375, -4.7188, -5.3125, -5.4375, -2.1250, -1.8359,\n",
      "         -5.0000, -5.1875, -5.3750, -5.2500, -5.3750, -5.4062, -5.4062, -5.1562,\n",
      "         -5.0000, -5.4375, -4.7812, -4.3750, -4.8438, -4.8750, -5.3125, -5.3750,\n",
      "         -5.5938, -5.4375, -5.3125, -5.4375, -5.4062, -5.4062, -5.3438, -5.4062,\n",
      "         -5.3438, -5.4062, -5.3750, -5.4062, -5.3438, -5.4375, -1.7031, -4.4375,\n",
      "         -4.9375, -4.9062, -4.9062, -5.1875, -5.1562, -5.3438, -5.2812, -5.2188,\n",
      "         -5.1562, -5.4688, -5.3438, -4.4688, -4.1875, -5.4688, -5.5000, -5.3438,\n",
      "         -5.4375, -5.3438, -5.3750, -5.4375, -5.4375, -5.5000, -5.3750, -5.3125,\n",
      "         -5.5312, -5.4375, -5.4688, -5.3750]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 158\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 158])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 158, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0312,  1.4688, -0.7969,  ...,  2.1562,  0.5430, -0.4121],\n",
      "        [-1.7578,  0.5352, -0.2969,  ...,  0.0347, -0.6914, -1.1875],\n",
      "        [-1.0703,  0.9570,  1.8438,  ...,  0.7031, -0.4766, -0.6406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  108.0000,   -97.0000,   -69.5000,  ...,   322.0000,\n",
      "             30.5000,   -28.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  2256.,  -1432.,  -5376.,  ...,     93.,   2304.,  -7456.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 158, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0312,  1.4688, -0.7969,  ...,  2.1562,  0.5430, -0.4121],\n",
      "        [-1.7578,  0.5352, -0.2969,  ...,  0.0347, -0.6914, -1.1875],\n",
      "        [-1.0703,  0.9570,  1.8438,  ...,  0.7031, -0.4766, -0.6406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 158]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 20]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 158, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-11.1875,  -9.8750,  -5.6562, -11.2500, -11.1875, -11.2500, -11.3125,\n",
      "         -11.1875, -11.1875, -11.2500, -11.2500, -11.2500, -11.2500,  -7.5625,\n",
      "          -1.9062, -11.3750, -11.3125, -10.7500, -11.3125, -11.3125, -11.2500,\n",
      "         -11.2500, -11.2500, -11.3125, -11.2500, -11.2500, -11.3750, -11.2500,\n",
      "         -11.3125, -11.2500, -11.3125, -11.2500, -11.3750, -11.2500, -11.3125,\n",
      "         -11.2500,  -4.5000,  -7.2500,  -2.1875,  -4.3750,  -6.4062,  -7.5000,\n",
      "          -5.7188,  -5.7812,  -3.4531,  -6.6875,  -5.1562,  -5.4688,  -3.7812,\n",
      "          -2.7969,  -3.9219,  -3.5781,  -5.6875,  -3.8750,  -4.9062,  -4.8750,\n",
      "          -5.6250,  -5.7500,  -6.3750,  -6.3125,  -5.5625,  -5.7188,  -7.0938,\n",
      "          -4.4062,  -6.2500,  -6.5000,  -6.7500,  -9.3750,  -7.5000,  -4.0000,\n",
      "          -4.6250,  -4.7500,  -5.9062,  -4.1875,  -4.5312,  -3.9844,  -4.3750,\n",
      "          -2.5469,  -2.5938,  -4.6250,  -4.6250,  -3.9688,  -5.3750,  -5.6562,\n",
      "          -2.9375,  -6.4062,  -4.2812,  -3.6094,  -4.5938,  -4.4062,  -6.3438,\n",
      "          -5.5000,  -6.2500,  -6.0625,  -5.9688,  -4.6875,  -8.3750,  -5.4062,\n",
      "          -7.2500,  -4.7188,  -5.6562,   3.2656,   2.2031,   1.7344,   1.0000,\n",
      "           0.8320,   1.7188,   1.6094,   0.4414,   0.9805,   1.2578,   0.1641,\n",
      "           0.4453,   1.5859,  -0.7422,   1.0781,   3.2812,  -0.9570,   1.2812,\n",
      "           1.8203,   2.0156,   0.1553,  -0.1338,   0.8711,  -5.0312,   0.5312,\n",
      "          -3.2969,  -6.0625,  -6.2188,  -5.3438,  -9.9375, -11.2500,  -7.4688,\n",
      "          -9.1875,  -9.0000,  -9.8750,  -9.8750,  -9.5625,  -9.1875, -10.7500,\n",
      "          -7.9062,  -8.5625,  -9.5625,  -9.0000,  -9.0625,  -9.7500,  -9.8125,\n",
      "          -9.5625,  -9.7500,  -9.9375,  -7.9375,  -7.2188,  -8.2500, -10.3750,\n",
      "         -10.3750,  -9.1875,  -8.5625,  -9.6875,  -7.2188, -10.3125,  -7.9688,\n",
      "          -7.7812, -10.5625,  -8.8125,  -9.0625,  -9.4375,  -9.1875, -10.1250,\n",
      "          -9.6250,  -9.3125,  -8.4375,  -9.7500,  -9.7500,  -7.4375,  -9.7500,\n",
      "         -10.1875, -10.4375, -10.4375, -10.1875,  -9.8750,  -7.5625,  -8.2500,\n",
      "          -9.1250,  -8.9375, -10.0000, -10.1250, -10.3125,  -9.4375,  -9.3125,\n",
      "         -10.1875, -10.0000,  -9.7500,  -8.6875,  -8.7500,  -9.9375, -10.0000,\n",
      "         -11.3125, -11.1875,  -7.0938,  -4.7188, -10.5000, -10.6250, -11.1875,\n",
      "         -10.8750, -11.1250, -10.7500, -11.2500, -11.1250, -10.7500, -11.2500,\n",
      "          -9.6875,  -9.3125, -10.5000, -10.2500, -11.1875, -11.3125, -11.3125,\n",
      "         -11.2500, -11.2500, -11.3125, -11.2500, -11.2500, -11.2500, -11.2500,\n",
      "         -11.1875, -11.3125, -11.3125, -11.2500, -11.0625, -11.2500,  -4.5312,\n",
      "          -9.8750, -10.4375, -10.0000, -10.1250, -10.3750, -10.6250, -10.9375,\n",
      "         -11.0000, -11.1875, -11.1250, -11.2500, -11.2500,  -9.6875,  -7.4062,\n",
      "         -11.3125, -11.3125, -11.2500, -11.2500, -11.3125, -11.2500, -11.3750,\n",
      "         -11.2500, -11.3125, -11.2500, -11.3125, -11.2500, -11.3125, -11.3125,\n",
      "         -11.2500]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 159\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 159])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 159, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.7578,  0.5352, -0.2969,  ...,  0.0347, -0.6914, -1.1875],\n",
      "        [-1.0703,  0.9570,  1.8438,  ...,  0.7031, -0.4766, -0.6406],\n",
      "        [-0.3320, -0.6172,  3.3125,  ...,  1.0625,  1.4453,  0.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -13.6250,  -243.0000,    90.5000,  ...,   816.0000,\n",
      "            131.0000,   -47.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  5472.,  -1976., -13760.,  ...,   -260.,   5152., -16320.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 159, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.7578,  0.5352, -0.2969,  ...,  0.0347, -0.6914, -1.1875],\n",
      "        [-1.0703,  0.9570,  1.8438,  ...,  0.7031, -0.4766, -0.6406],\n",
      "        [-0.3320, -0.6172,  3.3125,  ...,  1.0625,  1.4453,  0.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 159]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 20]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 159, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.2188, -8.8750, -4.0938, -7.2188, -7.1875, -7.1250, -7.2188, -7.2500,\n",
      "         -7.1562, -7.1250, -7.1875, -7.1562, -7.1562, -5.2188, -3.7500, -7.1875,\n",
      "         -7.1250, -6.9062, -7.1562, -7.1250, -7.1562, -7.1875, -6.9688, -7.2188,\n",
      "         -7.1250, -7.0625, -7.1562, -7.1562, -7.1875, -7.0938, -7.1875, -7.0625,\n",
      "         -7.0938, -7.0938, -7.0625, -7.1562, -2.2500, -5.2812, -4.0625, -4.3125,\n",
      "         -4.9062, -5.0312, -4.4688, -3.6406, -3.4531, -3.9219, -3.9062, -5.2500,\n",
      "         -1.9219, -2.8438, -1.6875, -3.9375, -4.5625, -4.9062, -4.5938, -4.1562,\n",
      "         -4.6250, -5.8438, -4.3125, -5.6875, -4.9375, -5.8438, -4.0625, -3.1562,\n",
      "         -5.6250, -5.0312, -3.6562, -5.8125, -5.2188, -4.6562, -6.0938, -5.0625,\n",
      "         -5.0625, -1.8594, -5.3750, -4.5938, -3.6719, -4.0625, -5.2812, -4.9688,\n",
      "         -4.4062, -4.9062, -4.7500, -4.7188, -4.5625, -6.4688, -4.6875, -4.7812,\n",
      "         -4.2500, -3.8281, -5.2500, -4.9375, -5.0938, -5.0000, -4.6875, -4.2500,\n",
      "         -5.4375, -5.2188, -5.7188, -4.2500, -6.5625,  3.8906, -1.6797, -1.0000,\n",
      "         -1.1172,  8.6875, -2.3125, -1.5625,  2.7812,  4.3750, -3.5625, -1.7188,\n",
      "          2.4219, -1.1719, -0.7773,  4.1875, -0.6250, -3.1094,  3.1719,  0.3164,\n",
      "         -0.6016,  4.4375, -1.7266, -1.5625, -4.0000,  1.3828, -1.8281, -4.2188,\n",
      "         -4.9688, -4.2188, -6.5312, -7.1562, -4.5312, -5.8750, -5.8438, -6.4062,\n",
      "         -6.5312, -6.4062, -6.2812, -6.7500, -5.8438, -5.8438, -6.0312, -5.8438,\n",
      "         -5.8750, -6.0000, -6.2188, -6.3125, -6.1562, -6.2188, -5.5938, -6.4375,\n",
      "         -5.6875, -6.5312, -6.6562, -6.0938, -6.0000, -5.3438, -4.8438, -6.6875,\n",
      "         -5.6875, -4.7812, -6.6875, -5.9062, -5.9375, -6.3750, -6.0938, -6.5938,\n",
      "         -6.4375, -5.9062, -5.3125, -6.5938, -6.1562, -5.7188, -6.4062, -6.5625,\n",
      "         -6.6250, -6.5625, -6.4688, -6.4375, -5.2812, -5.4688, -5.2188, -5.6875,\n",
      "         -6.4375, -6.4375, -6.5625, -6.0938, -6.1250, -6.5938, -6.5000, -6.5000,\n",
      "         -5.7500, -5.8125, -6.2500, -6.2500, -7.1250, -7.0000, -5.2188, -0.6680,\n",
      "         -6.8125, -6.9062, -7.1250, -7.0000, -7.0625, -6.9688, -7.2188, -7.1875,\n",
      "         -6.8125, -7.1875, -6.5312, -5.6562, -6.7500, -6.5625, -7.1250, -7.1562,\n",
      "         -7.2812, -7.1562, -7.1875, -7.2812, -7.1250, -7.1875, -7.1875, -7.1250,\n",
      "         -7.1562, -7.2500, -7.1562, -7.1562, -7.0625, -7.1875, -4.4062, -6.4375,\n",
      "         -6.6250, -6.3125, -6.5625, -6.6562, -6.6875, -7.0000, -6.9375, -7.1562,\n",
      "         -7.0000, -7.1875, -7.1562, -6.1250, -4.5312, -7.1562, -7.2188, -7.1250,\n",
      "         -7.2188, -7.2500, -7.0938, -7.1250, -7.0938, -7.1562, -7.1875, -7.2188,\n",
      "         -7.1250, -7.0938, -7.2188, -7.1250]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 160\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 160])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 160, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0703,  0.9570,  1.8438,  ...,  0.7031, -0.4766, -0.6406],\n",
      "        [-0.3320, -0.6172,  3.3125,  ...,  1.0625,  1.4453,  0.1406],\n",
      "        [-0.4199,  0.6289,  0.4062,  ...,  0.9102, -1.0234, -1.0469]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -53.0000,  -316.0000,   164.0000,  ...,   888.0000,\n",
      "            165.0000,  -104.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  6720.,  -2272., -17024.,  ...,   -384.,   6208., -19968.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 160, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0703,  0.9570,  1.8438,  ...,  0.7031, -0.4766, -0.6406],\n",
      "        [-0.3320, -0.6172,  3.3125,  ...,  1.0625,  1.4453,  0.1406],\n",
      "        [-0.4199,  0.6289,  0.4062,  ...,  0.9102, -1.0234, -1.0469]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 160]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 20]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 160, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.2500, -11.3750,  -5.3125, -10.2500, -10.3125, -10.1875, -10.2500,\n",
      "         -10.2500, -10.3125, -10.3125, -10.3750, -10.3125, -10.2500,  -6.4688,\n",
      "          -3.5781, -10.3125, -10.4375,  -9.9375, -10.3125, -10.2500, -10.2500,\n",
      "         -10.3125, -10.1875, -10.2500, -10.1875, -10.3125, -10.1875, -10.2500,\n",
      "         -10.3125, -10.3125, -10.3125, -10.2500, -10.1875, -10.2500, -10.1875,\n",
      "         -10.4375,  -1.3047,  -5.1875,  -2.8906,  -6.4688,  -6.6562,  -5.3438,\n",
      "          -5.5625,  -3.4219,  -4.9688,  -4.5312,  -5.7500,  -5.6562,  -1.1562,\n",
      "          -1.7109,  -0.9180,  -3.3438,  -4.3750,  -4.9062,  -5.0312,  -4.9062,\n",
      "          -4.5000,  -5.9375,  -5.5000,  -4.5312,  -6.5000,  -5.8750,  -3.6719,\n",
      "          -3.5469,  -6.0625,  -5.0938,  -4.5000,  -5.0938,  -7.0312,  -5.0312,\n",
      "          -5.9375,  -4.8125,  -6.2500,  -5.2500,  -6.0625,  -4.7188,  -5.6875,\n",
      "          -5.5000,  -5.2188,  -6.0938,  -6.0000,  -4.7188,  -2.6719,  -5.7188,\n",
      "          -4.1875,  -5.2812,  -5.6875,  -3.8125,  -4.2812,  -6.0312,  -6.2500,\n",
      "          -4.9688,  -4.5312,  -6.3438,  -5.9062,  -6.1562,  -7.2188,  -6.3125,\n",
      "          -5.5312,  -5.5938,  -6.1562,   4.1875,  -0.7578,   1.6094,   1.0312,\n",
      "           3.5469,  -2.0781,   0.2441,  -1.7266,  -1.0078,  -2.7656,  -0.8477,\n",
      "           2.0469,   0.0505,   8.8750,   1.2422,   3.3281,  -1.0156,   3.8906,\n",
      "           1.6250,   3.2344,  -0.8359,  -0.3418,  -0.8281,  -0.3379,  -1.9375,\n",
      "          -1.5625,  -4.3750,  -4.9062,  -4.6250,  -9.4375, -10.3125,  -6.0938,\n",
      "          -7.5625,  -7.1875,  -8.7500,  -8.9375,  -9.1250,  -8.2500,  -9.2500,\n",
      "          -6.4375,  -6.9375,  -8.1875,  -7.7500,  -7.4375,  -7.8125,  -8.0625,\n",
      "          -7.8438,  -8.3125,  -8.7500,  -6.8125,  -6.3125,  -7.2188,  -8.5625,\n",
      "          -9.0625,  -7.5938,  -7.4375,  -6.6250,  -6.7812,  -8.8125,  -6.9375,\n",
      "          -8.0625,  -9.3750,  -7.3750,  -7.5312,  -8.1875,  -8.0625,  -8.7500,\n",
      "          -8.2500,  -8.0625,  -7.1562,  -8.6250,  -8.0625,  -6.7812,  -8.4375,\n",
      "          -9.1250,  -9.2500,  -8.8750,  -8.6250,  -8.1875,  -8.1250,  -7.2500,\n",
      "          -6.4375,  -7.6250,  -8.6250,  -8.8125,  -9.1875,  -7.9375,  -7.4688,\n",
      "          -9.1250,  -8.6875,  -8.4375,  -7.7500,  -8.0000,  -8.5625,  -8.6250,\n",
      "         -10.2500, -10.2500,  -6.5625,  -3.1250,  -9.4375,  -9.4375, -10.1875,\n",
      "         -10.0000, -10.2500, -10.1875, -10.2500, -10.1250,  -9.5625, -10.3125,\n",
      "          -8.9375,  -7.7500,  -9.6250,  -9.6875, -10.3125, -10.3125, -10.5000,\n",
      "         -10.2500, -10.3125, -10.3125, -10.1250, -10.3125, -10.2500, -10.2500,\n",
      "         -10.2500, -10.2500, -10.3750, -10.4375, -10.0000, -10.3750,  -4.7812,\n",
      "          -8.5000,  -9.1250,  -8.6875,  -9.0625,  -9.1875,  -9.3125,  -9.6875,\n",
      "         -10.1250, -10.2500, -10.1875, -10.3125, -10.2500,  -8.1875,  -6.1562,\n",
      "         -10.3750, -10.3125, -10.1875, -10.2500, -10.3125, -10.3750, -10.1875,\n",
      "         -10.1875, -10.3125, -10.3125, -10.3125, -10.2500, -10.3750, -10.2500,\n",
      "         -10.2500]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 161\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 161])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 161, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3320, -0.6172,  3.3125,  ...,  1.0625,  1.4453,  0.1406],\n",
      "        [-0.4199,  0.6289,  0.4062,  ...,  0.9102, -1.0234, -1.0469],\n",
      "        [ 2.4219, -0.0859, -1.5000,  ...,  1.1484, -1.0469, -1.4062]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -49.5000,  -372.0000,   236.0000,  ...,   960.0000,\n",
      "            203.0000,  -127.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  7584.,  -2528., -19328.,  ...,   -480.,   7008., -22400.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 161, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.3320, -0.6172,  3.3125,  ...,  1.0625,  1.4453,  0.1406],\n",
      "        [-0.4199,  0.6289,  0.4062,  ...,  0.9102, -1.0234, -1.0469],\n",
      "        [ 2.4219, -0.0859, -1.5000,  ...,  1.1484, -1.0469, -1.4062]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 161]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 20]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 161, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.9375, -7.7188,  1.2422, -9.0000, -8.9375, -8.9375, -8.9375, -9.0000,\n",
      "         -8.9375, -8.9375, -9.0625, -9.0625, -9.0000, -5.2812,  2.0938, -8.9375,\n",
      "         -9.0000, -8.6875, -9.0000, -9.0000, -8.8750, -8.9375, -8.9375, -8.9375,\n",
      "         -8.8750, -8.9375, -8.8750, -8.8750, -8.8750, -9.0000, -9.0000, -9.0000,\n",
      "         -8.9375, -8.8750, -8.9375, -8.9375,  4.5938,  0.0139, -1.4297, -3.2969,\n",
      "         -4.0625, -4.1562, -4.0625, -1.3516, -1.9453, -1.7422, -0.1240, -3.8750,\n",
      "          4.0312,  0.2734,  3.2344, -1.3672, -2.0469, -2.8594, -3.8438, -3.3281,\n",
      "         -1.9375, -2.6250, -3.4219, -2.6250, -4.6562, -1.6406, -0.2334, -0.9297,\n",
      "         -1.2578, -2.1250, -4.5312, -1.4141, -4.5625, -2.8281, -3.3750,  0.2383,\n",
      "         -3.8906, -4.4062, -3.8594, -2.3594, -3.8594, -0.9570, -2.4844, -3.7188,\n",
      "         -5.4375, -4.4062, -4.0000, -2.7344, -4.9062, -2.3281, -3.2344, -3.3750,\n",
      "         -2.7344, -2.8594, -3.1094, -3.9219, -3.6094, -4.2188, -4.0625, -3.6562,\n",
      "         -4.5000, -2.5000, -1.2266, -3.1562, -2.9688, -0.3652, -2.5469,  8.0625,\n",
      "          0.8789,  0.7930, -0.9570,  2.2188, -0.5703,  1.2969,  0.9727,  0.3555,\n",
      "         -2.7969, -2.0938,  2.1406, -1.3438, -2.7812,  0.0596, -0.7891,  1.4688,\n",
      "          1.5859, -1.1875, -1.5469, -2.0312, -2.2500, -0.8555,  1.4766, -2.0000,\n",
      "         -2.6562, -2.4688, -7.6250, -9.0000, -4.9688, -5.3125, -5.3125, -6.9062,\n",
      "         -6.5000, -6.9062, -6.2500, -8.0625, -4.8438, -4.3438, -5.8750, -6.0312,\n",
      "         -4.9062, -5.6562, -5.9688, -5.3438, -6.2500, -6.8125, -4.7500, -2.3594,\n",
      "         -4.2812, -6.9062, -7.7500, -5.2500, -4.2812, -3.5000, -4.5625, -6.9375,\n",
      "         -3.7500, -4.3750, -7.8438, -5.9688, -6.0938, -5.9062, -6.3125, -6.7500,\n",
      "         -6.2500, -5.8125, -5.2188, -6.0625, -6.2812, -5.2188, -6.5000, -7.5312,\n",
      "         -7.5000, -7.1562, -7.4375, -6.1250, -5.7812, -4.7500, -4.2812, -5.4375,\n",
      "         -7.3750, -7.2500, -7.5312, -5.9375, -5.4062, -7.2812, -7.1562, -6.8125,\n",
      "         -4.8125, -5.1562, -6.8750, -7.1250, -8.8750, -8.9375, -2.5625, -2.1094,\n",
      "         -8.0625, -8.3750, -8.8750, -8.3750, -8.7500, -8.7500, -8.9375, -8.5625,\n",
      "         -7.8125, -9.0625, -6.9062, -5.2812, -7.9375, -8.0000, -8.9375, -8.9375,\n",
      "         -9.0625, -9.0000, -9.0625, -8.9375, -9.0000, -9.0000, -8.8750, -8.8750,\n",
      "         -9.0000, -9.0625, -8.7500, -8.8750, -8.6250, -8.9375, -2.2188, -6.2188,\n",
      "         -7.4688, -6.7812, -7.2812, -7.4375, -8.0625, -8.1250, -8.6250, -8.8125,\n",
      "         -8.8750, -8.9375, -8.8750, -5.7500, -3.3750, -8.9375, -8.9375, -9.0000,\n",
      "         -9.0000, -9.0000, -8.9375, -9.0000, -8.8750, -9.0000, -8.8750, -9.0625,\n",
      "         -9.0000, -9.0000, -8.8750, -9.0000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 162\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 162])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 162, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.4199,  0.6289,  0.4062,  ...,  0.9102, -1.0234, -1.0469],\n",
      "        [ 2.4219, -0.0859, -1.5000,  ...,  1.1484, -1.0469, -1.4062],\n",
      "        [-0.1133, -0.9297,  1.0312,  ...,  2.3750,  1.0469,  1.5703]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -42.0000,  -444.0000,   282.0000,  ...,  1040.0000,\n",
      "            245.0000,  -142.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  8576.,  -2832., -22016.,  ...,   -588.,   7936., -25472.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 162, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.4199,  0.6289,  0.4062,  ...,  0.9102, -1.0234, -1.0469],\n",
      "        [ 2.4219, -0.0859, -1.5000,  ...,  1.1484, -1.0469, -1.4062],\n",
      "        [-0.1133, -0.9297,  1.0312,  ...,  2.3750,  1.0469,  1.5703]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 162]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 20]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 162, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.8125, -6.6875, -3.2656, -8.8125, -8.8125, -8.8750, -8.8125, -8.8125,\n",
      "         -8.8125, -8.8125, -8.9375, -8.9375, -9.0000, -5.2500, -3.6250, -8.8750,\n",
      "         -8.8125, -8.6250, -8.8125, -8.8125, -8.7500, -8.7500, -8.8125, -8.8125,\n",
      "         -8.8125, -8.8750, -8.8125, -8.7500, -8.8125, -8.9375, -8.9375, -8.8125,\n",
      "         -8.8125, -8.8125, -8.8125, -8.7500, -0.1660, -3.2188, -3.4844, -3.3594,\n",
      "         -3.7812, -4.7188, -5.7188, -2.9531, -3.8906, -4.5312, -4.5312, -3.6875,\n",
      "         -1.0781, -1.8750, -0.5312, -2.0938, -4.5000, -3.0781, -3.3125, -3.5781,\n",
      "         -2.9531, -4.5312, -4.2188, -3.5469, -3.5469, -4.3438, -1.9375, -4.4688,\n",
      "         -4.0312, -3.6719, -4.4062, -3.7500, -5.6875, -3.8438, -4.3750, -4.7188,\n",
      "         -4.0312, -3.6250, -4.2188, -4.7500, -3.6875, -0.4707, -3.4062, -4.9688,\n",
      "         -3.5156, -3.9531, -4.6562, -2.4688, -3.7188, -7.3125, -3.7031, -4.4688,\n",
      "         -3.5938, -4.5938, -4.9062, -5.4688, -5.4375, -4.1562, -5.1875, -4.9062,\n",
      "         -6.1250, -6.3750, -5.8750, -5.2500, -5.1250,  2.3594, -1.7891, -1.7109,\n",
      "         -2.0156,  4.2500, -1.2656, -2.6719,  1.3359, 11.0000, -3.4531, -1.1953,\n",
      "          2.5781, -1.8438, -2.2812,  3.1562, -1.6641, -4.7188,  1.5703, -0.0236,\n",
      "          0.3418,  1.3984, -3.0156, -2.6875, -5.6250,  3.4688, -1.9766, -4.5625,\n",
      "         -3.4688, -4.8438, -7.7500, -8.8750, -4.3438, -6.5000, -6.1875, -7.5625,\n",
      "         -7.1562, -7.3750, -7.2188, -8.2500, -6.4688, -5.8125, -6.9688, -6.5938,\n",
      "         -6.3438, -6.6250, -7.3750, -6.4062, -7.0938, -6.9062, -6.4688, -5.9688,\n",
      "         -5.8438, -7.8438, -7.8750, -7.0000, -6.3750, -5.3438, -6.8438, -7.5312,\n",
      "         -5.4375, -6.0625, -7.9375, -6.3750, -6.9688, -6.5000, -7.0312, -7.4062,\n",
      "         -7.1250, -6.4062, -6.9688, -6.9688, -7.0312, -5.6875, -7.1562, -7.8438,\n",
      "         -7.7188, -7.5938, -7.7500, -6.8125, -6.5000, -5.4375, -6.4688, -6.5625,\n",
      "         -7.8750, -7.6562, -7.7188, -6.8750, -6.4062, -7.7500, -7.5625, -7.2188,\n",
      "         -5.9375, -6.6875, -7.3750, -7.4375, -8.7500, -8.8750, -5.1562, -1.9219,\n",
      "         -8.3125, -8.2500, -8.7500, -8.5000, -8.8125, -8.2500, -8.8125, -8.4375,\n",
      "         -8.1875, -8.8125, -7.4688, -6.8750, -8.3125, -8.2500, -8.6875, -8.8125,\n",
      "         -8.8750, -8.9375, -8.8750, -8.9375, -8.8125, -8.8125, -8.8125, -8.8750,\n",
      "         -8.7500, -8.9375, -8.8125, -8.8750, -8.7500, -8.9375, -4.1250, -7.2188,\n",
      "         -7.7500, -7.7188, -7.4688, -8.0625, -8.1250, -8.5000, -8.6250, -8.6875,\n",
      "         -8.6875, -8.7500, -8.8125, -7.2500, -6.1562, -8.8750, -8.9375, -8.8125,\n",
      "         -8.8125, -8.8750, -8.8125, -8.9375, -8.8750, -8.9375, -8.8125, -8.8750,\n",
      "         -8.9375, -9.0000, -8.7500, -8.8750]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 163\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  6]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 163])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 163, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4219, -0.0859, -1.5000,  ...,  1.1484, -1.0469, -1.4062],\n",
      "        [-0.1133, -0.9297,  1.0312,  ...,  2.3750,  1.0469,  1.5703],\n",
      "        [ 1.4531,  0.6172,  0.5273,  ...,  0.3047, -0.2236,  0.0747]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -47.5000,  -460.0000,   302.0000,  ...,  1072.0000,\n",
      "            264.0000,  -158.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9088.,  -2928., -23296.,  ...,   -668.,   8320., -26752.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 163, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.4219, -0.0859, -1.5000,  ...,  1.1484, -1.0469, -1.4062],\n",
      "        [-0.1133, -0.9297,  1.0312,  ...,  2.3750,  1.0469,  1.5703],\n",
      "        [ 1.4531,  0.6172,  0.5273,  ...,  0.3047, -0.2236,  0.0747]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 163]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         20]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 163, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.4062, -6.6250, -0.7930, -5.3438, -5.5000, -5.5000, -5.5000, -5.4688,\n",
      "         -5.3438, -5.4375, -5.5938, -5.5625, -5.4375, -4.1250,  0.4902, -5.5625,\n",
      "         -5.5312, -5.1250, -5.4375, -5.4688, -5.4062, -5.5312, -5.4688, -5.3750,\n",
      "         -5.4062, -5.4375, -5.4375, -5.4062, -5.4062, -5.5000, -5.4688, -5.3750,\n",
      "         -5.4062, -5.4375, -5.4688, -5.4062,  0.7031, -1.4219, -2.1562, -1.7578,\n",
      "         -2.0938, -3.7188, -3.9219, -2.0781, -1.6172, -3.1562, -2.6094, -2.0781,\n",
      "          0.2930,  0.4961,  0.4668, -1.3516, -5.2500, -3.3594, -4.0625, -4.0000,\n",
      "         -2.7031, -3.7188, -2.1875, -2.0000, -3.0938, -3.1250, -1.0469, -1.6328,\n",
      "         -3.5781, -3.9375, -2.6250, -2.1406, -2.9219, -4.1875, -3.2188, -3.5781,\n",
      "         -2.1094, -3.2969, -2.9219, -4.0000, -4.9062, -1.9766, -1.9688, -4.5938,\n",
      "          1.7578, -4.0938, -3.2500, -1.6406, -0.6914, -4.1562, -2.4688, -3.0156,\n",
      "         -2.3750, -4.5312, -3.8906, -2.9062, -2.9062, -2.7812, -3.6875, -3.4219,\n",
      "         -3.4219, -3.5469, -4.8750, -2.2031, -2.6562,  0.4648,  0.1279,  0.9023,\n",
      "          2.3906,  0.4434,  1.7109, -2.5312, -3.2344, -0.8867, -1.9688, -1.6250,\n",
      "         11.3125,  0.6523,  3.6250,  0.7969,  3.0000, -2.0938,  2.2031,  1.8828,\n",
      "         -0.8125, -0.6094, -0.3027, -2.4531, -1.4062, -0.8750, -1.2734, -4.0625,\n",
      "         -2.6562, -3.4688, -4.7500, -5.5938, -4.0625, -3.8906, -3.7188, -4.8750,\n",
      "         -4.4062, -4.8438, -3.9688, -5.0312, -3.4062, -4.2812, -4.3125, -4.3438,\n",
      "         -5.0625, -4.7188, -4.7500, -4.1562, -4.5312, -4.4375, -4.5625, -3.6250,\n",
      "         -3.9219, -4.7812, -4.8438, -4.7188, -4.0625, -3.1250, -4.6562, -4.7188,\n",
      "         -3.5312, -6.1250, -5.0938, -5.3438, -4.5312, -4.5625, -4.9688, -5.0312,\n",
      "         -4.7188, -4.3125, -4.4688, -4.7500, -4.8125, -4.5625, -4.6562, -4.9062,\n",
      "         -4.8125, -5.0312, -4.7500, -3.9844, -3.8438, -4.2500, -4.0938, -4.8438,\n",
      "         -4.8438, -4.8750, -4.6875, -4.0000, -4.8750, -4.9062, -4.7812, -4.4688,\n",
      "         -4.7812, -5.2188, -4.7812, -4.8750, -5.4062, -5.4062, -2.9844, -2.3438,\n",
      "         -5.0625, -5.0312, -5.4688, -5.1875, -5.3438, -5.4375, -5.4375, -5.3438,\n",
      "         -5.0625, -5.5000, -4.5312, -4.5000, -4.9688, -5.0000, -5.4375, -5.3438,\n",
      "         -5.5938, -5.3438, -5.4375, -5.4062, -5.3750, -5.4375, -5.4375, -5.4375,\n",
      "         -5.4062, -5.4375, -5.3750, -5.5312, -5.2500, -5.5000, -2.9688, -4.5312,\n",
      "         -4.7188, -4.7500, -4.9062, -4.8750, -5.0000, -5.3750, -5.2188, -5.3438,\n",
      "         -5.2500, -5.4375, -5.3750, -4.8125, -4.1250, -5.4688, -5.5625, -5.3438,\n",
      "         -5.4062, -5.4375, -5.4688, -5.3750, -5.4375, -5.5625, -5.5000, -5.4375,\n",
      "         -5.4688, -5.5938, -5.3438, -5.4062]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 164\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  7]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 164])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 164, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1133, -0.9297,  1.0312,  ...,  2.3750,  1.0469,  1.5703],\n",
      "        [ 1.4531,  0.6172,  0.5273,  ...,  0.3047, -0.2236,  0.0747],\n",
      "        [ 0.0078,  1.4062, -0.6914,  ...,  2.2188,  0.5391, -0.3574]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -59.5000,  -488.0000,   324.0000,  ...,  1104.0000,\n",
      "            280.0000,  -174.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9408.,  -2976., -24192.,  ...,   -728.,   8640., -27648.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 164, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.1133, -0.9297,  1.0312,  ...,  2.3750,  1.0469,  1.5703],\n",
      "        [ 1.4531,  0.6172,  0.5273,  ...,  0.3047, -0.2236,  0.0747],\n",
      "        [ 0.0078,  1.4062, -0.6914,  ...,  2.2188,  0.5391, -0.3574]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 164]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 20]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 164, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.1250, -5.6250,  4.3125, -6.9688, -7.0938, -7.0312, -6.9688, -7.1250,\n",
      "         -7.1250, -7.1562, -7.0312, -7.0625, -6.9375, -3.4844,  3.8125, -7.1250,\n",
      "         -7.0938, -6.5938, -7.1250, -7.0000, -6.9375, -7.0938, -7.1250, -6.9062,\n",
      "         -7.0312, -7.0625, -7.0000, -7.0625, -6.9375, -7.0312, -7.1250, -7.0625,\n",
      "         -7.0000, -7.0625, -7.0312, -7.0312,  4.8125,  2.3906,  0.3047, -1.7188,\n",
      "         -2.7031, -3.3125, -2.6406, -1.5469, -0.1533,  1.7578,  1.5703, -1.3047,\n",
      "          6.7188,  1.5078,  5.3125, -0.6133, -3.5469, -2.7969, -4.1250, -2.1250,\n",
      "         -2.6406, -2.3594, -2.3125, -3.1719, -2.8906, -2.8906,  0.5977,  1.0703,\n",
      "         -1.7266, -0.7891, -1.7812, -0.1807, -2.5000, -2.0781, -3.5000, -2.6250,\n",
      "         -0.9023, -1.3047, -2.5938, -2.4844, -3.7656, -2.0781, -2.2656, -3.0156,\n",
      "         -3.3125, -3.5469, -2.5156, -0.5312, -3.2656, -3.3125, -3.0938, -0.4531,\n",
      "         -2.6094, -2.0312, -2.6406, -2.5781, -2.7344, -2.6250, -2.9531, -2.4844,\n",
      "         -2.7812,  2.7344, -2.3438, -2.2500, -1.7891,  0.9219, -3.0781, -2.7812,\n",
      "          1.4766,  2.5312, -2.4531, -0.9453, -2.0781,  1.5781, -2.7344, -2.7031,\n",
      "          0.3984, -1.5312, -0.9180,  2.2188, -2.6875, -1.8281, -2.3125,  3.0938,\n",
      "         -1.4844,  0.4648, -1.5938, -2.4219, -2.3906, -0.8789, -2.4062, -3.3750,\n",
      "         -1.5938, -0.9883, -5.6250, -7.2188, -3.4062, -4.1562, -3.8750, -5.1250,\n",
      "         -5.0938, -4.9688, -5.0938, -6.3438, -2.5781, -3.8750, -4.8750, -4.6562,\n",
      "         -3.7500, -4.0000, -4.2812, -4.3438, -4.5938, -5.0938, -3.9688, -1.4375,\n",
      "         -2.2969, -5.5625, -6.0938, -3.6875, -3.7188, -0.9336, -3.2656, -5.3438,\n",
      "         -3.4062, -4.9688, -6.3438, -3.8281, -4.8438, -4.4375, -4.3438, -5.5312,\n",
      "         -4.9375, -4.4688, -3.9688, -4.6250, -4.5625, -2.8750, -5.1562, -5.7500,\n",
      "         -5.6562, -5.4688, -5.7812, -4.2812, -4.2500, -3.5469, -3.4531, -4.1562,\n",
      "         -5.6250, -5.6250, -5.3125, -4.2500, -4.6250, -5.5938, -5.1250, -5.0312,\n",
      "         -4.3438, -4.8750, -5.2500, -5.2500, -7.0312, -7.1250, -2.9688, -1.1094,\n",
      "         -6.4062, -6.3125, -7.1250, -6.6562, -6.8438, -7.0938, -7.0938, -6.7188,\n",
      "         -6.4375, -7.1562, -4.8438, -3.8438, -6.1875, -6.1875, -7.0000, -6.9375,\n",
      "         -7.1875, -7.0625, -7.0625, -7.0312, -7.1875, -7.0312, -7.0625, -7.0625,\n",
      "         -7.0312, -6.9688, -7.0625, -7.0625, -6.7812, -7.2188, -1.3438, -5.1250,\n",
      "         -5.6250, -5.4688, -5.5625, -6.3125, -6.1875, -6.5000, -6.8125, -7.0938,\n",
      "         -6.8438, -7.1875, -7.0000, -5.0938, -2.8906, -7.0938, -7.1250, -6.9688,\n",
      "         -7.0625, -7.1250, -7.0938, -7.0312, -7.0000, -7.1875, -7.1250, -7.0938,\n",
      "         -7.1562, -7.1250, -7.0312, -7.1250]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 165\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 168, 165])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 165, 256]), Cross output shape: torch.Size([1, 168, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.4531,  0.6172,  0.5273,  ...,  0.3047, -0.2236,  0.0747],\n",
      "        [ 0.0078,  1.4062, -0.6914,  ...,  2.2188,  0.5391, -0.3574],\n",
      "        [-1.7266,  0.5234, -0.2617,  ...,  0.0869, -0.6641, -1.1875]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   350.0000,  ...,  1136.0000,\n",
      "            288.0000,  -210.0000],\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 21, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3088., -25344.,  ...,   -828.,   9088., -29056.],\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 165, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.4531,  0.6172,  0.5273,  ...,  0.3047, -0.2236,  0.0747],\n",
      "        [ 0.0078,  1.4062, -0.6914,  ...,  2.2188,  0.5391, -0.3574],\n",
      "        [-1.7266,  0.5234, -0.2617,  ...,  0.0869, -0.6641, -1.1875]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 165]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 165, 168])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.5312, -7.0938,  1.9375, -5.5625, -5.5625, -5.6250, -5.4375, -5.5312,\n",
      "         -5.6250, -5.5312, -5.5625, -5.6250, -5.5625, -2.8281,  3.0938, -5.6875,\n",
      "         -5.5000, -5.3750, -5.5938, -5.4688, -5.5625, -5.5312, -5.5312, -5.5312,\n",
      "         -5.5938, -5.4688, -5.5625, -5.5625, -5.6250, -5.5625, -5.5312, -5.5312,\n",
      "         -5.5625, -5.5938, -5.5938, -5.5625, 10.2500, -2.7344,  0.3203, -1.5781,\n",
      "         -2.6875, -4.5000, -1.2656, -2.1250, -1.5469,  0.0156, -0.8672, -1.3906,\n",
      "          1.0547,  0.3945,  0.2891,  0.4805, -2.0625, -0.9727, -1.4922, -1.9766,\n",
      "         -2.2969, -1.9453, -2.3281, -3.2656, -3.2969, -2.9531, -0.3184, -2.4531,\n",
      "         -0.5156, -0.0311, -0.8516, -2.2656, -4.1875, -1.8672, -1.9297, -0.9180,\n",
      "         -1.9062, -2.6875, -1.5859, -2.5000, -2.0156, -1.2812, -1.4453, -2.7188,\n",
      "         -2.4688, -0.6250, -1.6328, -1.6719, -2.0625, -2.9219, -1.6250, -0.8086,\n",
      "         -1.9062, -2.7188, -0.1797, -1.6406, -2.8594, -2.2500, -2.3594, -2.1094,\n",
      "         -4.1562, -0.3945, -2.7500, -1.7109, -5.7500, -1.2109, -1.8984, -0.6289,\n",
      "         -1.1953, -1.6484, -2.0469, -1.2344, -1.6406, -0.2520, -1.8906, -1.4375,\n",
      "         -1.2344, -0.7695, -1.0156, -1.7578, -1.4375, -3.3750, -1.5078, -0.2715,\n",
      "         -0.5820, -2.3438, -1.0078, -1.4141, -1.9688, -1.3203, -2.8438, -3.4688,\n",
      "         -0.0859, -1.3125, -5.1250, -5.5938, -4.0938, -4.6875, -4.4062, -4.7812,\n",
      "         -4.8438, -4.8750, -4.4375, -5.2188, -3.5625, -4.5000, -4.4688, -4.6562,\n",
      "         -4.0938, -4.3125, -4.9375, -3.9688, -4.2500, -4.7812, -2.9219, -3.5312,\n",
      "         -3.7031, -4.9688, -4.8750, -2.9688, -4.2500, -5.0938, -3.6094, -4.7500,\n",
      "         -3.4375, -3.0312, -5.1250, -4.2500, -4.5312, -4.2812, -4.7812, -4.6875,\n",
      "         -4.4688, -4.5000, -4.0938, -4.0000, -4.5000, -3.2500, -4.5938, -5.0000,\n",
      "         -5.0938, -4.7500, -4.8438, -4.6250, -4.8438, -4.0000, -4.0312, -4.0625,\n",
      "         -4.6875, -4.8750, -5.0312, -4.3125, -4.4375, -4.7500, -4.6875, -4.3750,\n",
      "         -3.8125, -3.7500, -4.5625, -4.8438, -5.4688, -5.6250, -2.1719, -1.9531,\n",
      "         -5.1562, -5.3438, -5.5625, -5.4062, -5.5312, -5.5938, -5.5625, -5.3438,\n",
      "         -5.1250, -5.6250, -4.9375, -4.4062, -5.0000, -5.0312, -5.5000, -5.5312,\n",
      "         -5.8125, -5.5938, -5.5000, -5.6250, -5.5938, -5.5625, -5.5312, -5.5625,\n",
      "         -5.5312, -5.5938, -5.5312, -5.5625, -5.5000, -5.6250, -1.5234, -4.5312,\n",
      "         -5.0625, -5.0312, -5.0312, -5.3125, -5.3125, -5.5000, -5.4375, -5.3750,\n",
      "         -5.3125, -5.6250, -5.5312, -4.5312, -4.1875, -5.6562, -5.6562, -5.5000,\n",
      "         -5.5938, -5.5000, -5.5625, -5.5938, -5.6250, -5.6875, -5.5312, -5.4688,\n",
      "         -5.7188, -5.6250, -5.6250, -5.5625]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 166\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 176, 166])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 166, 256]), Cross output shape: torch.Size([1, 176, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0078,  1.4062, -0.6914,  ...,  2.2188,  0.5391, -0.3574],\n",
      "        [-1.7266,  0.5234, -0.2617,  ...,  0.0869, -0.6641, -1.1875],\n",
      "        [-1.0938,  0.8516,  1.8359,  ...,  0.6641, -0.5156, -0.7109]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 22, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  109.0000,   -97.0000,   -70.5000,  ...,   326.0000,\n",
      "             29.5000,   -30.5000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 22, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  2288.,  -1432.,  -5440.,  ...,     93.,   2336.,  -7520.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 166, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.0078,  1.4062, -0.6914,  ...,  2.2188,  0.5391, -0.3574],\n",
      "        [-1.7266,  0.5234, -0.2617,  ...,  0.0869, -0.6641, -1.1875],\n",
      "        [-1.0938,  0.8516,  1.8359,  ...,  0.6641, -0.5156, -0.7109]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 166]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 21]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 166, 176])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-11.3750, -10.0000,  -5.9375, -11.4375, -11.3750, -11.4375, -11.5000,\n",
      "         -11.3750, -11.3750, -11.4375, -11.4375, -11.4375, -11.4375,  -7.6875,\n",
      "          -2.3750, -11.5625, -11.5000, -10.9375, -11.5000, -11.5625, -11.4375,\n",
      "         -11.4375, -11.3750, -11.5000, -11.4375, -11.4375, -11.5625, -11.4375,\n",
      "         -11.5000, -11.4375, -11.5000, -11.4375, -11.5625, -11.4375, -11.5000,\n",
      "         -11.4375,  -4.7812,  -7.4688,  -2.3125,  -4.6250,  -6.4062,  -7.7188,\n",
      "          -5.8438,  -5.9688,  -3.6719,  -6.9688,  -5.1562,  -5.5312,  -3.9219,\n",
      "          -2.9219,  -4.2500,  -3.7188,  -5.7500,  -3.8281,  -4.7500,  -4.6875,\n",
      "          -5.5312,  -5.7188,  -6.3125,  -6.2812,  -5.5000,  -5.7188,  -7.2812,\n",
      "          -4.5938,  -6.3438,  -6.6250,  -7.0000,  -9.6875,  -7.6250,  -4.0312,\n",
      "          -4.5938,  -4.7812,  -5.7812,  -4.1562,  -4.5938,  -3.9688,  -4.3125,\n",
      "          -2.6719,  -2.6094,  -4.6250,  -4.5938,  -3.9531,  -5.3125,  -5.6875,\n",
      "          -3.0312,  -6.3750,  -4.2188,  -3.5625,  -4.6562,  -4.4375,  -6.2812,\n",
      "          -5.5312,  -6.2500,  -6.0312,  -6.0000,  -4.7812,  -8.5000,  -5.5625,\n",
      "          -7.4062,  -4.8750,  -5.7812,   3.3125,   2.1875,   1.7344,   1.0078,\n",
      "           0.8516,   1.7109,   1.6484,   0.4199,   0.8320,   1.2344,   0.0688,\n",
      "           0.4629,   1.5938,  -0.7031,   1.1484,   3.0938,  -0.9102,   1.3516,\n",
      "           1.8438,   2.0625,   0.0664,  -0.1182,   0.8086,  -4.8750,   0.6094,\n",
      "          -3.3594,  -6.1875,  -6.2188,  -5.5000, -10.1250, -11.4375,  -7.6562,\n",
      "          -9.3125,  -9.1250, -10.0625, -10.0000,  -9.7500,  -9.3750, -10.9375,\n",
      "          -7.9688,  -8.6875,  -9.7500,  -9.1250,  -9.1875,  -9.9375, -10.0000,\n",
      "          -9.7500,  -9.9375, -10.1250,  -8.0625,  -7.3125,  -8.3750, -10.5625,\n",
      "         -10.5625,  -9.3125,  -8.6875,  -9.8125,  -7.3750, -10.5625,  -8.1875,\n",
      "          -7.9375, -10.7500,  -9.0000,  -9.3125,  -9.6875,  -9.3750, -10.3125,\n",
      "          -9.8125,  -9.5000,  -8.5625,  -9.9375,  -9.9375,  -7.6562,  -9.9375,\n",
      "         -10.3750, -10.6250, -10.6250, -10.3750, -10.0000,  -7.7812,  -8.3750,\n",
      "          -9.3125,  -9.1250, -10.1875, -10.3125, -10.5000,  -9.6250,  -9.4375,\n",
      "         -10.3750, -10.1875, -10.0000,  -8.8750,  -8.8750, -10.1250, -10.1875,\n",
      "         -11.5000, -11.3750,  -7.1875,  -4.9062, -10.7500, -10.8125, -11.3750,\n",
      "         -11.0625, -11.3125, -10.9375, -11.4375, -11.3125, -10.9375, -11.4375,\n",
      "          -9.9375,  -9.3750, -10.6875, -10.4375, -11.3750, -11.5000, -11.5000,\n",
      "         -11.4375, -11.4375, -11.5000, -11.4375, -11.4375, -11.4375, -11.4375,\n",
      "         -11.3750, -11.5000, -11.5625, -11.4375, -11.2500, -11.4375,  -4.5000,\n",
      "         -10.0625, -10.6250, -10.1875, -10.3125, -10.5625, -10.8125, -11.1250,\n",
      "         -11.1875, -11.3750, -11.3125, -11.4375, -11.4375,  -9.8750,  -7.5938,\n",
      "         -11.5000, -11.5000, -11.4375, -11.4375, -11.5000, -11.4375, -11.5625,\n",
      "         -11.4375, -11.5000, -11.5000, -11.5000, -11.4375, -11.5000, -11.5000,\n",
      "         -11.4375]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 167\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 176, 167])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 167, 256]), Cross output shape: torch.Size([1, 176, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.7266,  0.5234, -0.2617,  ...,  0.0869, -0.6641, -1.1875],\n",
      "        [-1.0938,  0.8516,  1.8359,  ...,  0.6641, -0.5156, -0.7109],\n",
      "        [-1.0156, -0.0737,  0.2246,  ...,  1.8516, -1.0859, -0.7109]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 22, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [   21.7500,  -276.0000,   114.0000,  ...,   808.0000,\n",
      "            185.0000,   -80.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 22, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  5952.,  -1968., -14976.,  ...,   -380.,   5600., -17536.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 167, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.7266,  0.5234, -0.2617,  ...,  0.0869, -0.6641, -1.1875],\n",
      "        [-1.0938,  0.8516,  1.8359,  ...,  0.6641, -0.5156, -0.7109],\n",
      "        [-1.0156, -0.0737,  0.2246,  ...,  1.8516, -1.0859, -0.7109]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 167]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 21]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 167, 176])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-9.1875, -8.6875, -3.3906, -9.1875, -9.1875, -9.1875, -9.3750, -9.1875,\n",
      "         -9.1250, -9.1875, -9.1875, -9.1875, -9.2500, -6.8750, -1.6562, -9.2500,\n",
      "         -9.1250, -8.9375, -9.1875, -9.2500, -9.1875, -9.2500, -9.1250, -9.1250,\n",
      "         -9.1250, -9.1250, -9.1875, -9.1875, -9.3125, -9.1250, -9.2500, -9.2500,\n",
      "         -9.1250, -9.1250, -9.1875, -9.2500, -0.2305, -6.1562, -4.5312, -5.7500,\n",
      "         -6.2500, -6.6250, -5.5938, -6.0312, -4.4688, -3.7812, -4.4375, -5.5938,\n",
      "          0.0874, -2.6562, -0.4902, -2.9219, -5.2500, -4.1562, -4.6250, -4.6875,\n",
      "         -4.7500, -5.7188, -5.3438, -5.3438, -6.3438, -5.4375, -5.8750, -3.6094,\n",
      "         -5.2812, -4.5312, -4.5625, -7.6562, -6.4062, -5.3125, -4.2812, -5.0938,\n",
      "         -5.6562, -4.5312, -5.5000, -3.9844, -6.4062, -3.5469, -5.3438, -5.0312,\n",
      "         -6.0000, -4.3438, -4.0000, -5.3750, -3.6250, -6.7188, -4.6875, -4.1562,\n",
      "         -4.6562, -4.7812, -6.5312, -5.0625, -5.0000, -7.0625, -6.2812, -5.0938,\n",
      "         -7.5938, -3.5312, -6.0625, -4.4375, -7.1250, -0.7344,  1.7891,  2.3750,\n",
      "          2.3438, -0.0786,  0.9453,  1.8672, -1.4453,  1.4922, -2.3906, -0.6484,\n",
      "          1.5156,  2.8594,  6.5312, -1.7969,  3.3281, -0.6211,  2.9062,  2.8125,\n",
      "          2.5156,  1.3047,  1.2812, -0.7695, -3.1719, -1.8047, -3.5781, -5.3438,\n",
      "         -4.9062, -5.3125, -8.2500, -9.1875, -7.0938, -7.7500, -7.8125, -8.1875,\n",
      "         -8.1875, -8.0625, -7.9375, -9.1250, -7.9375, -7.5938, -7.7188, -7.6562,\n",
      "         -7.1562, -7.7812, -8.0625, -7.7188, -7.9062, -8.1250, -7.4375, -6.1875,\n",
      "         -6.6250, -8.8125, -8.6875, -7.8125, -7.4375, -7.5625, -7.2188, -8.5625,\n",
      "         -6.8438, -5.9375, -8.8125, -7.0625, -7.7500, -7.8750, -7.9062, -8.3125,\n",
      "         -8.2500, -7.9688, -6.6250, -7.7188, -7.8750, -6.9062, -8.0000, -8.3750,\n",
      "         -8.5000, -8.5625, -8.5625, -8.1875, -7.2500, -6.4688, -7.1562, -7.4688,\n",
      "         -8.1250, -8.1875, -8.4375, -7.8750, -7.8125, -8.2500, -8.2500, -8.0625,\n",
      "         -6.9062, -7.1875, -8.3750, -8.3750, -9.1875, -9.0625, -7.3438, -4.4062,\n",
      "         -8.5000, -8.9375, -9.1250, -8.8125, -9.0625, -8.8125, -9.1250, -9.2500,\n",
      "         -8.6250, -9.1875, -8.2500, -8.1250, -8.7500, -8.5000, -9.1250, -9.1875,\n",
      "         -9.3125, -9.1875, -9.1875, -9.3125, -9.1875, -9.1875, -9.1875, -9.1875,\n",
      "         -9.0625, -9.2500, -9.2500, -9.2500, -9.1875, -9.3125, -5.3750, -8.0625,\n",
      "         -8.4375, -8.1250, -8.2500, -8.3750, -8.7500, -8.8750, -9.0625, -9.1875,\n",
      "         -9.0625, -9.1875, -9.3125, -7.6250, -6.4375, -9.1875, -9.1875, -9.1250,\n",
      "         -9.1875, -9.2500, -9.1875, -9.3125, -9.1875, -9.1250, -9.2500, -9.1875,\n",
      "         -9.2500, -9.1875, -9.2500, -9.1250]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 168\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 176, 168])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 168, 256]), Cross output shape: torch.Size([1, 176, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0938,  0.8516,  1.8359,  ...,  0.6641, -0.5156, -0.7109],\n",
      "        [-1.0156, -0.0737,  0.2246,  ...,  1.8516, -1.0859, -0.7109],\n",
      "        [-1.1406,  0.9766, -2.0938,  ..., -0.1396, -2.6406, -0.6055]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 22, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -15.2500,  -356.0000,   198.0000,  ...,   912.0000,\n",
      "            207.0000,  -108.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 22, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7232.,  -2352., -18304.,  ...,   -464.,   6688., -21248.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 168, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0938,  0.8516,  1.8359,  ...,  0.6641, -0.5156, -0.7109],\n",
      "        [-1.0156, -0.0737,  0.2246,  ...,  1.8516, -1.0859, -0.7109],\n",
      "        [-1.1406,  0.9766, -2.0938,  ..., -0.1396, -2.6406, -0.6055]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 168]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 21]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 168, 176])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-9.1250, -8.3125, -1.1094, -9.3125, -9.2500, -9.2500, -9.3125, -9.2500,\n",
      "         -9.2500, -9.1875, -9.2500, -9.2500, -9.2500, -5.1250, -0.3984, -9.3125,\n",
      "         -9.2500, -9.0000, -9.3125, -9.3125, -9.1250, -9.3125, -9.1875, -9.1875,\n",
      "         -9.1875, -9.2500, -9.2500, -9.1875, -9.3125, -9.1875, -9.2500, -9.1875,\n",
      "         -9.1875, -9.2500, -9.2500, -9.2500,  1.8047, -3.8125, -3.4062, -4.6875,\n",
      "         -6.0938, -4.1250, -4.2188, -4.2812, -4.4062, -2.8750, -2.4219, -3.9531,\n",
      "          0.5078, -1.9766, -0.5078, -1.5703, -4.1875, -2.5000, -2.2188, -3.3125,\n",
      "         -4.0000, -3.3438, -3.9062, -4.4688, -5.4062, -4.5312, -4.0000, -2.7344,\n",
      "         -4.0312, -2.2969, -3.5156, -6.1562, -5.3125, -3.7344, -4.1875, -4.2812,\n",
      "         -0.9609, -4.1250, -5.0312, -3.1250, -4.8125, -3.5625, -4.8750, -4.4688,\n",
      "         -5.5312, -5.1250, -4.5312, -4.5938, -4.2812, -6.3125, -5.2500, -4.4062,\n",
      "         -3.6562, -3.6562, -6.6562, -4.8125, -3.6094, -4.3125, -4.9375, -5.0625,\n",
      "         -5.4688, -2.2656, -5.0312, -3.6250, -6.0312,  1.6953, -1.4531,  0.1221,\n",
      "         10.1875,  1.3828, -0.6289,  3.0938, -2.4062,  2.3750, -2.4062, -1.4375,\n",
      "         -0.9180, -0.5352,  1.3281,  0.4023, -1.2188, -2.3750, -1.7891,  0.7852,\n",
      "          2.1094, -0.0181, -0.2637, -2.3125,  0.4395,  2.1406, -1.5312, -3.7656,\n",
      "         -2.6562, -4.2188, -7.9688, -9.3125, -5.5312, -6.4062, -6.5625, -7.3750,\n",
      "         -7.7188, -7.7500, -7.3438, -8.5625, -5.6250, -6.5000, -7.0312, -6.4375,\n",
      "         -6.7812, -6.7188, -7.3750, -7.1562, -7.3750, -7.6562, -6.8438, -6.4062,\n",
      "         -5.3750, -8.1875, -8.3125, -6.9375, -6.0625, -5.7500, -4.9375, -7.8750,\n",
      "         -5.2188, -4.2500, -8.4375, -6.9062, -7.3438, -7.4688, -7.4375, -7.8750,\n",
      "         -7.5000, -7.4375, -6.1875, -7.1250, -6.9375, -4.9688, -7.6250, -7.9688,\n",
      "         -8.0000, -7.5938, -8.0000, -7.5000, -6.0000, -5.1250, -5.7188, -6.9062,\n",
      "         -7.8125, -7.9375, -7.9688, -6.9062, -7.0000, -7.9375, -7.7500, -7.6875,\n",
      "         -6.4062, -7.1250, -7.6250, -7.5625, -9.1250, -9.1250, -5.9062, -2.9531,\n",
      "         -8.5625, -8.7500, -9.1875, -8.8125, -9.1250, -9.0625, -9.1875, -9.1250,\n",
      "         -8.5625, -9.1250, -7.5938, -7.4688, -8.3750, -8.3125, -9.1875, -9.1875,\n",
      "         -9.3125, -9.2500, -9.3125, -9.3125, -9.1875, -9.2500, -9.2500, -9.1875,\n",
      "         -9.2500, -9.1875, -9.1875, -9.2500, -9.0625, -9.3125, -4.5938, -7.4688,\n",
      "         -7.9062, -7.7500, -7.8125, -8.1875, -8.3750, -8.7500, -9.1250, -9.3125,\n",
      "         -9.2500, -9.2500, -9.1875, -7.2500, -6.0938, -9.2500, -9.2500, -9.1250,\n",
      "         -9.2500, -9.2500, -9.2500, -9.2500, -9.1875, -9.1875, -9.2500, -9.1875,\n",
      "         -9.2500, -9.3125, -9.1875, -9.1875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 169\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 176, 169])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 169, 256]), Cross output shape: torch.Size([1, 176, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0156, -0.0737,  0.2246,  ...,  1.8516, -1.0859, -0.7109],\n",
      "        [-1.1406,  0.9766, -2.0938,  ..., -0.1396, -2.6406, -0.6055],\n",
      "        [-1.5938,  1.1719, -1.4062,  ..., -0.4492,  2.0000, -0.2168]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 22, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -388.0000,   238.0000,  ...,   960.0000,\n",
      "            231.0000,  -120.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 22, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7904.,  -2288., -20224.,  ...,   -592.,   7264., -23040.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 169, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0156, -0.0737,  0.2246,  ...,  1.8516, -1.0859, -0.7109],\n",
      "        [-1.1406,  0.9766, -2.0938,  ..., -0.1396, -2.6406, -0.6055],\n",
      "        [-1.5938,  1.1719, -1.4062,  ..., -0.4492,  2.0000, -0.2168]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 169]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 21]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 169, 176])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.5000, -4.1875,  4.5938, -6.3750, -6.5000, -6.5625, -6.5312, -6.5625,\n",
      "         -6.4688, -6.5625, -6.6562, -6.3438, -6.5938, -3.4375,  3.5156, -6.5938,\n",
      "         -6.5000, -6.1562, -6.4688, -6.4688, -6.5625, -6.6250, -6.4062, -6.4688,\n",
      "         -6.5625, -6.5000, -6.5000, -6.6250, -6.5938, -6.6250, -6.5938, -6.5312,\n",
      "         -6.5938, -6.4688, -6.4062, -6.5625, 10.5000,  0.4277, -0.6758, -1.7656,\n",
      "         -2.5312, -3.5938, -2.4844, -0.9609, -1.9766, -0.4824, -0.1318, -1.7109,\n",
      "          3.5938,  1.4609,  2.5156,  4.4062, -0.6641, -1.1797, -1.4453, -1.8516,\n",
      "         -2.2812, -1.6953, -2.1719, -1.4922, -2.4844, -2.3906, -0.0435, -0.4551,\n",
      "          0.9492, -1.7891, -1.3438, -1.6016, -3.2188, -1.7422, -1.3047, -1.5703,\n",
      "         -1.2734, -1.8984, -2.1250, -2.1719, -1.8828, -1.3828, -1.6172, -2.3281,\n",
      "         -3.2188, -1.4062, -0.4297, -1.5312, -0.9922, -4.3750, -1.3984, -1.7656,\n",
      "         -0.8594, -1.8359, -1.9141, -1.4766, -2.8750, -3.1562, -2.9844, -2.4688,\n",
      "         -2.7344, -0.7148, -1.3125, -0.2490, -3.3906, -0.9609, -2.1250, -2.2969,\n",
      "         -0.5508, -0.8711, -2.7500, -0.5898, -0.7617, -0.8867, -0.2734, -1.4531,\n",
      "         -1.9297, -1.1953,  0.0903, -0.2637, -2.0781, -0.8242, -1.9141, -1.1719,\n",
      "         -1.9688, -0.9531, -0.6875, -1.1484, -1.4375, -0.5742, -1.5312, -2.4219,\n",
      "          0.0820, -3.5938, -5.8125, -6.5000, -2.8906, -4.2188, -4.2188, -5.4688,\n",
      "         -5.5312, -5.4375, -4.8125, -5.8125, -3.7031, -3.9688, -4.9375, -4.4375,\n",
      "         -4.2188, -4.3438, -5.0312, -4.7500, -5.2188, -5.1250, -3.4844, -3.0000,\n",
      "         -3.4844, -5.5625, -5.8125, -3.4375, -4.0938, -4.6562, -3.4219, -5.2500,\n",
      "         -3.6875, -3.5156, -5.7500, -4.8438, -4.6562, -4.6875, -4.1875, -5.6250,\n",
      "         -4.6562, -4.8125, -4.2500, -4.9375, -4.4688, -2.8594, -5.1250, -5.8750,\n",
      "         -5.3750, -5.5000, -5.4062, -4.9375, -3.7031, -3.6562, -3.5312, -3.8281,\n",
      "         -5.4062, -5.4688, -5.6562, -4.1250, -4.4375, -5.6250, -5.5312, -5.0938,\n",
      "         -3.9219, -3.5625, -5.3125, -5.1875, -6.4688, -6.4375, -2.1094, -0.4609,\n",
      "         -5.9688, -5.7812, -6.5625, -6.1875, -6.5938, -6.1875, -6.5625, -6.2188,\n",
      "         -5.9688, -6.3438, -5.0625, -4.2812, -6.0938, -6.1250, -6.4688, -6.5000,\n",
      "         -6.5625, -6.5312, -6.5312, -6.5625, -6.4062, -6.4688, -6.3750, -6.4688,\n",
      "         -6.5625, -6.4375, -6.5938, -6.5625, -6.3438, -6.6562, -2.0000, -4.7812,\n",
      "         -5.7500, -5.0938, -5.4375, -5.6250, -5.8438, -5.9375, -6.2812, -6.4375,\n",
      "         -6.5625, -6.5000, -6.5000, -4.8438, -3.5156, -6.5312, -6.5625, -6.5000,\n",
      "         -6.4375, -6.5938, -6.6250, -6.4688, -6.6250, -6.5000, -6.5938, -6.5312,\n",
      "         -6.6250, -6.7500, -6.5312, -6.5000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 170\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 176, 170])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 170, 256]), Cross output shape: torch.Size([1, 176, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.1406,  0.9766, -2.0938,  ..., -0.1396, -2.6406, -0.6055],\n",
      "        [-1.5938,  1.1719, -1.4062,  ..., -0.4492,  2.0000, -0.2168],\n",
      "        [-1.8672,  0.0078,  0.8789,  ..., -0.3477, -0.4043,  0.1025]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 22, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -61.5000,  -504.0000,   344.0000,  ...,  1136.0000,\n",
      "            288.0000,  -206.0000],\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 22, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9792.,  -3040., -25216.,  ...,   -812.,   9024., -28928.],\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 170, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.1406,  0.9766, -2.0938,  ..., -0.1396, -2.6406, -0.6055],\n",
      "        [-1.5938,  1.1719, -1.4062,  ..., -0.4492,  2.0000, -0.2168],\n",
      "        [-1.8672,  0.0078,  0.8789,  ..., -0.3477, -0.4043,  0.1025]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 170]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 170, 176])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.6875, -10.4375,  -7.3125, -10.6875, -10.6875, -10.7500, -10.7500,\n",
      "         -10.6875, -10.6875, -10.7500, -10.7500, -10.6875, -10.6875,  -8.2500,\n",
      "          -4.6562, -10.7500, -10.7500, -10.3750, -10.8125, -10.7500, -10.6875,\n",
      "         -10.6875, -10.7500, -10.7500, -10.6875, -10.6875, -10.8125, -10.7500,\n",
      "         -10.7500, -10.7500, -10.8125, -10.7500, -10.7500, -10.6875, -10.8125,\n",
      "         -10.7500,  -5.1562,  -7.6875,  -4.6875,  -6.6250,  -6.5312,  -7.3125,\n",
      "          -7.7812,  -5.9062,  -5.0000,  -9.3125,  -7.7812,  -7.2812,  -5.8750,\n",
      "          -4.1875,  -5.4375,  -5.7500,  -6.1562,  -4.6562,  -5.3125,  -5.7188,\n",
      "          -6.3125,  -6.1250,  -6.6875,  -6.5312,  -6.2812,  -6.4375,  -8.1875,\n",
      "          -6.9062,  -7.1562,  -8.1250,  -8.2500,  -9.1250,  -8.2500,  -4.2188,\n",
      "          -4.9375,  -4.9062,  -6.0000,  -5.0312,  -4.7812,  -5.0000,  -5.2500,\n",
      "          -2.6406,  -3.3594,  -5.7188,  -4.7500,  -3.7656,  -5.8750,  -5.8438,\n",
      "          -2.6250,  -6.7188,  -4.7812,  -4.0312,  -4.5625,  -5.1562,  -6.4688,\n",
      "          -5.7812,  -7.2188,  -6.4375,  -6.9688,  -4.8125,  -8.3750,  -8.1875,\n",
      "          -7.3750,  -5.2188,  -6.9375,   2.2188,   2.4531,   2.5156,   1.4141,\n",
      "           1.9688,   2.0469,   1.8750,   1.3594,   0.9414,   1.0234,   0.1738,\n",
      "           1.1094,   2.4219,   0.1543,   2.0000,   4.7500,  -1.0703,   1.7812,\n",
      "           2.6562,   2.7812,   0.3965,   1.2031,   0.5273,  -6.4062,   0.4160,\n",
      "          -2.4375,  -7.4062,  -7.0938,  -7.7812,  -9.7500, -10.8125,  -8.8125,\n",
      "          -9.0625,  -8.8125,  -9.5625,  -9.6250,  -9.4375,  -9.0000, -10.4375,\n",
      "          -8.6250,  -8.5000,  -9.3125,  -8.9375,  -8.8750,  -9.3750,  -9.5000,\n",
      "          -8.9375,  -9.5000,  -9.8750,  -8.5000,  -7.4375,  -8.6250, -10.0625,\n",
      "         -10.0000,  -8.5625,  -8.6875,  -9.9375,  -8.4375, -10.0625,  -8.1875,\n",
      "          -8.8750, -10.2500,  -8.6875,  -9.2500,  -9.4375,  -9.2500,  -9.9375,\n",
      "          -9.5000,  -9.2500,  -8.6875,  -9.6250,  -9.7500,  -7.6875,  -9.7500,\n",
      "          -9.8750, -10.0000, -10.0000,  -9.8750,  -9.5625,  -7.7188,  -8.5625,\n",
      "          -9.5000,  -9.2500,  -9.8125,  -9.7500, -10.0625,  -9.1875,  -9.1250,\n",
      "          -9.9375,  -9.8125,  -9.6875,  -8.6875,  -8.8125,  -9.6250,  -9.6875,\n",
      "         -10.6875, -10.7500,  -7.0312,  -6.3438, -10.2500, -10.1875, -10.6875,\n",
      "         -10.5000, -10.5625, -10.3750, -10.7500, -10.6250, -10.3750, -10.6875,\n",
      "          -9.4375,  -9.0000, -10.1875, -10.0000, -10.6875, -10.7500, -10.7500,\n",
      "         -10.8125, -10.6875, -10.6875, -10.7500, -10.6875, -10.8125, -10.7500,\n",
      "         -10.6875, -10.7500, -10.7500, -10.6875, -10.6250, -10.6875,  -5.8125,\n",
      "          -9.6250, -10.1250, -10.0000, -10.0000, -10.0625, -10.2500, -10.4375,\n",
      "         -10.5625, -10.6875, -10.6250, -10.7500, -10.8125,  -9.5625,  -8.0000,\n",
      "         -10.7500, -10.8125, -10.6875, -10.6875, -10.7500, -10.6875, -10.8750,\n",
      "         -10.6875, -10.6875, -10.7500, -10.8125, -10.8125, -10.8125, -10.6875,\n",
      "         -10.6875]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 171\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 184, 171])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 171, 256]), Cross output shape: torch.Size([1, 184, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.5938,  1.1719, -1.4062,  ..., -0.4492,  2.0000, -0.2168],\n",
      "        [-1.8672,  0.0078,  0.8789,  ..., -0.3477, -0.4043,  0.1025],\n",
      "        [-0.4434, -1.2031,  3.4219,  ...,  0.8438,  1.3203,  0.2637]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  552.0000,   288.0000,  -250.0000,  ...,   -49.0000,\n",
      "           -118.0000,   -13.7500]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 23, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.],\n",
      "         [ -2464.,  -2288.,   6368.,  ...,    592.,  -1864.,   3504.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 171, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.5938,  1.1719, -1.4062,  ..., -0.4492,  2.0000, -0.2168],\n",
      "        [-1.8672,  0.0078,  0.8789,  ..., -0.3477, -0.4043,  0.1025],\n",
      "        [-0.4434, -1.2031,  3.4219,  ...,  0.8438,  1.3203,  0.2637]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 171]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 22]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 171, 184])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.6875, -9.3750, -4.3438, -7.6562, -7.6875, -7.5938, -7.6875, -7.6875,\n",
      "         -7.6250, -7.5938, -7.6875, -7.6250, -7.6250, -5.5000, -4.2500, -7.6250,\n",
      "         -7.5938, -7.4375, -7.6562, -7.5938, -7.6250, -7.6250, -7.4688, -7.6562,\n",
      "         -7.5938, -7.5625, -7.6250, -7.6250, -7.6562, -7.5938, -7.6875, -7.5625,\n",
      "         -7.5625, -7.5938, -7.5625, -7.6562, -1.6953, -4.1562, -3.9531, -4.6562,\n",
      "         -5.0312, -5.0312, -5.1250, -3.5312, -4.3438, -4.5000, -4.4062, -5.4375,\n",
      "         -2.0312, -2.5000, -0.9961, -4.2500, -4.8750, -5.0000, -4.9375, -4.3125,\n",
      "         -4.7188, -6.0312, -4.4688, -5.8438, -5.2188, -6.5312, -3.4062, -3.6094,\n",
      "         -5.7500, -5.3125, -3.5156, -5.1562, -5.6250, -4.7500, -6.1875, -4.8438,\n",
      "         -5.1562, -2.5781, -5.6250, -5.0312, -3.5469, -3.9531, -5.6875, -5.4062,\n",
      "         -4.2812, -5.2188, -5.0312, -4.9062, -4.1250, -6.9375, -4.6562, -4.8438,\n",
      "         -4.5312, -4.2812, -5.4375, -5.0000, -5.7812, -5.3750, -5.0938, -4.0938,\n",
      "         -5.3438, -5.6562, -5.9062, -4.3438, -6.0625,  3.8906, -1.8906, -1.4297,\n",
      "         -1.2422,  8.8125, -2.5312, -1.6953,  2.5156,  4.0625, -3.8438, -2.1719,\n",
      "          3.0938, -1.8906, -0.9805,  4.2188, -0.9336, -3.8438,  3.9375,  0.2002,\n",
      "         -0.7891,  4.5625, -1.9531, -1.5078, -4.7500,  1.3672, -1.8594, -4.5000,\n",
      "         -5.2500, -5.0625, -6.9688, -7.6562, -4.9688, -6.1875, -6.0000, -6.8750,\n",
      "         -6.9062, -6.8750, -6.5625, -7.1250, -6.2812, -6.0312, -6.3438, -6.1562,\n",
      "         -6.2188, -6.2188, -6.5312, -6.6250, -6.5000, -6.6250, -5.9375, -6.6875,\n",
      "         -5.9688, -6.8438, -6.9375, -6.2188, -6.3750, -5.2500, -5.5312, -6.9688,\n",
      "         -6.0938, -4.8750, -7.0938, -6.3750, -6.3438, -6.5625, -6.4688, -6.9688,\n",
      "         -6.5312, -6.0000, -5.9062, -6.9062, -6.5312, -6.0625, -6.7188, -7.0312,\n",
      "         -6.9062, -6.9062, -6.8438, -6.7812, -5.3750, -6.0000, -5.9062, -6.2812,\n",
      "         -6.8750, -6.8750, -7.0000, -6.5000, -6.5938, -6.9375, -6.8750, -6.8125,\n",
      "         -6.2500, -6.5625, -6.7500, -6.6562, -7.5625, -7.5312, -5.2188, -1.7031,\n",
      "         -7.3125, -7.2812, -7.5938, -7.4375, -7.5000, -7.4688, -7.6875, -7.5625,\n",
      "         -7.2500, -7.6562, -6.8438, -6.0000, -7.2188, -7.0938, -7.6250, -7.6250,\n",
      "         -7.7188, -7.6250, -7.6250, -7.6875, -7.6250, -7.6562, -7.6250, -7.5625,\n",
      "         -7.5938, -7.7188, -7.5938, -7.5938, -7.5312, -7.6562, -4.7500, -6.6875,\n",
      "         -7.0312, -6.6562, -7.0000, -7.1250, -7.1250, -7.3750, -7.4375, -7.5938,\n",
      "         -7.5312, -7.6250, -7.5938, -6.4688, -5.0000, -7.6562, -7.6562, -7.6250,\n",
      "         -7.6875, -7.6562, -7.5625, -7.6250, -7.5312, -7.5938, -7.6562, -7.6875,\n",
      "         -7.6250, -7.5938, -7.6875, -7.5938]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 172\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 184, 172])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 172, 256]), Cross output shape: torch.Size([1, 184, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.8672,  0.0078,  0.8789,  ..., -0.3477, -0.4043,  0.1025],\n",
      "        [-0.4434, -1.2031,  3.4219,  ...,  0.8438,  1.3203,  0.2637],\n",
      "        [-0.8008,  0.5352,  0.5508,  ...,  1.1641, -1.4453, -1.2891]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  -18.7500,  -180.0000,    25.7500,  ...,   720.0000,\n",
      "            117.0000,   -67.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 23, 512]), Global transformer output: tensor([[[-12928.0000, -36864.0000,  31488.0000,  ...,   4512.0000,\n",
      "           -9728.0000,  -5952.0000],\n",
      "         [ 21504.0000, -11584.0000, -56576.0000,  ...,  -1032.0000,\n",
      "           18944.0000, -68608.0000],\n",
      "         [  3040.0000, -14208.0000,  -7936.0000,  ...,   -992.0000,\n",
      "            4224.0000, -22656.0000],\n",
      "         ...,\n",
      "         [  9856.0000,  -3040.0000, -25344.0000,  ...,   -820.0000,\n",
      "            9088.0000, -28928.0000],\n",
      "         [  7936.0000,  -2288.0000, -20224.0000,  ...,   -592.0000,\n",
      "            7296.0000, -23040.0000],\n",
      "         [  4384.0000,  -1640.0000, -11136.0000,  ...,   -107.5000,\n",
      "            4192.0000, -13440.0000]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 172, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.8672,  0.0078,  0.8789,  ..., -0.3477, -0.4043,  0.1025],\n",
      "        [-0.4434, -1.2031,  3.4219,  ...,  0.8438,  1.3203,  0.2637],\n",
      "        [-0.8008,  0.5352,  0.5508,  ...,  1.1641, -1.4453, -1.2891]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 172]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 22]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 172, 184])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-10.6875, -11.9375,  -5.4688, -10.6875, -10.8125, -10.6250, -10.7500,\n",
      "         -10.6875, -10.7500, -10.8125, -10.8125, -10.7500, -10.6875,  -6.7500,\n",
      "          -3.5156, -10.7500, -10.8750, -10.4375, -10.7500, -10.6875, -10.6875,\n",
      "         -10.7500, -10.6250, -10.6875, -10.6250, -10.7500, -10.6250, -10.6875,\n",
      "         -10.7500, -10.7500, -10.7500, -10.6875, -10.5625, -10.6875, -10.6875,\n",
      "         -10.8750,  -0.9922,  -5.5625,  -3.4062,  -6.9375,  -6.8750,  -5.8125,\n",
      "          -5.9688,  -3.6250,  -5.3750,  -5.0938,  -5.8438,  -5.8125,  -1.3906,\n",
      "          -0.9219,  -0.4238,  -3.2500,  -4.4062,  -5.1250,  -5.3750,  -4.9688,\n",
      "          -4.5938,  -6.0625,  -5.4688,  -4.4688,  -6.5000,  -6.0312,  -3.9062,\n",
      "          -4.1250,  -6.1250,  -5.7500,  -4.7188,  -5.5938,  -7.1875,  -5.3438,\n",
      "          -5.7500,  -4.9062,  -6.2812,  -5.7500,  -6.0938,  -5.3438,  -5.8125,\n",
      "          -5.3438,  -5.2188,  -6.2188,  -6.0312,  -4.8125,  -2.8594,  -5.9688,\n",
      "          -4.2812,  -5.7500,  -5.7812,  -3.7188,  -4.6250,  -6.4375,  -6.3750,\n",
      "          -5.4688,  -5.1250,  -6.9062,  -6.4062,  -5.8125,  -7.5938,  -6.4688,\n",
      "          -6.0312,  -5.6250,  -6.0938,   4.0312,  -0.2070,   1.4375,   0.7383,\n",
      "           2.7812,  -1.6016,  -0.2148,  -1.4375,  -0.6797,  -2.4375,  -1.0469,\n",
      "           1.7109,  -0.0967,   8.5000,   1.5625,   3.2188,  -1.7578,   4.1250,\n",
      "           1.7578,   2.5312,  -0.6523,  -0.2578,  -0.9492,  -1.0859,  -1.9609,\n",
      "          -1.5234,  -4.6250,  -4.9375,  -5.2188,  -9.8750, -10.7500,  -6.5625,\n",
      "          -7.9688,  -7.5625,  -9.2500,  -9.3125,  -9.4375,  -8.5625,  -9.7500,\n",
      "          -7.0000,  -7.3438,  -8.5000,  -8.1250,  -7.9062,  -8.2500,  -8.5000,\n",
      "          -8.4375,  -8.7500,  -9.1875,  -7.2812,  -6.6250,  -7.5625,  -8.9375,\n",
      "          -9.5000,  -8.1875,  -8.0000,  -6.8125,  -7.2500,  -9.1875,  -7.4688,\n",
      "          -8.4375,  -9.8125,  -7.7812,  -7.9375,  -8.6250,  -8.6875,  -9.2500,\n",
      "          -8.5625,  -8.3125,  -7.7188,  -9.1250,  -8.5625,  -7.2188,  -8.8125,\n",
      "          -9.5625,  -9.6250,  -9.4375,  -9.0625,  -8.6250,  -8.3750,  -7.8750,\n",
      "          -7.0625,  -8.1875,  -9.0000,  -9.1875,  -9.6875,  -8.3750,  -8.0000,\n",
      "          -9.5625,  -9.1875,  -8.8750,  -8.1875,  -8.2500,  -9.0000,  -9.0625,\n",
      "         -10.6875, -10.7500,  -6.9375,  -3.6250,  -9.9375,  -9.8125, -10.6250,\n",
      "         -10.4375, -10.6875, -10.5625, -10.6875, -10.5000, -10.0000, -10.7500,\n",
      "          -9.2500,  -8.0625, -10.1250, -10.0625, -10.7500, -10.7500, -10.9375,\n",
      "         -10.6875, -10.7500, -10.6875, -10.5625, -10.6875, -10.6875, -10.6875,\n",
      "         -10.6875, -10.6875, -10.7500, -10.8750, -10.4375, -10.7500,  -4.9375,\n",
      "          -8.9375,  -9.5625,  -9.1875,  -9.5625,  -9.6875,  -9.8125, -10.1250,\n",
      "         -10.5625, -10.6875, -10.6250, -10.6875, -10.6875,  -8.6875,  -6.5625,\n",
      "         -10.7500, -10.7500, -10.6250, -10.7500, -10.7500, -10.8125, -10.6250,\n",
      "         -10.6250, -10.6875, -10.7500, -10.7500, -10.6875, -10.8125, -10.6875,\n",
      "         -10.6875]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 173\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 184, 173])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 173, 256]), Cross output shape: torch.Size([1, 184, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.4434, -1.2031,  3.4219,  ...,  0.8438,  1.3203,  0.2637],\n",
      "        [-0.8008,  0.5352,  0.5508,  ...,  1.1641, -1.4453, -1.2891],\n",
      "        [ 2.2188,  0.0410, -1.1641,  ...,  1.2344, -1.2969, -1.5625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  -42.0000,  -288.0000,   178.0000,  ...,   864.0000,\n",
      "            182.0000,   -91.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 23, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.],\n",
      "         [  6272.,  -2080., -15936.,  ...,   -294.,   5792., -18688.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 173, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.4434, -1.2031,  3.4219,  ...,  0.8438,  1.3203,  0.2637],\n",
      "        [-0.8008,  0.5352,  0.5508,  ...,  1.1641, -1.4453, -1.2891],\n",
      "        [ 2.2188,  0.0410, -1.1641,  ...,  1.2344, -1.2969, -1.5625]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 173]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 22]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 173, 184])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[ -9.8125,  -8.6250,   0.8242,  -9.8750,  -9.8125,  -9.8125,  -9.8125,\n",
      "          -9.8750,  -9.8125,  -9.8125,  -9.9375, -10.0000,  -9.8750,  -5.7500,\n",
      "           2.0312,  -9.8750,  -9.8750,  -9.4375,  -9.8750,  -9.8750,  -9.8125,\n",
      "          -9.8125,  -9.8125,  -9.8125,  -9.7500,  -9.8125,  -9.8125,  -9.7500,\n",
      "          -9.8125,  -9.8750,  -9.8750,  -9.8750,  -9.7500,  -9.7500,  -9.8125,\n",
      "          -9.8125,   5.0625,   0.3691,  -1.0938,  -4.0625,  -3.9531,  -4.2500,\n",
      "          -4.4688,  -0.9844,  -2.7812,  -2.5781,  -0.7852,  -4.4062,   3.0938,\n",
      "           0.4004,   4.1562,  -1.3125,  -2.2812,  -3.2812,  -4.1875,  -3.7969,\n",
      "          -2.2812,  -3.0938,  -3.7188,  -2.9375,  -4.8125,  -2.0000,  -0.3965,\n",
      "          -1.1953,  -1.4766,  -3.0156,  -4.8438,  -0.8203,  -5.0000,  -3.5156,\n",
      "          -3.6406,  -0.2910,  -4.6562,  -4.6562,  -4.0938,  -3.0781,  -4.0938,\n",
      "          -1.2500,  -2.5000,  -4.1875,  -5.4688,  -4.5312,  -4.5000,  -3.2969,\n",
      "          -4.9688,  -2.5781,  -3.7188,  -3.7500,  -3.0312,  -3.4219,  -3.4531,\n",
      "          -4.5625,  -4.0938,  -4.6562,  -4.1562,  -3.4844,  -4.9062,  -3.1719,\n",
      "          -1.7344,  -3.7656,  -3.0781,  -0.2930,  -2.5000,   7.6875,   0.2275,\n",
      "           1.1328,  -0.7031,   1.9609,  -0.2773,   1.7344,   0.9922,   0.3848,\n",
      "          -2.4219,  -1.8125,   2.2344,  -0.7969,  -2.4531,   0.0398,  -0.9180,\n",
      "           1.2578,   1.5547,  -1.0000,  -1.7656,  -2.0469,  -2.8438,  -0.6953,\n",
      "           1.3359,  -2.2031,  -3.3594,  -3.0625,  -8.3750,  -9.8750,  -5.5000,\n",
      "          -5.8750,  -5.8438,  -7.5938,  -7.1875,  -7.5938,  -6.9062,  -8.8750,\n",
      "          -5.3750,  -4.9375,  -6.4062,  -6.5000,  -5.4375,  -6.2500,  -6.6250,\n",
      "          -5.9375,  -6.9375,  -7.5000,  -5.1250,  -2.7188,  -4.6250,  -7.5625,\n",
      "          -8.4375,  -5.6875,  -4.8438,  -3.5156,  -5.1250,  -7.6562,  -4.2500,\n",
      "          -4.6562,  -8.6250,  -6.5625,  -6.5625,  -6.4062,  -6.9062,  -7.4062,\n",
      "          -6.8750,  -6.4062,  -5.7188,  -6.6875,  -6.8438,  -5.4375,  -7.1875,\n",
      "          -8.2500,  -8.1875,  -7.8125,  -8.1250,  -6.7500,  -5.8750,  -5.2812,\n",
      "          -4.7500,  -5.7500,  -7.9688,  -7.9375,  -8.2500,  -6.4688,  -6.0312,\n",
      "          -8.0000,  -7.9062,  -7.4688,  -5.3438,  -5.7500,  -7.5938,  -7.8750,\n",
      "          -9.7500,  -9.8125,  -2.9062,  -2.0000,  -8.8750,  -9.1875,  -9.7500,\n",
      "          -9.2500,  -9.5625,  -9.5625,  -9.8125,  -9.3750,  -8.6875,  -9.9375,\n",
      "          -7.5312,  -5.7188,  -8.6875,  -8.7500,  -9.7500,  -9.8125,  -9.9375,\n",
      "          -9.8750,  -9.9375,  -9.8125,  -9.8125,  -9.8125,  -9.7500,  -9.6875,\n",
      "          -9.8125,  -9.9375,  -9.6250,  -9.7500,  -9.5000,  -9.8125,  -2.2188,\n",
      "          -6.8438,  -8.3125,  -7.5625,  -8.0625,  -8.2500,  -8.8750,  -9.0000,\n",
      "          -9.5000,  -9.6875,  -9.6875,  -9.8125,  -9.7500,  -6.3438,  -3.9219,\n",
      "          -9.8125,  -9.8750,  -9.8750,  -9.8750,  -9.8125,  -9.7500,  -9.8125,\n",
      "          -9.7500,  -9.8750,  -9.7500,  -9.9375,  -9.8750,  -9.8750,  -9.8125,\n",
      "          -9.8750]], device='cuda:0', dtype=torch.float32,\n",
      "       grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 174\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 184, 174])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 174, 256]), Cross output shape: torch.Size([1, 184, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8008,  0.5352,  0.5508,  ...,  1.1641, -1.4453, -1.2891],\n",
      "        [ 2.2188,  0.0410, -1.1641,  ...,  1.2344, -1.2969, -1.5625],\n",
      "        [-0.6562, -0.5859,  1.5625,  ...,  2.2188,  1.0000,  1.3438]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  -33.0000,  -384.0000,   244.0000,  ...,   984.0000,\n",
      "            233.0000,  -112.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 23, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.],\n",
      "         [  7744.,  -2416., -19840.,  ...,   -448.,   7104., -22912.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 174, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.8008,  0.5352,  0.5508,  ...,  1.1641, -1.4453, -1.2891],\n",
      "        [ 2.2188,  0.0410, -1.1641,  ...,  1.2344, -1.2969, -1.5625],\n",
      "        [-0.6562, -0.5859,  1.5625,  ...,  2.2188,  1.0000,  1.3438]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 174]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 22]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 174, 184])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-9.6875, -7.9062, -3.9375, -9.5625, -9.6875, -9.7500, -9.6250, -9.6250,\n",
      "         -9.6875, -9.6250, -9.7500, -9.8125, -9.8125, -5.8438, -3.6562, -9.6875,\n",
      "         -9.6250, -9.3750, -9.6875, -9.6250, -9.6250, -9.5625, -9.6250, -9.6875,\n",
      "         -9.6250, -9.7500, -9.6875, -9.5625, -9.6875, -9.7500, -9.7500, -9.6875,\n",
      "         -9.6250, -9.6250, -9.6250, -9.5625,  0.1787, -3.4062, -3.5469, -3.9688,\n",
      "         -4.1562, -5.3125, -6.1562, -2.9844, -4.3438, -5.1250, -5.1875, -4.2812,\n",
      "         -1.4219, -1.5000, -0.5664, -2.5938, -4.9062, -3.5000, -3.4219, -3.8750,\n",
      "         -3.2969, -4.9688, -4.5625, -3.6094, -3.6719, -4.9375, -1.9375, -4.7500,\n",
      "         -4.3750, -4.2500, -4.8750, -4.1250, -6.2812, -4.1250, -4.7188, -5.0938,\n",
      "         -4.4062, -3.8594, -4.4688, -5.4688, -3.7500, -0.8203, -3.7812, -5.3750,\n",
      "         -3.7500, -3.9219, -4.8125, -2.9688, -4.0938, -7.8125, -4.4375, -4.6250,\n",
      "         -4.1250, -5.0000, -4.9688, -5.8750, -5.8750, -4.7500, -5.6875, -5.0938,\n",
      "         -6.6562, -7.0938, -6.4688, -5.3438, -5.4688,  2.2812, -1.6484, -1.7422,\n",
      "         -2.2188,  4.4375, -0.5898, -2.9062,  2.2812, 10.1250, -3.8125, -1.4844,\n",
      "          2.3281, -1.0625, -1.7969,  2.7188, -1.5234, -4.9375,  1.7031,  0.4238,\n",
      "          0.0630,  1.4297, -2.8125, -2.5000, -6.1250,  3.1562, -2.4062, -4.8750,\n",
      "         -3.7188, -5.4688, -8.5000, -9.6875, -4.9375, -7.0938, -6.7812, -8.2500,\n",
      "         -7.8438, -8.1250, -7.8750, -9.0000, -7.0938, -6.2812, -7.6250, -7.3125,\n",
      "         -6.9688, -7.3125, -8.0625, -7.0312, -7.8438, -7.6562, -7.0312, -6.4375,\n",
      "         -6.4062, -8.6250, -8.5625, -7.6562, -7.2188, -6.0938, -7.5312, -8.2500,\n",
      "         -5.9062, -6.4375, -8.6875, -7.1875, -7.6875, -7.1562, -7.6562, -8.2500,\n",
      "         -7.8750, -6.9062, -7.6875, -7.7188, -7.7500, -6.2188, -7.7812, -8.5625,\n",
      "         -8.4375, -8.3750, -8.4375, -7.5312, -7.0312, -6.2188, -7.1250, -7.1875,\n",
      "         -8.5000, -8.3125, -8.5000, -7.6875, -7.0312, -8.4375, -8.3125, -7.8438,\n",
      "         -6.4688, -7.2188, -8.1250, -8.1875, -9.5625, -9.7500, -5.3750, -2.6719,\n",
      "         -9.1875, -9.0625, -9.5625, -9.3125, -9.6250, -9.0625, -9.6875, -9.1875,\n",
      "         -9.0000, -9.6250, -8.1875, -7.4062, -9.0625, -9.0000, -9.5000, -9.6875,\n",
      "         -9.6875, -9.7500, -9.6875, -9.7500, -9.6250, -9.6250, -9.6250, -9.6250,\n",
      "         -9.6250, -9.8125, -9.6875, -9.6875, -9.6250, -9.7500, -4.0312, -7.9062,\n",
      "         -8.5000, -8.4375, -8.2500, -8.8125, -8.8750, -9.3125, -9.4375, -9.5000,\n",
      "         -9.4375, -9.6250, -9.6875, -7.8125, -6.7812, -9.6875, -9.8125, -9.6250,\n",
      "         -9.6250, -9.6875, -9.6250, -9.7500, -9.6875, -9.7500, -9.6250, -9.7500,\n",
      "         -9.7500, -9.8750, -9.6250, -9.6875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 175\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 184, 175])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 175, 256]), Cross output shape: torch.Size([1, 184, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.2188,  0.0410, -1.1641,  ...,  1.2344, -1.2969, -1.5625],\n",
      "        [-0.6562, -0.5859,  1.5625,  ...,  2.2188,  1.0000,  1.3438],\n",
      "        [ 1.1875,  0.6797,  0.5977,  ...,  0.3984, -0.4531, -0.0107]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  -34.5000,  -420.0000,   270.0000,  ...,  1024.0000,\n",
      "            253.0000,  -128.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 23, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.],\n",
      "         [  8448.,  -2576., -21760.,  ...,   -564.,   7744., -24832.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 175, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 2.2188,  0.0410, -1.1641,  ...,  1.2344, -1.2969, -1.5625],\n",
      "        [-0.6562, -0.5859,  1.5625,  ...,  2.2188,  1.0000,  1.3438],\n",
      "        [ 1.1875,  0.6797,  0.5977,  ...,  0.3984, -0.4531, -0.0107]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 175]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 22]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 175, 184])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.9375, -7.0000, -1.1016, -5.9062, -6.0000, -6.0312, -6.0312, -6.0000,\n",
      "         -5.8750, -5.9688, -6.0938, -6.0625, -5.9375, -4.4062,  0.3574, -6.0625,\n",
      "         -6.0312, -5.6250, -5.9688, -6.0000, -5.9688, -6.0625, -6.0000, -5.9062,\n",
      "         -5.9375, -5.9688, -5.9688, -5.9375, -5.9688, -6.0000, -6.0000, -5.9062,\n",
      "         -5.9375, -5.9688, -6.0000, -5.9375,  1.3438, -1.0391, -2.2500, -2.4375,\n",
      "         -2.2188, -4.0312, -4.3125, -2.0469, -2.0625, -3.5469, -2.7500, -2.4219,\n",
      "          0.1826,  0.3613,  1.1875, -1.4609, -5.3750, -3.5156, -4.3750, -4.2500,\n",
      "         -2.9688, -3.8594, -2.2969, -2.3281, -3.2188, -3.4219, -1.0859, -1.5547,\n",
      "         -3.7656, -4.4062, -2.5938, -1.8438, -3.3906, -4.5000, -3.2969, -3.7969,\n",
      "         -2.5156, -3.4219, -3.1719, -4.0312, -5.0000, -2.2031, -2.3750, -4.9375,\n",
      "          1.7578, -4.1875, -3.4531, -2.3906, -1.1641, -4.5938, -2.9531, -3.1719,\n",
      "         -2.6875, -4.9062, -4.0625, -3.3750, -3.0781, -3.0938, -3.8438, -3.7812,\n",
      "         -3.7344, -3.9688, -5.2812, -2.7031, -2.8906,  0.4414,  0.0184,  0.7305,\n",
      "          1.8516,  1.0156,  1.7266, -2.5000, -3.0156, -0.6992, -2.5469, -1.8359,\n",
      "         11.7500,  0.6641,  3.3438,  0.6289,  2.5312, -2.3125,  2.0312,  1.7891,\n",
      "         -0.9102, -0.4141, -0.5469, -2.7031, -1.5000, -1.1172, -1.4141, -4.4688,\n",
      "         -3.1250, -4.0312, -5.2188, -6.0938, -4.5625, -4.3125, -4.1250, -5.3438,\n",
      "         -4.8750, -5.3125, -4.4375, -5.5625, -3.8125, -4.7188, -4.6875, -4.7188,\n",
      "         -5.5000, -5.0938, -5.2188, -4.5938, -5.0000, -4.8750, -4.9688, -4.0312,\n",
      "         -4.2812, -5.2812, -5.3438, -5.0000, -4.4688, -3.5938, -5.0000, -5.1875,\n",
      "         -3.9531, -6.5625, -5.5625, -5.7812, -5.0000, -4.9062, -5.4375, -5.4688,\n",
      "         -5.1250, -4.7188, -4.9688, -5.1875, -5.2188, -4.9062, -5.0625, -5.4062,\n",
      "         -5.2500, -5.4375, -5.2188, -4.4688, -4.1562, -4.7188, -4.3750, -5.1250,\n",
      "         -5.2188, -5.3125, -5.1562, -4.4062, -5.2812, -5.4062, -5.2500, -4.9688,\n",
      "         -5.1875, -5.6562, -5.2812, -5.3750, -5.9062, -5.9375, -3.0781, -2.4844,\n",
      "         -5.5625, -5.5625, -6.0000, -5.6875, -5.8438, -5.9062, -5.9688, -5.8750,\n",
      "         -5.5938, -5.9688, -4.9375, -4.8750, -5.4688, -5.5000, -5.9688, -5.8750,\n",
      "         -6.0938, -5.8438, -5.9375, -5.9375, -5.8750, -5.9375, -5.9375, -5.9375,\n",
      "         -5.9375, -5.9688, -5.9062, -6.0312, -5.7812, -6.0000, -2.9688, -5.0312,\n",
      "         -5.2188, -5.2812, -5.4062, -5.3750, -5.5000, -5.8750, -5.7812, -5.8750,\n",
      "         -5.7500, -5.9688, -5.9375, -5.2188, -4.5938, -6.0000, -6.0625, -5.8750,\n",
      "         -5.9062, -5.9688, -6.0000, -5.8750, -5.9688, -6.0625, -6.0312, -5.9688,\n",
      "         -6.0000, -6.1250, -5.8750, -5.9375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 176\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  6]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 184, 176])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 176, 256]), Cross output shape: torch.Size([1, 184, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.6562, -0.5859,  1.5625,  ...,  2.2188,  1.0000,  1.3438],\n",
      "        [ 1.1875,  0.6797,  0.5977,  ...,  0.3984, -0.4531, -0.0107],\n",
      "        [-0.0449,  1.4844, -0.6875,  ...,  2.3438,  0.4551, -0.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  -53.0000,  -448.0000,   296.0000,  ...,  1056.0000,\n",
      "            272.0000,  -150.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 23, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.],\n",
      "         [  8832.,  -2608., -22784.,  ...,   -640.,   8096., -25984.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 176, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.6562, -0.5859,  1.5625,  ...,  2.2188,  1.0000,  1.3438],\n",
      "        [ 1.1875,  0.6797,  0.5977,  ...,  0.3984, -0.4531, -0.0107],\n",
      "        [-0.0449,  1.4844, -0.6875,  ...,  2.3438,  0.4551, -0.4219]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 176]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 22]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 176, 184])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.4688, -6.1562,  3.5156, -7.3125, -7.4062, -7.3438, -7.2812, -7.4062,\n",
      "         -7.4062, -7.4688, -7.3750, -7.3438, -7.2500, -3.7344,  3.8438, -7.4375,\n",
      "         -7.3750, -6.8125, -7.4375, -7.3750, -7.2812, -7.3750, -7.4375, -7.2500,\n",
      "         -7.3438, -7.4062, -7.3438, -7.4062, -7.3125, -7.3750, -7.5000, -7.4062,\n",
      "         -7.3125, -7.3750, -7.3438, -7.3438,  6.0938,  4.1562,  0.8672, -2.5312,\n",
      "         -2.2969, -3.4844, -2.8906, -0.5234, -1.3750,  0.3398,  0.6523, -1.9297,\n",
      "          6.1562,  2.4375,  7.7812, -0.3262, -3.5312, -2.7969, -4.3750, -2.7344,\n",
      "         -2.8125, -2.5781, -2.2969, -3.3281, -2.9688, -3.3281,  1.5781,  2.0938,\n",
      "         -2.0781, -1.9844, -1.9141,  1.2812, -2.8906, -2.7031, -3.3125, -2.6719,\n",
      "         -1.4219, -1.9297, -2.9375, -2.6250, -3.8438, -2.3125, -2.2344, -3.1875,\n",
      "         -3.4688, -3.2656, -2.7656, -1.3672, -3.0781, -3.6250, -3.1562, -0.4453,\n",
      "         -2.9062, -2.5312, -2.6562, -3.0000, -2.5000, -2.5000, -2.5781, -2.9688,\n",
      "         -2.7031,  1.5156, -2.8750, -3.0469, -1.4531,  0.5430, -2.7812, -2.8750,\n",
      "          0.7539,  1.9062, -2.3438, -1.4609, -1.8359,  1.1719, -2.7969, -2.7656,\n",
      "          0.8047, -1.9141, -0.5938,  1.5078, -2.8438, -2.1719, -1.7422,  3.5938,\n",
      "         -1.1094, -0.1279, -1.9141, -2.5625, -2.9219, -0.5820, -2.2812, -3.6406,\n",
      "         -2.6875, -1.3828, -5.9375, -7.5000, -3.9375, -4.3438, -3.9688, -5.4062,\n",
      "         -5.4062, -5.3438, -5.5000, -6.6562, -3.0000, -4.2500, -5.1250, -4.7500,\n",
      "         -4.2500, -4.4062, -4.7500, -4.7188, -5.0312, -5.3750, -4.5000, -1.8125,\n",
      "         -2.5938, -5.8125, -6.3438, -3.5625, -4.3125, -1.1719, -3.7812, -5.6875,\n",
      "         -3.8438, -4.9375, -6.6562, -4.6250, -5.1875, -4.6562, -4.5000, -5.7500,\n",
      "         -5.1250, -4.7188, -4.3750, -4.9688, -4.9062, -3.1406, -5.4688, -6.0938,\n",
      "         -5.9062, -5.8438, -6.0312, -4.5000, -3.9688, -3.9219, -3.8438, -4.1875,\n",
      "         -5.8125, -5.9688, -5.6875, -4.3750, -4.9688, -5.9688, -5.4688, -5.5000,\n",
      "         -4.5000, -5.0625, -5.5312, -5.6250, -7.3438, -7.4375, -2.7969, -1.5547,\n",
      "         -6.7188, -6.6562, -7.4375, -7.0000, -7.1875, -7.4062, -7.4375, -7.0312,\n",
      "         -6.8438, -7.4062, -5.1250, -4.1250, -6.5000, -6.5312, -7.3438, -7.2812,\n",
      "         -7.5000, -7.3750, -7.3438, -7.3750, -7.5312, -7.3750, -7.3750, -7.3750,\n",
      "         -7.3125, -7.3125, -7.3750, -7.3750, -7.0938, -7.5312, -0.8750, -5.1875,\n",
      "         -5.9688, -5.7500, -5.8438, -6.6250, -6.5625, -6.8125, -7.1562, -7.4062,\n",
      "         -7.1875, -7.5312, -7.3438, -5.1875, -3.1875, -7.4688, -7.4375, -7.3125,\n",
      "         -7.3750, -7.5000, -7.4375, -7.3438, -7.3125, -7.4688, -7.5000, -7.4375,\n",
      "         -7.4688, -7.4375, -7.3750, -7.4375]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 177\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 184, 177])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 177, 256]), Cross output shape: torch.Size([1, 184, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.1875,  0.6797,  0.5977,  ...,  0.3984, -0.4531, -0.0107],\n",
      "        [-0.0449,  1.4844, -0.6875,  ...,  2.3438,  0.4551, -0.4219],\n",
      "        [-1.0000, -0.2559,  0.8633,  ..., -0.2129,  0.1895, -0.9609]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -62.5000,  -508.0000,   348.0000,  ...,  1136.0000,\n",
      "            292.0000,  -206.0000],\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  -79.0000,  -496.0000,   350.0000,  ...,  1104.0000,\n",
      "            296.0000,  -182.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 23, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9856.,  -3040., -25344.,  ...,   -820.,   9088., -28928.],\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.],\n",
      "         [  9600.,  -2672., -24704.,  ...,   -816.,   8832., -27904.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 177, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 1.1875,  0.6797,  0.5977,  ...,  0.3984, -0.4531, -0.0107],\n",
      "        [-0.0449,  1.4844, -0.6875,  ...,  2.3438,  0.4551, -0.4219],\n",
      "        [-1.0000, -0.2559,  0.8633,  ..., -0.2129,  0.1895, -0.9609]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 177]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 177, 184])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-7.1250e+00, -5.0000e+00,  1.1500e+01, -7.0938e+00, -7.1250e+00,\n",
      "         -7.0938e+00, -7.1250e+00, -7.0938e+00, -7.0625e+00, -7.1562e+00,\n",
      "         -7.0938e+00, -7.1250e+00, -7.0625e+00, -2.8125e+00,  5.6250e+00,\n",
      "         -7.0938e+00, -7.1250e+00, -6.6562e+00, -7.1875e+00, -7.1250e+00,\n",
      "         -7.0000e+00, -7.0625e+00, -7.0625e+00, -7.0625e+00, -7.0312e+00,\n",
      "         -7.0000e+00, -7.1250e+00, -7.0625e+00, -7.0312e+00, -7.0625e+00,\n",
      "         -7.0625e+00, -7.0312e+00, -7.0938e+00, -7.0625e+00, -7.0000e+00,\n",
      "         -7.1875e+00,  6.8438e+00,  2.8125e-01,  3.1250e+00, -1.6016e+00,\n",
      "         -3.5625e+00, -3.3594e+00, -3.3125e+00, -2.0625e+00, -1.8516e+00,\n",
      "          1.5234e+00, -1.4062e-01, -1.5078e+00,  2.9375e+00,  4.4336e-01,\n",
      "          3.3281e+00,  6.8359e-01, -1.6797e+00, -2.5000e-01, -1.4219e+00,\n",
      "         -5.0391e-01, -5.5469e-01, -3.7109e-01, -1.1094e+00, -1.5312e+00,\n",
      "         -2.0312e+00, -1.6250e+00,  2.8687e-02,  1.5332e-01,  6.1279e-02,\n",
      "         -2.0469e+00, -1.5625e+00, -3.6094e+00, -3.1719e+00, -1.5156e+00,\n",
      "         -1.7891e+00, -1.2422e+00, -1.7969e+00, -2.5781e+00, -2.1406e+00,\n",
      "         -1.8594e+00, -7.7344e-01, -1.1953e+00, -1.1484e+00, -2.3906e+00,\n",
      "         -1.8672e+00, -2.4375e+00, -2.5000e+00, -1.4453e+00, -1.6484e+00,\n",
      "         -2.2969e+00, -1.0156e+00, -6.4844e-01, -9.8047e-01, -2.7344e+00,\n",
      "         -2.2188e+00, -2.7031e+00, -3.8594e+00, -2.7656e+00, -2.5469e+00,\n",
      "         -3.6250e+00, -2.6094e+00, -1.3672e+00, -2.2344e+00, -1.9062e+00,\n",
      "         -2.5781e+00, -2.8516e-01, -1.0234e+00, -1.4572e-03, -5.4297e-01,\n",
      "         -8.8672e-01, -8.4766e-01, -1.3281e-01, -1.2109e+00, -3.9062e-01,\n",
      "          1.9219e+00, -2.2188e+00, -7.3047e-01, -1.7578e+00, -1.8594e+00,\n",
      "         -1.1172e+00, -1.2188e+00, -1.8750e+00, -1.5859e+00, -9.1016e-01,\n",
      "         -1.3281e+00, -1.6406e+00, -1.2188e+00, -1.5625e+00,  5.9375e-01,\n",
      "          7.1777e-02, -2.0000e+00, -3.0156e+00,  2.1289e-01, -2.7656e+00,\n",
      "         -6.0938e+00, -7.1875e+00, -3.3594e+00, -4.0938e+00, -4.1562e+00,\n",
      "         -5.8125e+00, -5.6250e+00, -5.8438e+00, -4.6562e+00, -6.1250e+00,\n",
      "         -3.3125e+00, -3.3125e+00, -4.6250e+00, -4.5938e+00, -3.9375e+00,\n",
      "         -4.1875e+00, -4.8125e+00, -4.8750e+00, -5.2500e+00, -5.4062e+00,\n",
      "         -3.5156e+00, -1.6250e+00, -3.1094e+00, -5.5938e+00, -5.7812e+00,\n",
      "         -3.9531e+00, -3.6875e+00, -3.3125e+00, -3.8125e+00, -5.4375e+00,\n",
      "         -3.3750e+00, -2.9531e+00, -6.1250e+00, -2.7188e+00, -4.5312e+00,\n",
      "         -4.9688e+00, -4.5312e+00, -5.6250e+00, -4.8125e+00, -4.0938e+00,\n",
      "         -4.3750e+00, -4.6250e+00, -5.0000e+00, -3.3125e+00, -5.3750e+00,\n",
      "         -6.1250e+00, -5.8750e+00, -5.8750e+00, -5.7812e+00, -4.9062e+00,\n",
      "         -4.0625e+00, -4.2500e+00, -2.8906e+00, -3.8750e+00, -6.0000e+00,\n",
      "         -6.2188e+00, -5.8438e+00, -4.7812e+00, -4.3125e+00, -5.7812e+00,\n",
      "         -5.6875e+00, -5.2812e+00, -4.1875e+00, -4.4688e+00, -5.3438e+00,\n",
      "         -5.7188e+00, -7.0625e+00, -7.0625e+00, -1.2188e+00, -1.0078e+00,\n",
      "         -6.3125e+00, -6.2188e+00, -7.0000e+00, -6.5312e+00, -6.9062e+00,\n",
      "         -6.9062e+00, -7.0938e+00, -6.7812e+00, -6.2812e+00, -7.1562e+00,\n",
      "         -5.4375e+00, -4.3438e+00, -6.5625e+00, -6.5312e+00, -7.0938e+00,\n",
      "         -7.1562e+00, -7.1875e+00, -7.0938e+00, -7.0625e+00, -7.1875e+00,\n",
      "         -7.1250e+00, -7.1250e+00, -7.0625e+00, -7.0312e+00, -7.1250e+00,\n",
      "         -7.0938e+00, -7.0000e+00, -7.0625e+00, -6.8438e+00, -7.0625e+00,\n",
      "         -1.5312e+00, -5.2500e+00, -5.9062e+00, -5.4688e+00, -5.7500e+00,\n",
      "         -5.8438e+00, -6.3438e+00, -6.5625e+00, -6.9062e+00, -7.0625e+00,\n",
      "         -7.0938e+00, -7.0938e+00, -7.0938e+00, -4.7500e+00, -1.1328e+00,\n",
      "         -7.2188e+00, -7.0000e+00, -7.0312e+00, -7.1250e+00, -7.1250e+00,\n",
      "         -7.0312e+00, -7.1562e+00, -7.0000e+00, -7.0625e+00, -7.1875e+00,\n",
      "         -7.2188e+00, -7.0625e+00, -7.0625e+00, -7.1250e+00, -7.0938e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 178\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 192, 178])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 178, 256]), Cross output shape: torch.Size([1, 192, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0449,  1.4844, -0.6875,  ...,  2.3438,  0.4551, -0.4219],\n",
      "        [-1.0000, -0.2559,  0.8633,  ..., -0.2129,  0.1895, -0.9609],\n",
      "        [-0.7969, -1.0312, -0.5000,  ...,  0.9453,  2.3125,  0.3125]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 24, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -39.5000,  -392.0000,   240.0000,  ...,   960.0000,\n",
      "            231.0000,  -121.0000],\n",
      "         [  -79.0000,  -496.0000,   350.0000,  ...,  1104.0000,\n",
      "            296.0000,  -182.0000],\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 24, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  7936.,  -2288., -20224.,  ...,   -592.,   7296., -23040.],\n",
      "         [  9600.,  -2672., -24704.,  ...,   -816.,   8832., -27904.],\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 178, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.0449,  1.4844, -0.6875,  ...,  2.3438,  0.4551, -0.4219],\n",
      "        [-1.0000, -0.2559,  0.8633,  ..., -0.2129,  0.1895, -0.9609],\n",
      "        [-0.7969, -1.0312, -0.5000,  ...,  0.9453,  2.3125,  0.3125]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 178]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 178, 192])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[ 2.3281e+00,  1.9125e+01,  3.5156e+00,  2.3594e+00,  2.3125e+00,\n",
      "          2.3281e+00,  2.3594e+00,  2.3281e+00,  2.3438e+00,  2.3750e+00,\n",
      "          2.2188e+00,  2.3750e+00,  2.2812e+00,  2.3906e+00,  5.2344e-01,\n",
      "          2.2656e+00,  2.2656e+00,  2.3750e+00,  2.3438e+00,  2.3438e+00,\n",
      "          2.2188e+00,  2.4688e+00,  2.5469e+00,  2.5000e+00,  2.3281e+00,\n",
      "          2.3281e+00,  2.2656e+00,  2.2969e+00,  2.2344e+00,  2.3281e+00,\n",
      "          2.2969e+00,  2.3594e+00,  2.3906e+00,  2.2500e+00,  2.2812e+00,\n",
      "          2.2656e+00,  1.5625e-01,  4.8438e+00,  1.4453e+00,  3.3125e+00,\n",
      "          1.5312e+00,  2.3750e+00,  1.4922e+00,  2.8906e-01, -3.1250e-01,\n",
      "         -3.2812e-01,  4.3359e-01,  9.5312e-01,  7.8516e-01, -5.1025e-02,\n",
      "          2.5156e+00,  5.2188e+00,  6.4062e-01, -3.7109e-01, -5.7422e-01,\n",
      "          2.7344e-01,  2.2500e+00,  9.2188e-01,  1.0234e+00,  1.6484e+00,\n",
      "          9.8438e-01,  2.7344e-01,  1.2969e+00,  2.3906e+00,  4.3438e+00,\n",
      "          1.0625e+00,  3.4219e+00,  2.1562e+00,  2.5156e+00,  7.0312e-01,\n",
      "          2.0938e+00,  1.9922e-01,  5.4688e-01,  7.2656e-01,  1.4688e+00,\n",
      "          1.7734e+00,  2.2188e+00,  3.6406e+00,  5.7422e-01,  8.6914e-02,\n",
      "          5.7812e-01,  8.9355e-02,  3.0156e+00,  6.1328e-01,  2.2188e+00,\n",
      "          3.3008e-01,  1.2500e+00,  1.0840e-01,  8.2422e-01,  2.1250e+00,\n",
      "          9.2285e-02,  4.6484e-01, -1.4709e-02,  2.9375e+00,  6.4844e-01,\n",
      "          3.2422e-01,  1.5859e+00,  3.0078e-01,  3.3398e-01,  1.7480e-01,\n",
      "          6.2109e-01, -1.5859e+00,  6.2109e-01, -4.6875e-01, -9.3359e-01,\n",
      "         -4.8438e-01,  1.1953e+00,  3.9453e-01,  4.1406e-01,  5.5625e+00,\n",
      "          6.8750e-01,  1.7188e+00, -5.2734e-01, -1.6250e+00,  1.8984e+00,\n",
      "          1.2988e-01,  1.1182e-01,  5.3711e-02,  4.3945e-01, -1.7188e+00,\n",
      "         -2.7344e+00,  6.9531e-01,  1.1865e-01, -4.5117e-01, -5.0000e-01,\n",
      "          5.8594e-01, -1.3516e+00,  2.6094e+00,  2.0781e+00,  2.1875e+00,\n",
      "          2.2656e+00,  2.3281e+00,  4.5000e+00,  3.2031e+00,  3.0000e+00,\n",
      "          2.9219e+00,  2.5156e+00,  2.8281e+00,  2.7031e+00,  2.6562e+00,\n",
      "          1.6797e+00,  2.9375e+00,  2.3594e+00,  2.6406e+00,  3.8125e+00,\n",
      "          3.0000e+00,  3.3906e+00,  2.8438e+00,  2.8594e+00,  3.0000e+00,\n",
      "          3.7188e+00,  3.3125e+00,  3.1250e+00,  2.9062e+00,  2.5000e+00,\n",
      "          3.1875e+00,  2.8438e+00,  1.9062e+00,  3.0312e+00,  2.9844e+00,\n",
      "          4.0000e+00,  3.0781e+00,  2.3281e+00,  3.7031e+00,  2.9062e+00,\n",
      "          3.2188e+00,  3.1562e+00,  3.3281e+00,  2.5938e+00,  2.7969e+00,\n",
      "          1.5781e+00,  3.1094e+00,  3.2969e+00,  3.9531e+00,  2.6094e+00,\n",
      "          2.5312e+00,  2.5781e+00,  2.7344e+00,  2.5938e+00,  3.1406e+00,\n",
      "          3.4531e+00,  3.0469e+00,  3.8281e+00,  3.1094e+00,  2.4688e+00,\n",
      "          2.5938e+00,  2.8125e+00,  3.2500e+00,  4.0938e+00,  2.7969e+00,\n",
      "          2.9219e+00,  2.6875e+00,  3.8438e+00,  4.2188e+00,  2.9844e+00,\n",
      "          2.6875e+00,  2.3750e+00,  2.3281e+00,  1.0156e+00,  3.4531e+00,\n",
      "          2.5312e+00,  2.5000e+00,  2.3750e+00,  2.4531e+00,  2.4688e+00,\n",
      "          2.3750e+00,  2.4062e+00,  2.4219e+00,  2.5938e+00,  2.3906e+00,\n",
      "          2.7656e+00,  2.3594e+00,  2.2969e+00,  2.4375e+00,  2.4531e+00,\n",
      "          2.3125e+00,  2.2656e+00,  2.3281e+00,  2.4844e+00,  2.3438e+00,\n",
      "          2.3281e+00,  2.3906e+00,  2.2500e+00,  2.2188e+00,  2.4062e+00,\n",
      "          2.3438e+00,  2.2656e+00,  2.3594e+00,  2.4531e+00,  2.3125e+00,\n",
      "          9.2285e-02,  3.0156e+00,  2.8438e+00,  2.8125e+00,  2.7344e+00,\n",
      "          2.5625e+00,  2.5000e+00,  2.4375e+00,  2.2969e+00,  2.1562e+00,\n",
      "          2.4062e+00,  2.2656e+00,  2.3438e+00,  3.4531e+00,  3.0625e+00,\n",
      "          2.3594e+00,  2.2812e+00,  2.3750e+00,  2.2344e+00,  2.3281e+00,\n",
      "          2.2344e+00,  2.2969e+00,  2.2812e+00,  2.2500e+00,  2.2188e+00,\n",
      "          2.3125e+00,  2.2344e+00,  2.2344e+00,  2.3281e+00,  2.5000e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 179\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 200, 179])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 179, 256]), Cross output shape: torch.Size([1, 200, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0000, -0.2559,  0.8633,  ..., -0.2129,  0.1895, -0.9609],\n",
      "        [-0.7969, -1.0312, -0.5000,  ...,  0.9453,  2.3125,  0.3125],\n",
      "        [-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 25, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [  -79.0000,  -496.0000,   350.0000,  ...,  1104.0000,\n",
      "            296.0000,  -182.0000],\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000],\n",
      "         [  712.0000,   -53.2500,   200.0000,  ...,  1168.0000,\n",
      "           -268.0000,    84.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 25, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [  9600.,  -2672., -24704.,  ...,   -816.,   8832., -27904.],\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 179, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.0000, -0.2559,  0.8633,  ..., -0.2129,  0.1895, -0.9609],\n",
      "        [-0.7969, -1.0312, -0.5000,  ...,  0.9453,  2.3125,  0.3125],\n",
      "        [-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 179]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 179, 200])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-3.8906, -1.5312,  0.3711, -3.9219, -3.9531, -3.9844, -3.8750, -3.9219,\n",
      "         -3.8906, -3.9219, -4.0000, -3.9375, -3.9375, -1.5625,  1.3359, -3.9688,\n",
      "         -3.9844, -3.6094, -3.9219, -3.9688, -3.9688, -3.9531, -4.0000, -3.8906,\n",
      "         -3.9844, -3.9375, -3.9219, -3.9688, -3.9531, -3.8906, -3.9375, -3.9375,\n",
      "         -3.8906, -3.8438, -3.8125, -3.9531,  1.2344,  0.0396,  1.1250, 13.3750,\n",
      "         -1.2812, -0.3418, -0.0962, -0.6211, -0.0145,  1.5625, -1.1172, -0.3105,\n",
      "         -0.7148,  1.2031,  0.3887, -2.6250,  0.0623,  0.9219,  0.9766,  0.6758,\n",
      "         -0.2070,  0.0879, -0.3789,  1.1250,  0.0962,  0.1621, -1.3359, -1.9375,\n",
      "         -2.1719,  1.0234, -2.0469, -0.6094, -1.6328, -0.4141,  0.1689, -0.3633,\n",
      "          0.2871,  0.7148,  0.4883,  0.8984, -0.2578, -0.8906, -1.8203, -0.3711,\n",
      "          0.6641, -0.6680, -0.8633,  0.5312,  0.3809, -0.3828,  0.3945,  1.9062,\n",
      "          0.2266, -0.6523, -0.5508,  0.3887, -0.0557,  0.4141,  0.0967,  0.6250,\n",
      "         -2.5781, -0.7070, -1.4375,  0.0315, -1.1094,  0.9414,  0.7578, -0.3926,\n",
      "         -1.2422,  0.0354,  0.3398,  0.3379, -0.4902, -0.1504, -2.3594, -0.8711,\n",
      "         -0.7578, -0.0417, -1.1953, -0.7500,  0.0591, -0.9258, -0.1201, -0.4570,\n",
      "         -1.0703, -0.6289, -0.9453, -0.9531,  0.1157,  0.8125, -1.1016, -1.9141,\n",
      "         -1.2734, -2.7969, -3.1406, -3.9531, -0.7695, -1.4375, -1.6562, -2.7812,\n",
      "         -2.4062, -2.3125, -2.2812, -3.2656, -0.5859, -0.5117, -2.1250, -1.8047,\n",
      "         -1.9688, -1.8984, -2.3594, -2.1094, -2.4375, -2.6875, -1.5391, -1.2734,\n",
      "         -1.6797, -2.8750, -3.0000, -0.9102, -1.1953, -1.3047, -0.9062, -2.6875,\n",
      "         -0.8281, -1.8828, -3.2500, -1.7344, -1.9844, -2.0938, -1.7422, -2.4531,\n",
      "         -1.9375, -1.5547, -1.5391, -1.8828, -2.3281, -1.1641, -2.3750, -2.9531,\n",
      "         -3.0156, -3.0938, -2.8906, -1.8203, -1.1094, -1.1250, -1.7422, -1.7891,\n",
      "         -2.7344, -2.9219, -2.9062, -1.8828, -1.2422, -2.8281, -2.5938, -2.2656,\n",
      "         -1.5469, -2.0781, -2.7188, -2.7969, -3.8750, -3.9375, -1.0312, -1.0938,\n",
      "         -3.2656, -3.5469, -3.9688, -3.4062, -3.7656, -3.6250, -4.0000, -3.7656,\n",
      "         -3.2969, -3.9375, -2.4219, -2.0156, -3.2344, -3.3438, -3.9219, -3.9062,\n",
      "         -4.0000, -3.8750, -3.9375, -3.8906, -3.9375, -3.9062, -3.9062, -3.8750,\n",
      "         -3.9375, -3.9531, -3.9844, -3.9219, -3.8750, -3.9688,  0.6172, -2.2969,\n",
      "         -2.9531, -2.5156, -2.7812, -3.0312, -3.2969, -3.4531, -3.8281, -3.8438,\n",
      "         -3.8750, -3.9375, -3.9219, -1.9141,  1.3906, -4.0312, -3.8906, -3.9375,\n",
      "         -3.9219, -3.8438, -3.8906, -3.8906, -3.9844, -3.9531, -3.9375, -3.9375,\n",
      "         -3.8750, -3.8906, -3.9531, -3.9219]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 180\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24, 25]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1,  1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 208, 180])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 180, 256]), Cross output shape: torch.Size([1, 208, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.7969, -1.0312, -0.5000,  ...,  0.9453,  2.3125,  0.3125],\n",
      "        [-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.2969,  2.3438, -1.6250,  ...,  0.1934,  2.8125, -1.4844]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000],\n",
      "         [  712.0000,   -53.2500,   200.0000,  ...,  1168.0000,\n",
      "           -268.0000,    84.0000],\n",
      "         [  704.0000,   -80.0000,   254.0000,  ...,  1152.0000,\n",
      "           -320.0000,    94.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 26, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.],\n",
      "         [  2080., -11584.,  -5280.,  ...,   4800.,    440., -17664.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 180, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-0.7969, -1.0312, -0.5000,  ...,  0.9453,  2.3125,  0.3125],\n",
      "        [-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.2969,  2.3438, -1.6250,  ...,  0.1934,  2.8125, -1.4844]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 180]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24, 25]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 180, 208])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-3.6406, -1.2109,  0.4570, -3.6875, -3.7188, -3.7500, -3.6562, -3.6719,\n",
      "         -3.6562, -3.6875, -3.7812, -3.7031, -3.7031, -1.2891,  1.3750, -3.7188,\n",
      "         -3.7656, -3.3750, -3.6875, -3.7188, -3.7500, -3.7188, -3.7656, -3.6562,\n",
      "         -3.7500, -3.7031, -3.6875, -3.7344, -3.7188, -3.6562, -3.7031, -3.6875,\n",
      "         -3.6562, -3.6094, -3.5938, -3.7188,  1.1797,  0.1650,  1.2422, 13.6250,\n",
      "         -1.1328, -0.1885,  0.0894, -0.4141,  0.0557,  1.6328, -1.0156, -0.3066,\n",
      "         -0.7266,  1.2578,  0.4062, -2.5625,  0.1709,  0.9375,  1.0000,  0.7539,\n",
      "         -0.0977,  0.1631, -0.3535,  1.1406,  0.2188,  0.2734, -1.2266, -1.8438,\n",
      "         -2.0781,  1.1641, -1.8906, -0.5664, -1.3750, -0.3984,  0.2676, -0.3086,\n",
      "          0.3926,  0.7422,  0.5430,  0.9922, -0.3008, -0.8594, -1.6016, -0.1006,\n",
      "          0.8047, -0.5430, -0.7969,  0.4902,  0.4121, -0.1426,  0.4688,  1.9219,\n",
      "          0.1719, -0.5938, -0.4082,  0.4395,  0.1924,  0.4453,  0.3867,  0.8047,\n",
      "         -2.2344, -0.5703, -1.2734,  0.2402, -0.9062,  0.7891,  0.9336, -0.4023,\n",
      "         -1.2578,  0.0698,  0.3477,  0.5039, -0.5078, -0.1611, -2.1094, -0.6445,\n",
      "         -0.6445,  0.0247, -1.0938, -0.7734,  0.0269, -0.8281, -0.0737, -0.4570,\n",
      "         -1.1328, -0.5156, -0.9258, -0.9570,  0.1504,  0.9336, -0.7617, -1.7266,\n",
      "         -1.1172, -2.5312, -2.9375, -3.7188, -0.5234, -1.2109, -1.4297, -2.5469,\n",
      "         -2.1562, -2.1094, -2.0625, -3.0156, -0.3379, -0.2910, -1.8984, -1.6016,\n",
      "         -1.7578, -1.6406, -2.1094, -1.8750, -2.2031, -2.4531, -1.3203, -1.0156,\n",
      "         -1.4297, -2.6406, -2.7500, -0.7773, -0.9180, -1.0469, -0.6172, -2.4531,\n",
      "         -0.5391, -1.7578, -3.0156, -1.4766, -1.7500, -1.8672, -1.5312, -2.2344,\n",
      "         -1.7188, -1.3047, -1.3516, -1.6719, -2.0625, -0.9453, -2.1406, -2.7188,\n",
      "         -2.7656, -2.8750, -2.6562, -1.6172, -0.8281, -0.9062, -1.5469, -1.5703,\n",
      "         -2.5156, -2.7031, -2.6719, -1.6875, -0.9961, -2.5938, -2.3281, -2.0469,\n",
      "         -1.3359, -1.8516, -2.4844, -2.5312, -3.6562, -3.7031, -0.8242, -0.8438,\n",
      "         -3.0312, -3.3125, -3.7188, -3.1719, -3.5312, -3.3750, -3.7500, -3.5156,\n",
      "         -3.0469, -3.7031, -2.1562, -1.7109, -3.0000, -3.1250, -3.6875, -3.6719,\n",
      "         -3.7656, -3.6250, -3.7031, -3.6406, -3.7031, -3.6719, -3.6719, -3.6406,\n",
      "         -3.6875, -3.7344, -3.7500, -3.6875, -3.6250, -3.7344,  0.7852, -2.0469,\n",
      "         -2.7188, -2.2656, -2.5469, -2.7812, -3.0469, -3.2031, -3.5938, -3.6250,\n",
      "         -3.6406, -3.7031, -3.6719, -1.6641,  1.7188, -3.8125, -3.6719, -3.6875,\n",
      "         -3.6875, -3.6094, -3.6562, -3.6562, -3.7344, -3.7188, -3.7031, -3.7031,\n",
      "         -3.6406, -3.6562, -3.7188, -3.6875]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 181\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24, 25,\n",
      "         25]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1,  2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 208, 181])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 181, 256]), Cross output shape: torch.Size([1, 208, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.2969,  2.3438, -1.6250,  ...,  0.1934,  2.8125, -1.4844],\n",
      "        [-3.1406,  2.1719, -1.8438,  ..., -0.0898,  2.7656, -1.5156]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[ 8.4400e+02, -2.7500e+01,  7.8000e+01,  ...,  1.1360e+03,\n",
      "          -1.0480e+03,  2.2500e+01],\n",
      "         [ 1.1300e+02, -1.4400e+03,  1.2880e+03,  ...,  1.8720e+03,\n",
      "           7.2000e+02, -7.6000e+02],\n",
      "         [ 2.1300e+02, -4.4600e+02,  3.6000e+02,  ...,  1.2800e+03,\n",
      "          -7.2800e+02, -4.0000e+02],\n",
      "         ...,\n",
      "         [ 2.5000e+01, -2.6500e+01, -1.8750e+01,  ..., -1.4600e+02,\n",
      "          -2.6800e+02,  1.7400e+02],\n",
      "         [ 7.1200e+02, -5.3250e+01,  2.0000e+02,  ...,  1.1680e+03,\n",
      "          -2.6800e+02,  8.4000e+01],\n",
      "         [ 5.2800e+02, -3.0800e+02,  4.1600e+02,  ...,  1.3600e+03,\n",
      "          -4.7000e+01, -1.6562e+00]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 26, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.],\n",
      "         [  6528.,  -9984., -16896.,  ...,   3584.,   4512., -27904.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 181, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.2969,  2.3438, -1.6250,  ...,  0.1934,  2.8125, -1.4844],\n",
      "        [-3.1406,  2.1719, -1.8438,  ..., -0.0898,  2.7656, -1.5156]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 181]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24, 24,\n",
      "         25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 181, 208])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-3.6406e+00, -1.3672e+00,  4.0820e-01, -3.6875e+00, -3.7188e+00,\n",
      "         -3.7344e+00, -3.6719e+00, -3.6719e+00, -3.6562e+00, -3.6875e+00,\n",
      "         -3.7812e+00, -3.7188e+00, -3.7031e+00, -1.2969e+00,  1.5391e+00,\n",
      "         -3.7188e+00, -3.7656e+00, -3.4062e+00, -3.7031e+00, -3.7344e+00,\n",
      "         -3.7500e+00, -3.7188e+00, -3.7656e+00, -3.6562e+00, -3.7344e+00,\n",
      "         -3.7188e+00, -3.7031e+00, -3.7500e+00, -3.7344e+00, -3.6719e+00,\n",
      "         -3.7188e+00, -3.7031e+00, -3.6719e+00, -3.6094e+00, -3.6250e+00,\n",
      "         -3.7031e+00,  1.4375e+00,  3.5352e-01,  1.2891e+00,  1.3875e+01,\n",
      "         -1.1719e+00, -1.8750e-01, -8.6060e-03, -4.7070e-01,  7.3242e-02,\n",
      "          1.6328e+00, -8.7109e-01, -1.8652e-01, -5.5078e-01,  1.4766e+00,\n",
      "          4.5898e-01, -2.3281e+00,  2.1289e-01,  9.3359e-01,  1.0156e+00,\n",
      "          6.4844e-01, -1.3477e-01,  1.5625e-01, -4.0430e-01,  9.8828e-01,\n",
      "          1.2402e-01,  5.5908e-02, -1.1562e+00, -1.7422e+00, -2.2500e+00,\n",
      "          1.4297e+00, -1.8906e+00, -4.9023e-01, -1.4531e+00, -4.1602e-01,\n",
      "          1.2207e-01, -3.3203e-01,  3.4570e-01,  6.6016e-01,  4.4336e-01,\n",
      "          8.3984e-01, -4.0820e-01, -6.9141e-01, -1.5625e+00, -2.2070e-01,\n",
      "          6.1328e-01, -5.9375e-01, -8.6328e-01,  4.9414e-01,  4.7461e-01,\n",
      "         -1.2207e-01,  4.6289e-01,  1.8594e+00,  1.3281e-01, -7.0703e-01,\n",
      "         -4.2578e-01,  4.9805e-01,  5.6885e-02,  3.2422e-01,  2.3535e-01,\n",
      "          8.7109e-01, -2.2656e+00, -6.2500e-01, -1.4219e+00,  3.0078e-01,\n",
      "         -8.9062e-01,  6.9141e-01,  1.0625e+00, -4.3945e-01, -1.2031e+00,\n",
      "          8.2031e-02,  2.6758e-01,  2.6758e-01, -6.3281e-01, -3.2812e-01,\n",
      "         -2.0625e+00, -8.0078e-01, -6.1328e-01, -5.6641e-02, -1.0938e+00,\n",
      "         -7.3047e-01,  1.1414e-02, -9.3750e-01,  4.9561e-02, -2.3828e-01,\n",
      "         -1.1406e+00, -4.9414e-01, -9.4531e-01, -9.0625e-01,  2.6172e-01,\n",
      "          9.2188e-01, -8.9062e-01, -1.7031e+00, -1.1797e+00, -2.4688e+00,\n",
      "         -3.0156e+00, -3.7344e+00, -5.3125e-01, -1.2578e+00, -1.4766e+00,\n",
      "         -2.5781e+00, -2.1875e+00, -2.1406e+00, -2.0312e+00, -3.0000e+00,\n",
      "         -4.4727e-01, -3.0859e-01, -1.9141e+00, -1.6641e+00, -1.7500e+00,\n",
      "         -1.6641e+00, -2.1562e+00, -1.9844e+00, -2.2188e+00, -2.4531e+00,\n",
      "         -1.3203e+00, -1.1641e+00, -1.5156e+00, -2.6562e+00, -2.7031e+00,\n",
      "         -8.3203e-01, -9.4141e-01, -1.0312e+00, -6.0547e-01, -2.4844e+00,\n",
      "         -5.9766e-01, -1.6172e+00, -3.0312e+00, -1.4453e+00, -1.7969e+00,\n",
      "         -1.9453e+00, -1.5234e+00, -2.2969e+00, -1.7422e+00, -1.3672e+00,\n",
      "         -1.3359e+00, -1.7578e+00, -2.0781e+00, -1.0703e+00, -2.1875e+00,\n",
      "         -2.7500e+00, -2.7656e+00, -2.9219e+00, -2.6719e+00, -1.6875e+00,\n",
      "         -9.4531e-01, -9.3359e-01, -1.7031e+00, -1.6484e+00, -2.5781e+00,\n",
      "         -2.7656e+00, -2.6875e+00, -1.7500e+00, -1.0312e+00, -2.6250e+00,\n",
      "         -2.3750e+00, -2.0938e+00, -1.3828e+00, -1.8594e+00, -2.4688e+00,\n",
      "         -2.5469e+00, -3.6719e+00, -3.7031e+00, -7.1875e-01, -1.0312e+00,\n",
      "         -3.0469e+00, -3.2969e+00, -3.7188e+00, -3.2031e+00, -3.5156e+00,\n",
      "         -3.4062e+00, -3.7500e+00, -3.5312e+00, -3.0625e+00, -3.6875e+00,\n",
      "         -2.1719e+00, -1.8125e+00, -3.0469e+00, -3.1719e+00, -3.7031e+00,\n",
      "         -3.6719e+00, -3.7812e+00, -3.6250e+00, -3.7031e+00, -3.6562e+00,\n",
      "         -3.7188e+00, -3.6719e+00, -3.6719e+00, -3.6406e+00, -3.6875e+00,\n",
      "         -3.7188e+00, -3.7500e+00, -3.6875e+00, -3.6406e+00, -3.7344e+00,\n",
      "          8.4766e-01, -2.1250e+00, -2.7500e+00, -2.3594e+00, -2.6094e+00,\n",
      "         -2.8125e+00, -3.0469e+00, -3.2344e+00, -3.6250e+00, -3.6250e+00,\n",
      "         -3.6562e+00, -3.7188e+00, -3.6562e+00, -1.7031e+00,  1.7109e+00,\n",
      "         -3.8125e+00, -3.6719e+00, -3.6875e+00, -3.7031e+00, -3.6094e+00,\n",
      "         -3.6719e+00, -3.6562e+00, -3.7344e+00, -3.7188e+00, -3.7188e+00,\n",
      "         -3.7031e+00, -3.6406e+00, -3.6719e+00, -3.7188e+00, -3.6875e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 182\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39]],\n",
      "       device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39]],\n",
      "       device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39]],\n",
      "       device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24, 25,\n",
      "         25, 25]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1,  3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 208, 182])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 182, 256]), Cross output shape: torch.Size([1, 208, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-3.2969,  2.3438, -1.6250,  ...,  0.1934,  2.8125, -1.4844],\n",
      "        [-3.1406,  2.1719, -1.8438,  ..., -0.0898,  2.7656, -1.5156],\n",
      "        [-1.3984,  0.5664, -2.6562,  ..., -1.8906,  1.4141, -2.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000],\n",
      "         [  712.0000,   -53.2500,   200.0000,  ...,  1168.0000,\n",
      "           -268.0000,    84.0000],\n",
      "         [   75.5000, -1040.0000,   976.0000,  ...,  1952.0000,\n",
      "            584.0000,  -374.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 26, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.],\n",
      "         [ 19200.,  -6592., -49664.,  ...,   -332.,  16256., -57344.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 182, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-3.2969,  2.3438, -1.6250,  ...,  0.1934,  2.8125, -1.4844],\n",
      "        [-3.1406,  2.1719, -1.8438,  ..., -0.0898,  2.7656, -1.5156],\n",
      "        [-1.3984,  0.5664, -2.6562,  ..., -1.8906,  1.4141, -2.1406]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 182]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24, 24,\n",
      "         24, 25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 182, 208])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-8.6875e+00, -5.9062e+00, -6.6797e-01, -8.6250e+00, -8.5625e+00,\n",
      "         -8.6875e+00, -8.8125e+00, -8.5625e+00, -8.6875e+00, -8.6250e+00,\n",
      "         -8.6250e+00, -8.7500e+00, -8.6875e+00, -6.0312e+00,  1.0234e+00,\n",
      "         -8.6250e+00, -8.7500e+00, -8.2500e+00, -8.8125e+00, -8.8125e+00,\n",
      "         -8.7500e+00, -8.6875e+00, -8.6250e+00, -8.5625e+00, -8.5625e+00,\n",
      "         -8.8125e+00, -8.7500e+00, -8.6875e+00, -8.7500e+00, -8.6875e+00,\n",
      "         -8.7500e+00, -8.6250e+00, -8.5000e+00, -8.6250e+00, -8.7500e+00,\n",
      "         -8.6250e+00,  1.0625e+01, -1.5859e+00, -2.3281e+00, -2.8438e+00,\n",
      "         -3.6406e+00, -3.3438e+00, -5.0625e+00, -1.2891e+00, -1.8984e+00,\n",
      "         -3.1875e+00, -1.3359e+00, -2.8438e+00,  7.5391e-01, -5.5237e-03,\n",
      "          8.1543e-02, -7.8613e-02, -1.3125e+00, -2.0000e+00, -2.4062e+00,\n",
      "         -3.1094e+00, -3.2812e+00, -2.0938e+00, -3.1406e+00, -3.8281e+00,\n",
      "         -3.3906e+00, -3.7969e+00, -1.1719e+00, -2.9062e+00, -3.6719e+00,\n",
      "         -2.0000e+00, -3.9375e+00, -1.7188e+00, -6.0312e+00, -3.0938e+00,\n",
      "         -3.0625e+00, -3.4219e+00, -3.5312e+00, -2.0312e+00, -3.6875e+00,\n",
      "         -2.7344e+00, -4.0625e+00,  3.1006e-02, -2.8438e+00, -3.9219e+00,\n",
      "         -3.9062e+00, -3.1250e+00, -4.4688e+00, -2.2188e+00, -2.8125e+00,\n",
      "         -4.8438e+00, -3.6562e+00, -2.9062e+00, -2.0938e+00, -4.0938e+00,\n",
      "         -3.7031e+00, -2.3125e+00, -6.0312e+00, -4.1250e+00, -5.2188e+00,\n",
      "         -4.0625e+00, -6.3438e+00, -3.8750e+00, -4.0938e+00, -2.9531e+00,\n",
      "         -3.3125e+00, -8.5547e-01,  6.1328e-01, -1.0781e+00, -1.7578e-01,\n",
      "         -1.4551e-01, -3.8867e-01, -1.1426e-01, -2.3438e+00, -8.2031e-01,\n",
      "         -7.9688e-01, -2.1406e+00, -5.5078e-01, -6.2500e-01,  4.1992e-01,\n",
      "          8.9844e-01, -1.3516e+00, -3.4375e+00, -1.6562e+00,  1.1914e-01,\n",
      "          1.2578e+00, -7.9297e-01, -1.0078e+00, -1.6406e+00, -2.0938e+00,\n",
      "         -2.1406e+00, -4.2812e+00, -4.3750e+00, -3.5781e+00, -4.5938e+00,\n",
      "         -8.1250e+00, -8.7500e+00, -3.7500e+00, -5.9688e+00, -6.2188e+00,\n",
      "         -7.0312e+00, -7.1250e+00, -7.0938e+00, -6.4062e+00, -7.7500e+00,\n",
      "         -5.3438e+00, -5.5312e+00, -6.5000e+00, -6.3125e+00, -5.1875e+00,\n",
      "         -5.9062e+00, -6.8125e+00, -6.2500e+00, -6.8438e+00, -7.0000e+00,\n",
      "         -5.1562e+00, -5.3750e+00, -5.3125e+00, -6.8750e+00, -7.5312e+00,\n",
      "         -5.5312e+00, -5.7812e+00, -3.7969e+00, -5.7188e+00, -7.4062e+00,\n",
      "         -6.1875e+00, -3.3438e+00, -8.0625e+00, -5.6562e+00, -7.2812e+00,\n",
      "         -7.0938e+00, -6.0312e+00, -7.3438e+00, -6.9375e+00, -7.1875e+00,\n",
      "         -5.8125e+00, -6.8438e+00, -6.5625e+00, -4.3438e+00, -7.6875e+00,\n",
      "         -7.5625e+00, -7.3438e+00, -7.3125e+00, -7.4688e+00, -7.2500e+00,\n",
      "         -6.9375e+00, -5.5938e+00, -6.4062e+00, -5.8125e+00, -7.5625e+00,\n",
      "         -7.9375e+00, -7.3125e+00, -6.6875e+00, -6.1875e+00, -7.5938e+00,\n",
      "         -7.5000e+00, -7.5625e+00, -6.0625e+00, -5.8438e+00, -7.1250e+00,\n",
      "         -7.4688e+00, -8.6875e+00, -8.6250e+00, -3.4219e+00, -5.0312e+00,\n",
      "         -8.0000e+00, -8.2500e+00, -8.8125e+00, -8.3125e+00, -8.5000e+00,\n",
      "         -8.5625e+00, -8.6250e+00, -8.6875e+00, -8.2500e+00, -8.5625e+00,\n",
      "         -7.3438e+00, -6.6875e+00, -8.1250e+00, -8.1250e+00, -8.7500e+00,\n",
      "         -8.6250e+00, -8.7500e+00, -8.6875e+00, -8.6250e+00, -8.6875e+00,\n",
      "         -8.7500e+00, -8.5625e+00, -8.6250e+00, -8.5625e+00, -8.6875e+00,\n",
      "         -8.5625e+00, -8.6250e+00, -8.5625e+00, -8.6250e+00, -8.6875e+00,\n",
      "         -8.4375e-01, -6.9375e+00, -7.7812e+00, -7.4375e+00, -7.6250e+00,\n",
      "         -7.9688e+00, -7.9375e+00, -8.2500e+00, -8.6250e+00, -8.7500e+00,\n",
      "         -8.6875e+00, -8.6875e+00, -8.5625e+00, -6.4375e+00, -5.5000e+00,\n",
      "         -8.7500e+00, -8.6250e+00, -8.6250e+00, -8.6875e+00, -8.6875e+00,\n",
      "         -8.7500e+00, -8.7500e+00, -8.6875e+00, -8.6875e+00, -8.7500e+00,\n",
      "         -8.6875e+00, -8.6875e+00, -8.7500e+00, -8.6250e+00, -8.8125e+00]],\n",
      "       device='cuda:0', dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 183\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24, 25,\n",
      "         25, 25, 25]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1,  4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 208, 183])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 183, 256]), Cross output shape: torch.Size([1, 208, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-3.1406,  2.1719, -1.8438,  ..., -0.0898,  2.7656, -1.5156],\n",
      "        [-1.3984,  0.5664, -2.6562,  ..., -1.8906,  1.4141, -2.1406],\n",
      "        [ 0.8477,  0.0166,  0.5312,  ..., -0.8125, -0.2891, -1.7266]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000],\n",
      "         [  712.0000,   -53.2500,   200.0000,  ...,  1168.0000,\n",
      "           -268.0000,    84.0000],\n",
      "         [  -11.6250, -1136.0000,  1040.0000,  ...,  2032.0000,\n",
      "            644.0000,  -454.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 26, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.],\n",
      "         [ 21120.,  -6304., -54784.,  ...,   -856.,  18048., -61952.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 183, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-3.1406,  2.1719, -1.8438,  ..., -0.0898,  2.7656, -1.5156],\n",
      "        [-1.3984,  0.5664, -2.6562,  ..., -1.8906,  1.4141, -2.1406],\n",
      "        [ 0.8477,  0.0166,  0.5312,  ..., -0.8125, -0.2891, -1.7266]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 183]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24, 24,\n",
      "         24, 24, 25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 183, 208])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-4.0938, -0.5859, -1.5078, -4.0625, -3.9688, -4.1562, -4.2188, -4.0625,\n",
      "         -4.0000, -4.0625, -4.0625, -4.1250, -4.0625, -2.0312,  0.2061, -4.0625,\n",
      "         -4.1250, -3.9375, -4.1875, -4.1562, -4.1250, -4.2188, -4.0312, -4.0938,\n",
      "         -4.0312, -4.1875, -4.1562, -4.1250, -4.1562, -4.1250, -4.0938, -4.0000,\n",
      "         -4.0312, -4.1562, -4.1562, -4.0938,  1.3984, -2.1562, -1.4453, -1.3438,\n",
      "         -3.6406, -0.0183, -2.2500, -0.4453, -1.2969, -2.5469, -1.8828, -1.7422,\n",
      "         -0.3965, -0.6680, -0.9609,  0.3340, -1.4062, -0.4902, -0.3223, -1.3203,\n",
      "         -0.3906, -1.5234, -1.5469, -1.0469, -1.7031, -2.1562, -1.6250, -0.4785,\n",
      "         -1.3828, -2.5312, -2.1406, -1.1797, -3.0938,  1.2891, -0.2832,  0.4941,\n",
      "         -0.2471,  1.1328,  0.1152,  0.6719, -0.0231, 12.1250, -1.0859, -0.7070,\n",
      "          0.2197, -0.5039,  0.2617,  1.2969,  0.9492, -1.0078,  3.9844,  0.3125,\n",
      "          0.7617,  0.5078,  0.9180,  1.4453, -2.3281, -0.5430, -3.1406, -1.8516,\n",
      "         -1.7812, -3.4375, -1.6250, -1.0391, -1.4141, -0.2041, -1.1562, -0.5391,\n",
      "         -0.2832, -0.6680,  0.2441, -1.1250, -1.3594,  0.7969, -0.0300, -1.2422,\n",
      "          0.6016, -1.5781,  0.4434, -0.4121, -1.5625, -0.5391,  1.0234, -0.2539,\n",
      "         -0.1729, -0.7461, -1.0234, -0.3398, -0.3418, -0.2422, -1.6562, -1.7344,\n",
      "         -1.5234, -1.6562, -3.5781, -4.1250, -0.7539, -2.6875, -2.6406, -2.9375,\n",
      "         -3.2500, -2.9375, -2.7656, -3.3750, -2.8750, -2.5000, -2.8594, -2.7656,\n",
      "         -2.5156, -2.5625, -2.6094, -2.3906, -2.9531, -3.0156, -1.6797, -1.2188,\n",
      "         -1.5859, -3.1406, -3.5156, -2.9375, -2.4375, -2.0781, -1.5156, -3.2969,\n",
      "         -2.3281, -1.6953, -3.5938, -2.2500, -2.9375, -3.3125, -2.5625, -3.1875,\n",
      "         -3.1562, -3.1094, -3.0000, -3.1250, -3.3125, -3.0000, -3.5312, -3.3281,\n",
      "         -3.2344, -3.5156, -3.2656, -2.9531, -2.5312, -2.9062, -1.9062, -2.6250,\n",
      "         -3.3594, -3.2500, -3.7031, -3.1406, -2.4844, -3.5469, -3.4375, -3.1406,\n",
      "         -2.6250, -2.5625, -3.2812, -3.3906, -4.0938, -4.0938, -0.6875, -3.1875,\n",
      "         -3.4844, -3.6406, -4.2188, -3.8438, -3.9844, -3.7812, -4.0938, -4.1250,\n",
      "         -3.7812, -4.1250, -3.0000, -2.9844, -3.6719, -3.5781, -4.1562, -4.1250,\n",
      "         -4.1562, -4.0938, -4.0625, -4.1875, -4.1250, -4.0938, -4.0938, -4.0312,\n",
      "         -4.0938, -4.0938, -4.0625, -4.0000, -3.9062, -4.0312, -0.0303, -2.9062,\n",
      "         -3.0938, -3.4375, -3.3281, -3.6562, -3.7344, -3.9062, -4.0938, -4.0312,\n",
      "         -3.9375, -4.1250, -4.0625, -2.6094, -2.4062, -4.1562, -4.0625, -4.0625,\n",
      "         -4.0938, -4.1875, -4.2188, -4.0938, -4.0625, -4.0625, -4.1875, -4.0938,\n",
      "         -4.1250, -4.1250, -4.0000, -4.1250]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 184\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24, 25,\n",
      "         25, 25, 25, 25]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1,  5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 208, 184])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 184, 256]), Cross output shape: torch.Size([1, 208, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.3984,  0.5664, -2.6562,  ..., -1.8906,  1.4141, -2.1406],\n",
      "        [ 0.8477,  0.0166,  0.5312,  ..., -0.8125, -0.2891, -1.7266],\n",
      "        [ 0.2930,  1.1953, -0.1562,  ...,  1.9844,  1.2969,  0.4141]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000],\n",
      "         [  712.0000,   -53.2500,   200.0000,  ...,  1168.0000,\n",
      "           -268.0000,    84.0000],\n",
      "         [  -43.0000, -1296.0000,  1152.0000,  ...,  2240.0000,\n",
      "            732.0000,  -552.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 26, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.],\n",
      "         [ 23552.,  -6304., -60928.,  ...,  -1368.,  20224., -68608.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 184, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [-1.3984,  0.5664, -2.6562,  ..., -1.8906,  1.4141, -2.1406],\n",
      "        [ 0.8477,  0.0166,  0.5312,  ..., -0.8125, -0.2891, -1.7266],\n",
      "        [ 0.2930,  1.1953, -0.1562,  ...,  1.9844,  1.2969,  0.4141]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 184]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24, 24,\n",
      "         24, 24, 24, 25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 184, 208])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.2812, -2.7188, -2.1406, -5.2812, -5.3125, -5.2812, -5.2188, -5.2500,\n",
      "         -5.1562, -5.1250, -5.3750, -5.3750, -5.2812, -4.0625, -1.8359, -5.2812,\n",
      "         -5.2812, -5.0625, -5.3750, -5.4062, -5.2500, -5.3438, -5.4688, -5.1875,\n",
      "         -5.2812, -5.3438, -5.3125, -5.3438, -5.2500, -5.3438, -5.4375, -5.3438,\n",
      "         -5.1562, -5.2812, -5.3125, -5.3438,  0.1660, -3.3750, -2.3906, -5.2812,\n",
      "         -4.7500, -2.2344, -2.7656, -2.0312, -2.5938, -3.0312, -2.0938, -2.2969,\n",
      "         -0.0264, -0.8359, -1.6562, -1.5938, -0.9219, -2.2656, -1.1875, -2.3438,\n",
      "         -2.0781, -2.1094, -2.0312, -2.4531, -3.1875, -3.6406, -1.9375, -2.8438,\n",
      "         -2.4062, -3.1875, -2.7656, -2.4375, -4.4062, -1.6172, -2.9375, -2.9219,\n",
      "         -2.0625, -3.5156, -3.5469, -3.2031, -2.0000, -2.3750, -2.3125, -3.4219,\n",
      "         -2.7031, -1.8984,  0.3594, -1.7734, -2.2031, -3.3750, -3.2500, -1.9453,\n",
      "         -2.5156, -2.8438, -2.6250, -2.5312, -2.5312, -3.6250, -3.9062, -3.3594,\n",
      "         -3.9062, -3.2969, -1.8281, -1.5547, -2.6562,  0.5391, -1.1484, -0.3047,\n",
      "          1.2969,  0.7773, -1.7188, -1.8828,  1.1875,  0.3438, -1.8359, -1.1719,\n",
      "          0.4863,  1.9688, 12.6250,  1.0391, -0.4805, -2.5938,  0.2471,  2.2656,\n",
      "          0.3047,  1.5234,  0.0889, -1.0625,  0.0752, -1.9219, -2.8750, -2.4062,\n",
      "         -2.2500, -4.0938, -4.5625, -5.4688, -2.8906, -3.4062, -3.7188, -4.5000,\n",
      "         -4.3750, -4.3438, -3.6406, -4.8438, -3.1250, -3.6562, -3.8281, -3.6875,\n",
      "         -3.5781, -3.9844, -4.1250, -3.3906, -4.2188, -4.4375, -3.4844, -3.6250,\n",
      "         -3.5156, -4.1875, -4.6562, -4.0312, -3.7188, -0.8867, -4.1562, -4.5625,\n",
      "         -2.7344, -3.8594, -4.8438, -3.3438, -4.5000, -3.7969, -4.7812, -4.2500,\n",
      "         -4.2188, -4.0000, -3.9844, -4.6250, -4.0938, -3.6719, -4.5312, -4.5938,\n",
      "         -4.6562, -4.4375, -4.5938, -4.1875, -4.8438, -4.1250, -1.5625, -3.0312,\n",
      "         -4.3125, -4.5625, -4.4688, -4.1875, -3.3906, -4.5938, -4.4688, -4.2500,\n",
      "         -3.4219, -3.2500, -4.4688, -4.7812, -5.2500, -5.3438, -3.0312, -2.6875,\n",
      "         -5.0625, -4.9375, -5.3125, -5.2188, -5.1250, -5.3125, -5.4062, -5.1562,\n",
      "         -4.9375, -5.2812, -4.2500, -3.2344, -5.1562, -4.9688, -5.2812, -5.3125,\n",
      "         -5.4062, -5.3438, -5.3125, -5.4062, -5.3125, -5.3125, -5.3750, -5.3438,\n",
      "         -5.3438, -5.2500, -5.2188, -5.2812, -5.1875, -5.4062, -2.3438, -4.5000,\n",
      "         -4.5625, -4.5000, -4.8750, -4.8125, -4.9375, -4.9688, -5.1562, -5.1875,\n",
      "         -5.2188, -5.2812, -5.3750, -3.8125, -3.3594, -5.4062, -5.3750, -5.2500,\n",
      "         -5.3750, -5.3125, -5.2812, -5.2812, -5.3438, -5.2812, -5.3750, -5.1875,\n",
      "         -5.3125, -5.4062, -5.2188, -5.3750]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 185\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77, 114]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77, 114]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77, 114]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24, 25,\n",
      "         25, 25, 25, 25, 25]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1,  6]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 208, 185])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 185, 256]), Cross output shape: torch.Size([1, 208, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.8477,  0.0166,  0.5312,  ..., -0.8125, -0.2891, -1.7266],\n",
      "        [ 0.2930,  1.1953, -0.1562,  ...,  1.9844,  1.2969,  0.4141],\n",
      "        [ 1.5703,  2.0469, -0.2891,  ..., -1.3203, -0.4121,  1.1094]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000],\n",
      "         [  712.0000,   -53.2500,   200.0000,  ...,  1168.0000,\n",
      "           -268.0000,    84.0000],\n",
      "         [  -62.5000, -1328.0000,  1168.0000,  ...,  2272.0000,\n",
      "            752.0000,  -576.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 26, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.],\n",
      "         [ 23936.,  -6144., -61952.,  ...,  -1488.,  20608., -69632.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 185, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.8477,  0.0166,  0.5312,  ..., -0.8125, -0.2891, -1.7266],\n",
      "        [ 0.2930,  1.1953, -0.1562,  ...,  1.9844,  1.2969,  0.4141],\n",
      "        [ 1.5703,  2.0469, -0.2891,  ..., -1.3203, -0.4121,  1.1094]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 185]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24, 24,\n",
      "         24, 24, 24, 24, 25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 185, 208])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-5.8750, -4.1562, -1.5391, -5.8438, -5.8438, -5.9062, -5.8750, -5.9062,\n",
      "         -5.7500, -5.7812, -6.0312, -5.9062, -5.9062, -4.0625, -1.3125, -5.9375,\n",
      "         -5.9375, -5.6875, -6.0000, -6.0000, -5.9375, -5.8750, -5.9688, -5.8750,\n",
      "         -5.9688, -5.9375, -5.9688, -5.9375, -5.8750, -5.9375, -6.0938, -5.9062,\n",
      "         -5.8125, -5.9062, -5.8750, -5.9062, -0.0427, -2.4062, -2.5156, -3.7656,\n",
      "         -2.9844, -4.0312, -4.1875, -2.2188, -1.6641, -3.5938, -2.4531, -2.0781,\n",
      "         -1.1797, -1.3594, -1.0000, -1.3906, -1.5312, -0.6641, -1.4766, -2.4062,\n",
      "         -2.3125, -1.7188, -1.0234, -2.1250, -1.6250, -3.0156,  0.1367, -2.4844,\n",
      "         -2.4844, -2.2031, -3.0000, -3.0781, -3.9531, -2.0000, -3.1875, -1.3984,\n",
      "         -2.1406, -1.7188, -3.8281, -3.2188, -2.2969, -2.0625, -1.2266, -3.4844,\n",
      "         -2.2500, -1.7344, -2.4844, -2.1719, -1.3438, -3.4062, -1.3203,  1.2344,\n",
      "         -1.6328, -2.1094, -2.2344, -2.5781, -3.1094, -2.7188, -4.6875, -2.7500,\n",
      "         -3.7500, -2.8906, -3.7188, -0.8828, -2.1719, -1.4844, -1.0938,  1.3828,\n",
      "          0.7539, -0.9727, -1.8438, -0.9062, -1.7422, -1.2344,  0.1162, -1.7812,\n",
      "          0.7461,  0.7578,  1.7188, -0.7227,  0.9688, -0.3496,  1.3281, 12.8125,\n",
      "         -0.1050, -1.0859, -0.1157, -1.1484, -1.6328, -2.7344, -1.7031, -1.5234,\n",
      "         -3.7188, -2.6250, -5.3750, -5.9688, -3.0938, -4.0938, -4.1875, -5.2812,\n",
      "         -4.8438, -4.6250, -4.1875, -5.6562, -3.4531, -2.6719, -4.3438, -3.7188,\n",
      "         -3.5000, -4.5312, -4.6562, -3.9531, -4.9062, -4.9062, -4.5938, -3.1250,\n",
      "         -4.1875, -5.1562, -5.3438, -4.0938, -4.0312, -2.0938, -4.6562, -5.1562,\n",
      "         -3.5625, -6.4375, -5.2500, -4.4688, -4.5000, -4.2188, -4.8438, -5.2188,\n",
      "         -4.2188, -4.4688, -4.1562, -4.6875, -4.8438, -4.3750, -5.2500, -5.3125,\n",
      "         -5.0938, -5.0938, -5.4375, -4.4062, -5.8125, -4.3438, -4.5625, -4.0625,\n",
      "         -5.2188, -5.0938, -5.3438, -4.3750, -3.8750, -5.5938, -5.2500, -5.0625,\n",
      "         -3.9062, -4.0312, -5.0625, -5.5000, -5.8125, -5.9062, -2.9062, -3.2969,\n",
      "         -5.8438, -5.4062, -5.9688, -5.7500, -5.6875, -6.1250, -6.0312, -5.8750,\n",
      "         -5.5000, -5.8438, -4.9375, -4.3438, -5.5938, -5.5000, -5.9062, -5.9375,\n",
      "         -6.0000, -5.9375, -5.9062, -5.9688, -5.9062, -5.8750, -5.9688, -5.8750,\n",
      "         -5.9062, -5.9375, -5.7812, -5.9375, -5.7188, -5.9375, -2.7500, -5.0000,\n",
      "         -5.1562, -5.2500, -5.2188, -5.5000, -5.7188, -5.6875, -5.6250, -5.7812,\n",
      "         -5.7188, -5.9375, -5.9688, -4.4062, -3.7188, -5.9688, -6.0000, -5.9375,\n",
      "         -5.8750, -5.9688, -5.9062, -5.9688, -5.9375, -5.9375, -6.0000, -5.8125,\n",
      "         -5.9062, -6.0000, -5.7500, -6.0000]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n",
      "batch size: 1, sequence length: 186\n",
      "nb_boe=0\n",
      "tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77, 114, 119]], device='cuda:0')\n",
      "local_encoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77, 114, 119]], device='cuda:0')\n",
      "local_decoder_tokens: tensor([[  1,  39,  39,  39,  36,  77, 114, 119, 120, 118, 121, 103, 120, 109,\n",
      "         115, 114,  62,  14,  71, 118, 105, 101, 120, 105,  36, 101,  36, 119,\n",
      "         105, 114, 120, 105, 114, 103, 105,  36, 121, 119, 109, 114, 107,  36,\n",
      "         120, 108, 105,  36, 106, 115, 112, 112, 115, 123, 109, 114, 107,  36,\n",
      "         123, 115, 118, 104, 119,  62,  36,  38, 101, 116, 116, 112, 105,  48,\n",
      "          36, 102, 101, 114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109,\n",
      "         112,  50,  38,  14,  14,  39,  39,  39,  36,  86, 105, 119, 116, 115,\n",
      "         114, 119, 105,  62,  14,  69, 116, 116, 112, 105,  48,  36, 102, 101,\n",
      "         114, 101, 114, 101,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36,\n",
      "         116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103, 109, 112,\n",
      "          48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 116, 105, 114, 103,\n",
      "         109, 112,  48,  36, 116, 105, 114, 103, 109, 112,  48,  36, 101, 114,\n",
      "         104,  36, 116, 105, 114, 103, 109, 112,  50,   2,   1,  39,  39,  39,\n",
      "          36,  77, 114, 119]], device='cuda:0')\n",
      "Patch IDs: tensor([[ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16,\n",
      "         16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "         18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20,\n",
      "         20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 24, 25,\n",
      "         25, 25, 25, 25, 25, 25]], device='cuda:0'), Patch lengths: tensor([[ 1, 16,  8,  2,  9,  6,  4, 10,  6,  8,  8,  8, 16,  7,  8,  8,  8,  8,\n",
      "          8,  8,  8,  5,  7,  1,  1,  7]], device='cuda:0')\n",
      "Cross attention mask encoder shape: torch.Size([1, 1, 208, 186])\n",
      "Cross attention mask encoder: tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          [0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "encoder_hash_tok_embedding=None encoder_hash_byte_group_nb_functions=3 encoder_hash_byte_group_size=None encoder_hash_byte_group_vocab=50002\n",
      "Encoder output shape: torch.Size([1, 186, 256]), Cross output shape: torch.Size([1, 208, 256])\n",
      "Encoder `h_encoder` output: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.2930,  1.1953, -0.1562,  ...,  1.9844,  1.2969,  0.4141],\n",
      "        [ 1.5703,  2.0469, -0.2891,  ..., -1.3203, -0.4121,  1.1094],\n",
      "        [ 1.5938,  1.7188, -0.0840,  ..., -0.2656, -1.6406,  0.9609]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[  844.0000,   -27.5000,    78.0000,  ...,  1136.0000,\n",
      "          -1048.0000,    22.5000],\n",
      "         [  113.0000, -1440.0000,  1288.0000,  ...,  1872.0000,\n",
      "            720.0000,  -760.0000],\n",
      "         [  213.0000,  -446.0000,   360.0000,  ...,  1280.0000,\n",
      "           -728.0000,  -400.0000],\n",
      "         ...,\n",
      "         [   25.0000,   -26.5000,   -18.7500,  ...,  -146.0000,\n",
      "           -268.0000,   174.0000],\n",
      "         [  712.0000,   -53.2500,   200.0000,  ...,  1168.0000,\n",
      "           -268.0000,    84.0000],\n",
      "         [  -63.0000, -1344.0000,  1184.0000,  ...,  2304.0000,\n",
      "            752.0000,  -576.0000]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Global transformer output shape: torch.Size([1, 26, 512]), Global transformer output: tensor([[[-12928., -36864.,  31488.,  ...,   4512.,  -9728.,  -5952.],\n",
      "         [ 21504., -11584., -56576.,  ...,  -1032.,  18944., -68608.],\n",
      "         [  3040., -14208.,  -7936.,  ...,   -992.,   4224., -22656.],\n",
      "         ...,\n",
      "         [-10880., -16768.,  26112.,  ...,   2464.,  -8000.,   9472.],\n",
      "         [  1744., -11456.,  -4480.,  ...,   4896.,    167., -16768.],\n",
      "         [ 24064.,  -6176., -62464.,  ...,  -1520.,  20736., -69632.]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 186, 256]), Decoder embeddings: tensor([[-3.3438,  2.3594, -1.5312,  ...,  0.0762,  2.9531, -1.3594],\n",
      "        [-3.3438,  2.3438, -1.6172,  ...,  0.1904,  2.8281, -1.4688],\n",
      "        [-3.1562,  2.1719, -1.8359,  ..., -0.0776,  2.7656, -1.5312],\n",
      "        ...,\n",
      "        [ 0.2930,  1.1953, -0.1562,  ...,  1.9844,  1.2969,  0.4141],\n",
      "        [ 1.5703,  2.0469, -0.2891,  ..., -1.3203, -0.4121,  1.1094],\n",
      "        [ 1.5938,  1.7188, -0.0840,  ..., -0.2656, -1.6406,  0.9609]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Decoder patch IDs shape: torch.Size([1, 186]), Decoder patch IDs: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,\n",
      "          4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
      "         13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17,\n",
      "         17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
      "         19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 22, 23, 24, 24,\n",
      "         24, 24, 24, 24, 24, 25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: torch.Size([1, 1, 186, 208])\n",
      "Cross attention mask decoder: tensor([[[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "Decoder logits shape: torch.Size([1, 260]), Decoder logits: tensor([[-6.7188, -6.5000, -1.4141, -6.6562, -6.6875, -6.7500, -6.7500, -6.7500,\n",
      "         -6.6250, -6.6562, -6.7812, -6.7188, -6.7500, -4.2812, -1.3984, -6.7500,\n",
      "         -6.8125, -6.5000, -6.7500, -6.7188, -6.7188, -6.6875, -6.7500, -6.7500,\n",
      "         -6.7188, -6.6875, -6.7188, -6.6875, -6.6875, -6.7500, -6.8125, -6.7500,\n",
      "         -6.5938, -6.7188, -6.7188, -6.7188,  0.3047, -1.5703, -2.6406, -5.1250,\n",
      "         -2.0781, -3.8125, -3.8594, -1.7734, -1.9844, -2.5156, -2.4219, -1.8281,\n",
      "          0.2441, -1.5234, -0.8711, -1.6953, -1.0000, -0.9648, -1.4688, -1.6953,\n",
      "         -2.3594, -1.4688, -2.6250, -2.9531, -3.2188, -2.2500, -1.6719, -2.2656,\n",
      "         -3.1250, -1.3516, -3.8125, -2.8125, -4.5625, -1.9531, -2.6875, -2.5625,\n",
      "         -3.0469, -2.6719, -3.4531, -2.7031, -4.1562, -3.6875, -1.7188, -3.7812,\n",
      "         -3.0938, -2.3125, -2.7188, -2.6875, -2.7500, -3.3125, -3.1875, -1.9688,\n",
      "         -0.0123, -2.0000, -3.4375, -3.2969, -3.7812, -3.8594, -4.4375, -2.5625,\n",
      "         -4.7188, -1.1094, -3.5781, -2.3125, -3.1562, -0.9297, -1.0547,  0.5156,\n",
      "         -1.8281,  0.3809, -1.6484, -0.9922, -0.2109,  0.2168, -1.2734, -1.6719,\n",
      "         -0.2373,  0.6680, -0.4609,  0.1426,  0.8008, -0.9805, -0.2305,  0.3691,\n",
      "         12.1250,  2.0156, -2.6094, -0.6562, -1.7188, -1.6953, -0.7578, -2.3750,\n",
      "         -3.1562, -2.4062, -6.0312, -6.7500, -4.0625, -5.0625, -4.9375, -5.5625,\n",
      "         -5.5000, -5.4375, -5.2188, -6.2812, -4.0312, -3.0625, -5.0625, -4.4062,\n",
      "         -3.8906, -4.8438, -5.3750, -4.8125, -5.4375, -5.5312, -4.9375, -3.1406,\n",
      "         -4.1562, -6.0000, -5.8438, -4.5938, -4.5938, -4.0938, -5.4062, -5.8750,\n",
      "         -4.1875, -5.4688, -6.0312, -5.2812, -5.3750, -4.9375, -5.0938, -5.7500,\n",
      "         -5.1562, -5.6250, -4.5000, -4.7812, -5.5000, -3.8438, -5.5625, -5.8438,\n",
      "         -5.7500, -5.6875, -5.9062, -5.2812, -6.0312, -4.0000, -5.0625, -4.5938,\n",
      "         -6.0625, -5.8438, -5.8750, -4.4375, -4.9062, -5.9062, -5.7500, -5.7500,\n",
      "         -4.4688, -5.0000, -5.8750, -5.6875, -6.6250, -6.7812, -3.7188, -2.4531,\n",
      "         -6.2188, -6.3438, -6.6875, -6.4375, -6.4688, -6.7188, -6.8125, -6.5938,\n",
      "         -6.3125, -6.6250, -5.3438, -4.9062, -6.3125, -6.3125, -6.7188, -6.6875,\n",
      "         -6.7812, -6.7188, -6.7500, -6.7188, -6.6875, -6.7188, -6.7188, -6.6875,\n",
      "         -6.6250, -6.7188, -6.5625, -6.7188, -6.6250, -6.6562, -2.7188, -5.7812,\n",
      "         -5.8750, -5.9688, -5.9062, -6.1250, -6.3750, -6.2188, -6.5938, -6.6250,\n",
      "         -6.6250, -6.6875, -6.7500, -5.4375, -4.2500, -6.8125, -6.7188, -6.6250,\n",
      "         -6.6250, -6.8125, -6.7188, -6.7188, -6.7188, -6.7188, -6.7500, -6.7188,\n",
      "         -6.6562, -6.7812, -6.5938, -6.7500]], device='cuda:0',\n",
      "       dtype=torch.float32, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "prompts = [prompt]\n",
    "\n",
    "max_prompt_len: int = 256\n",
    "max_gen_len: int = 10\n",
    "use_sampling: bool = False\n",
    "temp: float = 1.0\n",
    "top_k: int = 0\n",
    "top_p: float = 0.0\n",
    "remove_prompts: bool = True\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt_tokens = [tokenizer.encode(t, add_eos=False) for t in prompts]\n",
    "n_truncated_prompts = sum([max_prompt_len < len(t) for t in prompt_tokens])\n",
    "total_truncated_prompts = dist_sum(n_truncated_prompts)\n",
    "\n",
    "# Truncation\n",
    "prompt_tokens = [\n",
    "    t if len(t) < max_prompt_len else t[len(t) - max_prompt_len :]\n",
    "    for t in prompt_tokens\n",
    "]\n",
    "\n",
    "if total_truncated_prompts > 0:\n",
    "    logger.info(\n",
    "        f\"There are {total_truncated_prompts} prompts that are truncated on the left, \"\n",
    "        f\"length greater than max_prompt_len = {max_prompt_len}, \"\n",
    "        f\"maximum prompt length = {get_max_length(prompt_tokens)} across all gpus.\"\n",
    "    )\n",
    "\n",
    "start_pos, end_pos = get_generation_range(prompt_tokens, max_gen_len)\n",
    "batch_size = len(prompt_tokens)\n",
    "tokens = torch.full((batch_size, end_pos), tokenizer.pad_id).cuda().long()\n",
    "\n",
    "# Copy inputs to tensor for generated tokens\n",
    "for i, row_tokens in enumerate(prompt_tokens):\n",
    "    tokens[i, : len(row_tokens)] = torch.tensor(row_tokens).long()\n",
    "input_text_mask = tokens != tokenizer.pad_id\n",
    "\n",
    "for i, curr_pos in enumerate(range(start_pos, end_pos)):\n",
    "    current_tokens = tokens[:, :curr_pos]\n",
    "    patch_lengths, _ = patcher.patch(current_tokens, include_next_token=False)\n",
    "    # logits = model(current_tokens, patch_lengths=patch_lengths)[:, -1]\n",
    "    \n",
    "    ################################\n",
    "    #### START MODEL PROCESSING ####\n",
    "    ################################\n",
    "    ngram_ids = None\n",
    "    bs, N = current_tokens.shape  # Batch size and sequence length\n",
    "\n",
    "    print(f\"batch size: {bs}, sequence length: {N}\")\n",
    "\n",
    "    # Get megabyte inputs\n",
    "    nb_boe = int(0 if model.patching_mode != \"\" else model.patch_size - 1)\n",
    "    print(f\"nb_boe={nb_boe}\")\n",
    "    local_encoder_tokens, _, local_decoder_tokens = get_blt_input(\n",
    "        tokens=current_tokens,\n",
    "        enforce_patch_size_multiple=False,\n",
    "        nb_boe=nb_boe,\n",
    "        patch_size=model.patch_size,\n",
    "        boe_id=model.boe_id,\n",
    "    )\n",
    "    print(f\"tokens: {current_tokens}\")\n",
    "    print(f\"local_encoder_tokens: {local_encoder_tokens}\")\n",
    "    print(f\"local_decoder_tokens: {local_decoder_tokens}\")\n",
    "\n",
    "    # Patching\n",
    "    if nb_boe > 0:\n",
    "        patch_lengths[:, 0] += nb_boe\n",
    "\n",
    "    assert torch.min(patch_lengths) >= 0\n",
    "\n",
    "    # Generate patch IDs from patch_lengths\n",
    "    patch_ids = patch_ids_from_lengths(\n",
    "        patch_lengths, local_encoder_tokens.shape[-1]\n",
    "    )\n",
    "    print(f\"Patch IDs: {patch_ids}, Patch lengths: {patch_lengths}\")\n",
    "    assert torch.max(patch_ids) + 1 <= torch.max(\n",
    "        (patch_lengths != 0).sum(dim=-1)\n",
    "    ), f\"{torch.max(patch_ids) + 1} > {torch.max((patch_lengths != 0).sum(dim=-1))}\"\n",
    "\n",
    "    cross_attn_mask_enc = None\n",
    "    # Cross-attention encoder\n",
    "    if model.cross_attn_encoder:\n",
    "        cross_attn_mask_enc = cross_attn_mask(\n",
    "            patch_ids,\n",
    "            patch_lengths,\n",
    "            N,\n",
    "            patches_as_queries=True,\n",
    "            cross_attn_k=model.cross_attn_k,\n",
    "            window=model.cross_attn_window_encoder,\n",
    "            block_mask=model.cross_attn_use_flex_attention,\n",
    "        )\n",
    "        print(f\"Cross attention mask encoder shape: {cross_attn_mask_enc.shape}\")\n",
    "        print(f\"Cross attention mask encoder: {cross_attn_mask_enc}\")\n",
    "        # print(f\"Cross attention mask encoder: {cross_attn_mask_enc.to_dense()}\")\n",
    "\n",
    "    # Hashing and embedding\n",
    "    print(\n",
    "        f\"encoder_hash_tok_embedding={model.encoder_hash_tok_embedding}\",\n",
    "        f\"encoder_hash_byte_group_nb_functions={model.encoder_hash_byte_group_nb_functions}\",\n",
    "        f\"encoder_hash_byte_group_size={model.encoder_hash_byte_group_size}\",\n",
    "        f\"encoder_hash_byte_group_vocab={model.encoder_hash_byte_group_vocab}\",\n",
    "    )\n",
    "    local_encoder_embeds = compute_hash_embeddings(\n",
    "        local_encoder_tokens=local_encoder_tokens,\n",
    "        local_encoder=model.local_encoder,\n",
    "        encoder_hash_tok_embedding=model.encoder_hash_tok_embedding,\n",
    "        encoder_hash_byte_group_nb_functions=model.encoder_hash_byte_group_nb_functions,\n",
    "        encoder_hash_byte_group_size=model.encoder_hash_byte_group_size,\n",
    "        encoder_hash_byte_group_vocab=model.encoder_hash_byte_group_vocab,\n",
    "    )\n",
    "    if local_encoder_embeds:\n",
    "        print(f\"local_encoder_embeds.shape={local_encoder_embeds.shape}\")\n",
    "        print(f\"local_encoder_embeds={local_encoder_embeds}\")\n",
    "\n",
    "    # N-gram table embeddings\n",
    "    if model.encoder_ngram_embedding is not None:\n",
    "        assert ngram_ids is not None, \"ngram_ids must be provided\"\n",
    "        if local_encoder_embeds is None:\n",
    "            local_encoder_embeds = model.local_encoder.tok_embeddings(\n",
    "                local_encoder_tokens\n",
    "            )\n",
    "        assert len(ngram_ids) == len(\n",
    "            model.encoder_ngram_embedding\n",
    "        ), f\"ngram_ids.shape[0]={ngram_ids.shape[0]} versus len(encoder_ngram_embedding)={len(model.encoder_ngram_embedding)}, ngram_ids.shape={ngram_ids.shape}\"\n",
    "        for i in range(ngram_ids.shape[0]):\n",
    "            ngram_embedding = model.encoder_ngram_embedding[i]\n",
    "            ngram_embeds = ngram_embedding(ngram_ids[i])\n",
    "            assert (\n",
    "                local_encoder_embeds.shape == ngram_embeds.shape\n",
    "            ), f\"Shape mismatch: {local_encoder_embeds.shape} vs {ngram_embeds.shape}, ngram_ids.shape={ngram_ids.shape}\"\n",
    "            local_encoder_embeds = local_encoder_embeds + ngram_embeds\n",
    "\n",
    "    # Local encoder\n",
    "    (h_encoder, h_cross), cache_encoder = model.local_encoder(\n",
    "        tokens=local_encoder_tokens,\n",
    "        embeds=local_encoder_embeds,\n",
    "        patch_embeds=None,\n",
    "        cross_mask=cross_attn_mask_enc,\n",
    "        num_patches=patch_lengths.shape[1],\n",
    "        patch_ids=patch_ids,\n",
    "    )\n",
    "    print(f\"Encoder output shape: {h_encoder.shape}, Cross output shape: {h_cross.shape}\")\n",
    "    print(f\"Encoder `h_encoder` output: {h_encoder[0]}\")\n",
    "\n",
    "    # Downsampling\n",
    "    if not model.cross_attn_encoder:\n",
    "        assert (\n",
    "            patch_ids.shape[1] == h_encoder.shape[1]\n",
    "        ), f\"{patch_ids.shape[1]} != {h_encoder.shape[1]}\"\n",
    "        h = downsample(\n",
    "            h_encoder,\n",
    "            patch_lengths.shape[1],\n",
    "            patch_lengths,\n",
    "            patch_ids,\n",
    "            downsampling_by_pooling=model.downsampling_by_pooling,\n",
    "            patch_size=model.patch_size,\n",
    "        )\n",
    "    else:\n",
    "        # Reshape h_cross\n",
    "        h = h_cross.view(bs, patch_lengths.shape[1], -1)\n",
    "    print(f\"Global transformer input shape: {h.shape}, Global transformer input: {h}\")\n",
    "\n",
    "    # Global transformer\n",
    "    global_tokens = current_tokens.new(h.shape[0], h.shape[1]).fill_(model.boe_id)\n",
    "    rows, cols = torch.where(local_encoder_tokens == model.eos_id)\n",
    "    eos_patch_ids = patch_ids[rows, cols]\n",
    "    global_tokens[rows, eos_patch_ids] = model.eos_id\n",
    "\n",
    "    h, _ = model.global_transformer(\n",
    "        embeds=h,\n",
    "        tokens=global_tokens,\n",
    "    )\n",
    "    print(f\"Global transformer output shape: {h.shape}, Global transformer output: {h}\")\n",
    "\n",
    "    # Unpatching\n",
    "    dec_embeds = h_encoder[:, nb_boe : nb_boe + N, :]\n",
    "    print(f\"Decoder embeddings `dec_embeds` shape: {dec_embeds.shape}, Decoder embeddings: {dec_embeds[0]}\")\n",
    "\n",
    "    # Generate decoder patch IDs\n",
    "    decoder_patch_ids = decoder_patch_ids_from_lengths(\n",
    "        patch_lengths, nb_boe, local_decoder_tokens.shape[-1]\n",
    "    )\n",
    "    print(f\"Decoder patch IDs shape: {decoder_patch_ids.shape}, Decoder patch IDs: {decoder_patch_ids}\")\n",
    "    assert (\n",
    "        torch.max(decoder_patch_ids) + 1 <= h.shape[1]\n",
    "    ), f\"{torch.max(decoder_patch_ids) + 1} > {h.shape[1]}\"\n",
    "    assert (\n",
    "        decoder_patch_ids.shape[1] == dec_embeds.shape[1]\n",
    "    ), f\"{decoder_patch_ids.shape[1]} != {dec_embeds.shape[1]}\"\n",
    "\n",
    "    # Cross-attention decoder\n",
    "    if not model.cross_attn_decoder:\n",
    "        h = torch.gather(\n",
    "            h, 1, decoder_patch_ids.unsqueeze(-1).expand(-1, -1, h.shape[-1])\n",
    "        )\n",
    "        cross_attn_mask_dec = None\n",
    "        assert local_decoder_tokens.shape == h.shape[:-1]\n",
    "    else:\n",
    "        cross_attn_mask_dec = cross_attn_mask(\n",
    "            decoder_patch_ids,\n",
    "            patch_lengths,\n",
    "            N,\n",
    "            patches_as_queries=False,\n",
    "            cross_attn_k=model.cross_attn_k,\n",
    "            window=model.cross_attn_window_decoder,\n",
    "            block_mask=model.cross_attn_use_flex_attention,\n",
    "        )\n",
    "        print(f\"Cross attention mask decoder shape: {cross_attn_mask_dec.shape}\")\n",
    "        print(f\"Cross attention mask decoder: {cross_attn_mask_dec}\")\n",
    "        # print(f\"Cross attention mask decoder: {cross_attn_mask_dec.to_dense()}\")\n",
    "\n",
    "    # Local decoder\n",
    "    logits, _ = model.local_decoder(\n",
    "        embeds=dec_embeds,\n",
    "        patch_embeds=h,\n",
    "        tokens=local_decoder_tokens,\n",
    "        cross_mask=cross_attn_mask_dec,\n",
    "    )\n",
    "    logits = logits[:, -1]\n",
    "    print(f\"Decoder logits shape: {logits.shape}, Decoder logits: {logits}\")\n",
    "\n",
    "    ##############################\n",
    "    #### END MODEL PROCESSING ####\n",
    "    ##############################\n",
    "\n",
    "    if use_sampling:\n",
    "        probs = torch.softmax(logits / temp, dim=-1)\n",
    "        if top_p > 0.0:\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        elif top_k > 0:\n",
    "            next_token = sample_top_k(probs, top_k)\n",
    "        else:\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "    else:\n",
    "        next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    next_token = torch.where(\n",
    "        input_text_mask[:, curr_pos], tokens[:, curr_pos], next_token\n",
    "    )\n",
    "    tokens[:, curr_pos] = next_token\n",
    "\n",
    "if remove_prompts:\n",
    "    generated_tokens = [\n",
    "        t[len(prompt_tokens[i]) : len(prompt_tokens[i]) + max_gen_len].tolist()\n",
    "        for i, t in enumerate(tokens)\n",
    "    ]\n",
    "else:\n",
    "    generated_tokens = [\n",
    "        t[: len(prompt_tokens[i]) + max_gen_len].tolist()\n",
    "        for i, t in enumerate(tokens)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2402880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"### Instruction:\n",
      "Create a sentence using the following words: \"apple, banana, pencil.\"\" Completion: \"\n",
      "\n",
      "### Response:\n",
      "Apple, banana, pencil, pencil, pencil, pencil, pencil, pencil, and pencil.### Inst\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_outputs = [tokenizer.decode(t) for t in generated_tokens]\n",
    "for p, t in zip(prompts, text_outputs):\n",
    "    print(f'Prompt: \"{p}\" Completion: \"{t}\"')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f081ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "1 [0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "2 [-inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "3 [-inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "4 [-inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "5 [-inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "6 [-inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "7 [-inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "8 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "9 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "10 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "11 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "12 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "13 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "14 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "15 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "16 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "17 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "18 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "19 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "20 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "21 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "22 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "23 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "24 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "25 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "26 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "27 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "28 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "29 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "30 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "31 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "32 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf]\n",
      "33 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf]\n",
      "34 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf]\n",
      "35 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, -inf, -inf, -inf]\n",
      "36 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf]\n",
      "37 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf]\n",
      "38 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0]\n",
      "39 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(cross_attn_mask_enc[0, 0]):\n",
    "    print(i, [float(x) for x in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "179d0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "1 [-inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "2 [-inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "3 [-inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "4 [-inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "5 [-inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "6 [-inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "7 [-inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "8 [-inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "9 [-inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "10 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "11 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "12 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "13 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "14 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "15 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "16 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "17 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "18 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "19 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "20 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "21 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "22 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "23 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "24 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "25 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "26 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "27 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "28 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "29 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "30 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "31 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "32 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "33 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "34 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "35 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "36 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "37 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "38 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "39 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "40 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "41 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf]\n",
      "42 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf, -inf, -inf]\n",
      "43 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0, -inf, -inf]\n",
      "44 [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(cross_attn_mask_dec[0, 0]):\n",
    "    print(i, [float(x) for x in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46d0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb0c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6f494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03396b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1, sequence length: 45\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 42, 45)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 42, 45), sparsity=-766.88%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 45, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0157,  0.0247, -0.0125,  ..., -0.0310,  0.0352, -0.0135],\n",
      "         [-0.0713,  0.0004,  0.0305,  ...,  0.0398,  0.0195,  0.0193],\n",
      "         [ 0.0371, -0.0303,  0.0079,  ..., -0.0068, -0.0161,  0.0566]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 45, 1024]), Cross output shape: torch.Size([1, 42, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 1.2793e-01, -4.5410e-02, -3.7891e-01,  ..., -1.5332e-01,\n",
      "         -6.0938e-01, -3.2031e-01],\n",
      "        [-9.7656e-03,  1.1719e-02, -1.6846e-02,  ..., -6.7383e-02,\n",
      "         -8.9355e-02, -4.9316e-02],\n",
      "        [ 3.2715e-02, -5.0781e-02, -8.5938e-02,  ..., -7.8735e-03,\n",
      "         -1.0840e-01, -4.2480e-02],\n",
      "        ...,\n",
      "        [-3.2471e-02, -2.5146e-02, -2.4902e-02,  ..., -1.1963e-02,\n",
      "          4.7852e-02, -5.0781e-02],\n",
      "        [-2.5635e-02,  2.2888e-03, -2.7100e-02,  ...,  4.0039e-02,\n",
      "          2.1973e-03, -4.5166e-03],\n",
      "        [ 2.4170e-02, -4.1016e-02,  5.3711e-03,  ..., -2.7588e-02,\n",
      "         -9.0332e-03, -2.4414e-04]], device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [-0.0698,  0.0210,  0.0175,  ..., -0.0330,  0.0117,  0.0547],\n",
      "         [-0.0698, -0.0042, -0.0006,  ...,  0.0013,  0.0085,  0.0038],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 21, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [ 1.7031, -1.5391,  1.6719,  ...,  0.2441,  1.2266, -0.1069],\n",
      "         [ 2.5000, -0.4570, -0.3770,  ...,  2.7344,  1.5391, -0.8750],\n",
      "         [ 2.3438, -1.2812,  1.2422,  ...,  2.1562, -0.1367,  0.6289]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 45, 1024]), Decoder embeddings: tensor([[ 1.2793e-01, -4.5410e-02, -3.7891e-01,  ..., -1.5332e-01,\n",
      "         -6.0938e-01, -3.2031e-01],\n",
      "        [-9.7656e-03,  1.1719e-02, -1.6846e-02,  ..., -6.7383e-02,\n",
      "         -8.9355e-02, -4.9316e-02],\n",
      "        [ 3.2715e-02, -5.0781e-02, -8.5938e-02,  ..., -7.8735e-03,\n",
      "         -1.0840e-01, -4.2480e-02],\n",
      "        ...,\n",
      "        [-3.2471e-02, -2.5146e-02, -2.4902e-02,  ..., -1.1963e-02,\n",
      "          4.7852e-02, -5.0781e-02],\n",
      "        [-2.5635e-02,  2.2888e-03, -2.7100e-02,  ...,  4.0039e-02,\n",
      "          2.1973e-03, -4.5166e-03],\n",
      "        [ 2.4170e-02, -4.1016e-02,  5.3711e-03,  ..., -2.7588e-02,\n",
      "         -9.0332e-03, -2.4414e-04]], device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 45]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 45, 42)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 45, 42), sparsity=-11169.42%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 45, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-11.1250, -11.0625,  -3.3281,  ..., -11.0000, -11.0000, -11.0000],\n",
      "         [-11.1875, -11.5000,  -4.2500,  ..., -10.9375, -10.8750, -10.8750],\n",
      "         [-12.8125, -13.6875, -10.3750,  ..., -12.8125, -12.8125, -12.8125]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 46\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 2]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 42, 46)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 42, 46), sparsity=-68590.68%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 46, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0713,  0.0004,  0.0305,  ...,  0.0398,  0.0195,  0.0193],\n",
      "         [ 0.0371, -0.0303,  0.0079,  ..., -0.0068, -0.0161,  0.0566],\n",
      "         [-0.0496,  0.0240,  0.0403,  ...,  0.0508,  0.0272,  0.0205]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 46, 1024]), Cross output shape: torch.Size([1, 42, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 1.2793e-01, -4.5410e-02, -3.7891e-01,  ..., -1.5332e-01,\n",
      "         -6.0938e-01, -3.2031e-01],\n",
      "        [-9.7656e-03,  1.1719e-02, -1.6846e-02,  ..., -6.7383e-02,\n",
      "         -8.9355e-02, -4.9316e-02],\n",
      "        [ 3.2715e-02, -5.0781e-02, -8.5938e-02,  ..., -7.8735e-03,\n",
      "         -1.0840e-01, -4.2480e-02],\n",
      "        ...,\n",
      "        [-2.5635e-02,  2.2888e-03, -2.7100e-02,  ...,  4.0039e-02,\n",
      "          2.1973e-03, -4.5166e-03],\n",
      "        [ 2.4170e-02, -4.1016e-02,  5.3711e-03,  ..., -2.7588e-02,\n",
      "         -9.0332e-03, -2.4414e-04],\n",
      "        [-6.1279e-02, -9.0332e-03,  4.8340e-02,  ..., -2.1973e-02,\n",
      "          1.2207e-02, -1.4404e-02]], device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [-0.0698,  0.0210,  0.0175,  ..., -0.0330,  0.0117,  0.0547],\n",
      "         [-0.0698, -0.0042, -0.0006,  ...,  0.0013,  0.0085,  0.0038],\n",
      "         [ 0.0210, -0.0159,  0.0044,  ...,  0.1348, -0.0226, -0.0415]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 21, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [ 1.7031, -1.5391,  1.6719,  ...,  0.2441,  1.2266, -0.1069],\n",
      "         [ 2.5000, -0.4570, -0.3770,  ...,  2.7344,  1.5391, -0.8750],\n",
      "         [ 1.8594, -0.7109,  0.6562,  ...,  0.9609,  0.4609,  0.4805]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 46, 1024]), Decoder embeddings: tensor([[ 1.2793e-01, -4.5410e-02, -3.7891e-01,  ..., -1.5332e-01,\n",
      "         -6.0938e-01, -3.2031e-01],\n",
      "        [-9.7656e-03,  1.1719e-02, -1.6846e-02,  ..., -6.7383e-02,\n",
      "         -8.9355e-02, -4.9316e-02],\n",
      "        [ 3.2715e-02, -5.0781e-02, -8.5938e-02,  ..., -7.8735e-03,\n",
      "         -1.0840e-01, -4.2480e-02],\n",
      "        ...,\n",
      "        [-2.5635e-02,  2.2888e-03, -2.7100e-02,  ...,  4.0039e-02,\n",
      "          2.1973e-03, -4.5166e-03],\n",
      "        [ 2.4170e-02, -4.1016e-02,  5.3711e-03,  ..., -2.7588e-02,\n",
      "         -9.0332e-03, -2.4414e-04],\n",
      "        [-6.1279e-02, -9.0332e-03,  4.8340e-02,  ..., -2.1973e-02,\n",
      "          1.2207e-02, -1.4404e-02]], device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 46]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 46, 42)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 46, 42), sparsity=-10924.43%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 46, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-11.1875, -11.5000,  -4.2500,  ..., -10.9375, -10.8750, -10.8750],\n",
      "         [-12.8125, -13.6875, -10.3750,  ..., -12.8125, -12.8125, -12.8125],\n",
      "         [-13.4375, -14.0000,  -6.1250,  ..., -13.3750, -13.3750, -13.3750]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 47\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 42, 47)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 42, 47), sparsity=-42229.48%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 47, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [ 0.0371, -0.0303,  0.0079,  ..., -0.0068, -0.0161,  0.0566],\n",
      "         [-0.0496,  0.0240,  0.0403,  ...,  0.0508,  0.0272,  0.0205],\n",
      "         [-0.0312,  0.0557,  0.0013,  ..., -0.0247, -0.0022, -0.0317]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 47, 1024]), Cross output shape: torch.Size([1, 42, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 1.2793e-01, -4.5410e-02, -3.7891e-01,  ..., -1.5332e-01,\n",
      "         -6.0938e-01, -3.2031e-01],\n",
      "        [-9.7656e-03,  1.1719e-02, -1.6846e-02,  ..., -6.7383e-02,\n",
      "         -8.9355e-02, -4.9316e-02],\n",
      "        [ 3.2715e-02, -5.0781e-02, -8.5938e-02,  ..., -7.8735e-03,\n",
      "         -1.0840e-01, -4.2480e-02],\n",
      "        ...,\n",
      "        [ 2.4170e-02, -4.1016e-02,  5.3711e-03,  ..., -2.7588e-02,\n",
      "         -9.0332e-03, -2.4414e-04],\n",
      "        [-6.1279e-02, -9.0332e-03,  4.8340e-02,  ..., -2.1973e-02,\n",
      "          1.2207e-02, -1.4404e-02],\n",
      "        [-2.3438e-02,  1.9287e-02,  1.6479e-03,  ..., -5.0293e-02,\n",
      "          1.7334e-02, -3.3203e-02]], device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 21, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [-0.0698,  0.0210,  0.0175,  ..., -0.0330,  0.0117,  0.0547],\n",
      "         [-0.0698, -0.0042, -0.0006,  ...,  0.0013,  0.0085,  0.0038],\n",
      "         [ 0.0143,  0.0083, -0.0155,  ..., -0.0306, -0.0708, -0.0398]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 21, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [ 1.7031, -1.5391,  1.6719,  ...,  0.2441,  1.2266, -0.1069],\n",
      "         [ 2.5000, -0.4570, -0.3770,  ...,  2.7344,  1.5391, -0.8750],\n",
      "         [ 1.7812, -1.1406,  0.8750,  ..., -0.9922, -0.4980, -0.9609]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 47, 1024]), Decoder embeddings: tensor([[ 1.2793e-01, -4.5410e-02, -3.7891e-01,  ..., -1.5332e-01,\n",
      "         -6.0938e-01, -3.2031e-01],\n",
      "        [-9.7656e-03,  1.1719e-02, -1.6846e-02,  ..., -6.7383e-02,\n",
      "         -8.9355e-02, -4.9316e-02],\n",
      "        [ 3.2715e-02, -5.0781e-02, -8.5938e-02,  ..., -7.8735e-03,\n",
      "         -1.0840e-01, -4.2480e-02],\n",
      "        ...,\n",
      "        [ 2.4170e-02, -4.1016e-02,  5.3711e-03,  ..., -2.7588e-02,\n",
      "         -9.0332e-03, -2.4414e-04],\n",
      "        [-6.1279e-02, -9.0332e-03,  4.8340e-02,  ..., -2.1973e-02,\n",
      "          1.2207e-02, -1.4404e-02],\n",
      "        [-2.3438e-02,  1.9287e-02,  1.6479e-03,  ..., -5.0293e-02,\n",
      "          1.7334e-02, -3.3203e-02]], device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 47]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 47, 42)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 47, 42), sparsity=-10689.87%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 47, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-12.8125, -13.6875, -10.3750,  ..., -12.8125, -12.8125, -12.8125],\n",
      "         [-13.4375, -14.0000,  -6.1250,  ..., -13.3750, -13.3750, -13.3750],\n",
      "         [-12.3125, -13.6250,  -4.9375,  ..., -12.1875, -12.1875, -12.1875]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 48\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 44, 48)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 44, 48), sparsity=-40239.39%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 48, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0496,  0.0240,  0.0403,  ...,  0.0508,  0.0272,  0.0205],\n",
      "         [-0.0312,  0.0557,  0.0013,  ..., -0.0247, -0.0022, -0.0317],\n",
      "         [ 0.0072,  0.0059, -0.0150,  ...,  0.0138,  0.0496,  0.0153]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 48, 1024]), Cross output shape: torch.Size([1, 44, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0613, -0.0090,  0.0483,  ..., -0.0220,  0.0122, -0.0144],\n",
      "        [-0.0234,  0.0193,  0.0016,  ..., -0.0503,  0.0173, -0.0332],\n",
      "        [ 0.0095,  0.0299, -0.0537,  ..., -0.0049, -0.0024, -0.0244]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 22, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [-0.0698, -0.0042, -0.0006,  ...,  0.0013,  0.0085,  0.0038],\n",
      "         [ 0.0297,  0.0030, -0.0503,  ...,  0.0044, -0.0195, -0.0172],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 22, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [ 2.5000, -0.4570, -0.3770,  ...,  2.7344,  1.5391, -0.8750],\n",
      "         [ 0.8516, -2.2812, -0.1016,  ..., -2.2812, -0.5195, -1.5234],\n",
      "         [ 1.2656, -0.4531,  0.3223,  ..., -2.6562, -2.1562, -1.9219]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 48, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0613, -0.0090,  0.0483,  ..., -0.0220,  0.0122, -0.0144],\n",
      "        [-0.0234,  0.0193,  0.0016,  ..., -0.0503,  0.0173, -0.0332],\n",
      "        [ 0.0095,  0.0299, -0.0537,  ..., -0.0049, -0.0024, -0.0244]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 48]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 48, 44)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 48, 44), sparsity=-21621.21%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 48, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-13.4375, -14.0000,  -6.1250,  ..., -13.3750, -13.3750, -13.3750],\n",
      "         [-12.3125, -13.6250,  -4.9375,  ..., -12.1875, -12.1875, -12.1875],\n",
      "         [-14.5000, -15.0000, -10.8750,  ..., -14.1875, -14.1875, -14.1250]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 49\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 46, 49)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 46, 49), sparsity=-63139.04%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 49, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0312,  0.0557,  0.0013,  ..., -0.0247, -0.0022, -0.0317],\n",
      "         [ 0.0072,  0.0059, -0.0150,  ...,  0.0138,  0.0496,  0.0153],\n",
      "         [-0.0040, -0.0420,  0.0378,  ..., -0.0012,  0.0859,  0.0208]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 49, 1024]), Cross output shape: torch.Size([1, 46, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0234,  0.0193,  0.0016,  ..., -0.0503,  0.0173, -0.0332],\n",
      "        [ 0.0095,  0.0299, -0.0537,  ..., -0.0049, -0.0024, -0.0244],\n",
      "        [-0.0208, -0.0811,  0.0229,  ..., -0.0186,  0.0693, -0.0220]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 23, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0297,  0.0030, -0.0503,  ...,  0.0044, -0.0195, -0.0172],\n",
      "         [ 0.0757, -0.0004, -0.0164,  ..., -0.0027, -0.0583, -0.0957],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 23, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [ 0.8516, -2.2812, -0.1016,  ..., -2.2812, -0.5195, -1.5234],\n",
      "         [ 2.1719, -2.8125, -1.1719,  ..., -3.1094, -0.9297, -2.9844],\n",
      "         [ 2.7500, -0.2266,  0.2402,  ..., -1.2031, -4.1250, -2.0000]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 49, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0234,  0.0193,  0.0016,  ..., -0.0503,  0.0173, -0.0332],\n",
      "        [ 0.0095,  0.0299, -0.0537,  ..., -0.0049, -0.0024, -0.0244],\n",
      "        [-0.0208, -0.0811,  0.0229,  ..., -0.0186,  0.0693, -0.0220]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 49]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 49, 46)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 49, 46), sparsity=-8622.63%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 49, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-12.3125, -13.6250,  -4.9375,  ..., -12.1875, -12.1875, -12.1875],\n",
      "         [-14.5000, -15.0000, -10.8750,  ..., -14.1875, -14.1875, -14.1250],\n",
      "         [-15.6250, -15.7500,  -4.3125,  ..., -15.1250, -15.1250, -15.1250]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 50\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 48, 50)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 48, 50), sparsity=-59292.00%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 50, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [ 0.0072,  0.0059, -0.0150,  ...,  0.0138,  0.0496,  0.0153],\n",
      "         [-0.0040, -0.0420,  0.0378,  ..., -0.0012,  0.0859,  0.0208],\n",
      "         [-0.0017,  0.0087,  0.0547,  ...,  0.0608,  0.1104,  0.0273]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 50, 1024]), Cross output shape: torch.Size([1, 48, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [ 0.0095,  0.0299, -0.0537,  ..., -0.0049, -0.0024, -0.0244],\n",
      "        [-0.0208, -0.0811,  0.0229,  ..., -0.0186,  0.0693, -0.0220],\n",
      "        [ 0.0206, -0.0417,  0.0275,  ...,  0.0596,  0.0654,  0.0342]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 24, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0757, -0.0004, -0.0164,  ..., -0.0027, -0.0583, -0.0957],\n",
      "         [-0.0266,  0.0767,  0.0049,  ..., -0.0698,  0.0006, -0.0172],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 24, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [ 2.1719, -2.8125, -1.1719,  ..., -3.1094, -0.9297, -2.9844],\n",
      "         [ 0.6719, -1.9609, -0.3633,  ...,  0.6484, -3.4531, -2.2969],\n",
      "         [ 1.9688,  0.3047,  0.1035,  ..., -0.4941, -3.4062, -1.6719]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 50, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [ 0.0095,  0.0299, -0.0537,  ..., -0.0049, -0.0024, -0.0244],\n",
      "        [-0.0208, -0.0811,  0.0229,  ..., -0.0186,  0.0693, -0.0220],\n",
      "        [ 0.0206, -0.0417,  0.0275,  ...,  0.0596,  0.0654,  0.0342]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 50]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 50, 48)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 50, 48), sparsity=-9457.33%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 50, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-14.5000, -15.0000, -10.8750,  ..., -14.1875, -14.1875, -14.1250],\n",
      "         [-15.6250, -15.7500,  -4.3125,  ..., -15.1250, -15.1250, -15.1250],\n",
      "         [-12.8125, -12.8125, -10.3750,  ..., -12.6250, -12.6250, -12.6250]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 51\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 50, 51)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 50, 51), sparsity=-35238.04%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 51, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0040, -0.0420,  0.0378,  ..., -0.0012,  0.0859,  0.0208],\n",
      "         [-0.0017,  0.0087,  0.0547,  ...,  0.0608,  0.1104,  0.0273],\n",
      "         [-0.0085,  0.0166, -0.0349,  ..., -0.0144, -0.0378, -0.0126]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 51, 1024]), Cross output shape: torch.Size([1, 50, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0208, -0.0811,  0.0229,  ..., -0.0186,  0.0693, -0.0220],\n",
      "        [ 0.0206, -0.0417,  0.0275,  ...,  0.0596,  0.0654,  0.0342],\n",
      "        [-0.0513, -0.0591, -0.0552,  ..., -0.0209, -0.0118, -0.0513]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 25, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [-0.0266,  0.0767,  0.0049,  ..., -0.0698,  0.0006, -0.0172],\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 25, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [ 0.6719, -1.9609, -0.3633,  ...,  0.6484, -3.4531, -2.2969],\n",
      "         [-0.4375, -0.4590,  0.7148,  ..., -0.4961, -3.4375, -3.3438],\n",
      "         [ 0.7109, -0.6055,  0.4434,  ..., -0.3789, -1.9844, -0.1182]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 51, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0208, -0.0811,  0.0229,  ..., -0.0186,  0.0693, -0.0220],\n",
      "        [ 0.0206, -0.0417,  0.0275,  ...,  0.0596,  0.0654,  0.0342],\n",
      "        [-0.0513, -0.0591, -0.0552,  ..., -0.0209, -0.0118, -0.0513]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 51]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 51, 50)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 51, 50), sparsity=-5682.59%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 51, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-15.6250, -15.7500,  -4.3125,  ..., -15.1250, -15.1250, -15.1250],\n",
      "         [-12.8125, -12.8125, -10.3750,  ..., -12.6250, -12.6250, -12.6250],\n",
      "         [-15.3125, -15.6250,  -7.9375,  ..., -15.1250, -15.1250, -15.1250]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 52\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 52, 52)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 52, 52), sparsity=-2929.59%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 52, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0017,  0.0087,  0.0547,  ...,  0.0608,  0.1104,  0.0273],\n",
      "         [-0.0085,  0.0166, -0.0349,  ..., -0.0144, -0.0378, -0.0126],\n",
      "         [ 0.0270, -0.0513,  0.0430,  ...,  0.0265,  0.0417,  0.0386]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 52, 1024]), Cross output shape: torch.Size([1, 52, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [ 0.0206, -0.0417,  0.0275,  ...,  0.0596,  0.0654,  0.0342],\n",
      "        [-0.0513, -0.0591, -0.0552,  ..., -0.0209, -0.0118, -0.0513],\n",
      "        [-0.0112, -0.0569, -0.0273,  ...,  0.0028,  0.0201, -0.0078]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 26, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.4375, -0.4590,  0.7148,  ..., -0.4961, -3.4375, -3.3438],\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [ 1.5156,  0.2930,  2.3750,  ..., -1.6719,  0.5000, -0.6836]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 52, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [ 0.0206, -0.0417,  0.0275,  ...,  0.0596,  0.0654,  0.0342],\n",
      "        [-0.0513, -0.0591, -0.0552,  ..., -0.0209, -0.0118, -0.0513],\n",
      "        [-0.0112, -0.0569, -0.0273,  ...,  0.0028,  0.0201, -0.0078]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 52]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 52, 52)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 52, 52), sparsity=-5959.17%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 52, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-12.8125, -12.8125, -10.3750,  ..., -12.6250, -12.6250, -12.6250],\n",
      "         [-15.3125, -15.6250,  -7.9375,  ..., -15.1250, -15.1250, -15.1250],\n",
      "         [-11.9375, -12.3125,  -6.2188,  ..., -11.8750, -11.8750, -11.8750]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 53\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 52, 53)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 52, 53), sparsity=-33785.63%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 53, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0085,  0.0166, -0.0349,  ..., -0.0144, -0.0378, -0.0126],\n",
      "         [ 0.0270, -0.0513,  0.0430,  ...,  0.0265,  0.0417,  0.0386],\n",
      "         [ 0.0325, -0.0430,  0.0183,  ..., -0.0033, -0.0177,  0.0659]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 53, 1024]), Cross output shape: torch.Size([1, 52, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0513, -0.0591, -0.0552,  ..., -0.0209, -0.0118, -0.0513],\n",
      "        [-0.0112, -0.0569, -0.0273,  ...,  0.0028,  0.0201, -0.0078],\n",
      "        [-0.0046, -0.0194, -0.0737,  ..., -0.0120, -0.0762, -0.0413]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [ 0.0364, -0.0679,  0.0552,  ..., -0.0037, -0.0432, -0.0261]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 26, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.4375, -0.4590,  0.7148,  ..., -0.4961, -3.4375, -3.3438],\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [-0.0625,  1.0781,  2.5312,  ..., -0.7266, -0.8750, -1.2578]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 53, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0513, -0.0591, -0.0552,  ..., -0.0209, -0.0118, -0.0513],\n",
      "        [-0.0112, -0.0569, -0.0273,  ...,  0.0028,  0.0201, -0.0078],\n",
      "        [-0.0046, -0.0194, -0.0737,  ..., -0.0120, -0.0762, -0.0413]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 53]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 53, 52)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 53, 52), sparsity=-5844.85%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 53, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-15.3125, -15.6250,  -7.9375,  ..., -15.1250, -15.1250, -15.1250],\n",
      "         [-11.9375, -12.3125,  -6.2188,  ..., -11.8750, -11.8750, -11.8750],\n",
      "         [-12.8125, -13.3750,  -4.7188,  ..., -12.6875, -12.6875, -12.6875]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 54\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25, 25]],\n",
      "       device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 3]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 52, 54)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 52, 54), sparsity=-3984.33%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 54, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [ 0.0270, -0.0513,  0.0430,  ...,  0.0265,  0.0417,  0.0386],\n",
      "         [ 0.0325, -0.0430,  0.0183,  ..., -0.0033, -0.0177,  0.0659],\n",
      "         [-0.0283,  0.0625, -0.0581,  ...,  0.0172,  0.0679,  0.0112]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 54, 1024]), Cross output shape: torch.Size([1, 52, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0112, -0.0569, -0.0273,  ...,  0.0028,  0.0201, -0.0078],\n",
      "        [-0.0046, -0.0194, -0.0737,  ..., -0.0120, -0.0762, -0.0413],\n",
      "        [-0.0454,  0.0505, -0.0801,  ..., -0.0214,  0.0046, -0.0188]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [-0.0454, -0.0116,  0.0342,  ..., -0.0503,  0.0222, -0.0308]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 26, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.4375, -0.4590,  0.7148,  ..., -0.4961, -3.4375, -3.3438],\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [ 0.0547,  1.2969,  2.2969,  ..., -0.5547, -1.8594, -1.8828]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 54, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0112, -0.0569, -0.0273,  ...,  0.0028,  0.0201, -0.0078],\n",
      "        [-0.0046, -0.0194, -0.0737,  ..., -0.0120, -0.0762, -0.0413],\n",
      "        [-0.0454,  0.0505, -0.0801,  ..., -0.0214,  0.0046, -0.0188]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 54]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24]],\n",
      "       device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 54, 52)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 54, 52), sparsity=-5734.76%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 54, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-11.9375, -12.3125,  -6.2188,  ..., -11.8750, -11.8750, -11.8750],\n",
      "         [-12.8125, -13.3750,  -4.7188,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [-11.7500, -12.5000,  -4.3125,  ..., -12.0000, -12.0000, -12.0000]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 55\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25, 25,\n",
      "         25]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 4]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 52, 55)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 52, 55), sparsity=-33699.16%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 55, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [ 0.0325, -0.0430,  0.0183,  ..., -0.0033, -0.0177,  0.0659],\n",
      "         [-0.0283,  0.0625, -0.0581,  ...,  0.0172,  0.0679,  0.0112],\n",
      "         [ 0.0222,  0.0342,  0.0270,  ...,  0.0110,  0.0408,  0.0039]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 55, 1024]), Cross output shape: torch.Size([1, 52, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0046, -0.0194, -0.0737,  ..., -0.0120, -0.0762, -0.0413],\n",
      "        [-0.0454,  0.0505, -0.0801,  ..., -0.0214,  0.0046, -0.0188],\n",
      "        [-0.0132,  0.0087, -0.0300,  ..., -0.0188,  0.0242,  0.0007]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [ 0.0084, -0.0112,  0.0334,  ..., -0.0408,  0.0322, -0.0923]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 26, 2048]), Global transformer output: tensor([[[ 2.1625e+01,  9.8125e+00,  1.9297e+00,  ..., -6.6250e+00,\n",
      "           5.5312e+00,  1.0188e+01],\n",
      "         [ 1.4609e+00,  1.8516e+00,  1.1250e+00,  ..., -1.6797e-01,\n",
      "          -9.7266e-01,  2.6406e+00],\n",
      "         [ 1.4688e+00,  3.9844e-01, -1.0859e+00,  ..., -1.2656e+00,\n",
      "           2.0938e+00,  1.4766e+00],\n",
      "         ...,\n",
      "         [-4.3750e-01, -4.5898e-01,  7.1484e-01,  ..., -4.9609e-01,\n",
      "          -3.4375e+00, -3.3438e+00],\n",
      "         [-1.6406e-01,  1.3281e+00,  2.0312e+00,  ..., -3.0469e-01,\n",
      "          -1.5234e+00, -2.2188e+00],\n",
      "         [-1.5625e-02,  5.6250e-01,  2.8438e+00,  ..., -1.1875e+00,\n",
      "          -1.7109e+00, -8.7891e-01]]], device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 55, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0046, -0.0194, -0.0737,  ..., -0.0120, -0.0762, -0.0413],\n",
      "        [-0.0454,  0.0505, -0.0801,  ..., -0.0214,  0.0046, -0.0188],\n",
      "        [-0.0132,  0.0087, -0.0300,  ..., -0.0188,  0.0242,  0.0007]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 55]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24,\n",
      "         24]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 55, 52)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 55, 52), sparsity=-5628.67%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 55, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-12.8125, -13.3750,  -4.7188,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [-11.7500, -12.5000,  -4.3125,  ..., -12.0000, -12.0000, -12.0000],\n",
      "         [-10.6875, -11.3750,  -1.0156,  ..., -10.6875, -10.6875, -10.6875]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 56\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25, 25,\n",
      "         25, 25]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 5]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 52, 56)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 52, 56), sparsity=-3275.82%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 56, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0283,  0.0625, -0.0581,  ...,  0.0172,  0.0679,  0.0112],\n",
      "         [ 0.0222,  0.0342,  0.0270,  ...,  0.0110,  0.0408,  0.0039],\n",
      "         [ 0.0120,  0.0254,  0.0420,  ...,  0.0688,  0.0493,  0.1196]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 56, 1024]), Cross output shape: torch.Size([1, 52, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0454,  0.0505, -0.0801,  ..., -0.0214,  0.0046, -0.0188],\n",
      "        [-0.0132,  0.0087, -0.0300,  ..., -0.0188,  0.0242,  0.0007],\n",
      "        [-0.0061, -0.0135,  0.0126,  ...,  0.0415,  0.0129,  0.0703]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [-0.0037,  0.0106,  0.0654,  ..., -0.0194,  0.0074, -0.0796]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 26, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.4375, -0.4590,  0.7148,  ..., -0.4961, -3.4375, -3.3438],\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [ 0.4648,  0.0859,  2.1406,  ..., -1.6484, -1.8828, -1.3125]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 56, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0454,  0.0505, -0.0801,  ..., -0.0214,  0.0046, -0.0188],\n",
      "        [-0.0132,  0.0087, -0.0300,  ..., -0.0188,  0.0242,  0.0007],\n",
      "        [-0.0061, -0.0135,  0.0126,  ...,  0.0415,  0.0129,  0.0703]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 56]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24,\n",
      "         24, 24]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 56, 52)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 56, 52), sparsity=-5526.37%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 56, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-11.7500, -12.5000,  -4.3125,  ..., -12.0000, -12.0000, -12.0000],\n",
      "         [-10.6875, -11.3750,  -1.0156,  ..., -10.6875, -10.6875, -10.6875],\n",
      "         [-10.5000, -11.0625,  -2.2656,  ..., -10.5625, -10.5625, -10.5625]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 57\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25, 25,\n",
      "         25, 25, 25]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 6]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 52, 57)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 52, 57), sparsity=-33618.76%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 57, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [ 0.0222,  0.0342,  0.0270,  ...,  0.0110,  0.0408,  0.0039],\n",
      "         [ 0.0120,  0.0254,  0.0420,  ...,  0.0688,  0.0493,  0.1196],\n",
      "         [ 0.0830, -0.0957,  0.0229,  ..., -0.0198,  0.0223, -0.0332]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 57, 1024]), Cross output shape: torch.Size([1, 52, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0132,  0.0087, -0.0300,  ..., -0.0188,  0.0242,  0.0007],\n",
      "        [-0.0061, -0.0135,  0.0126,  ...,  0.0415,  0.0129,  0.0703],\n",
      "        [ 0.0469, -0.0864, -0.0033,  ..., -0.0361, -0.0090, -0.0723]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [ 0.0215,  0.0527,  0.0469,  ..., -0.0106,  0.0233, -0.0845]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 26, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.4375, -0.4590,  0.7148,  ..., -0.4961, -3.4375, -3.3438],\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [ 0.4141, -0.1055,  2.2656,  ..., -1.1953, -1.8906, -1.2812]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 57, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0132,  0.0087, -0.0300,  ..., -0.0188,  0.0242,  0.0007],\n",
      "        [-0.0061, -0.0135,  0.0126,  ...,  0.0415,  0.0129,  0.0703],\n",
      "        [ 0.0469, -0.0864, -0.0033,  ..., -0.0361, -0.0090, -0.0723]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 57]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24,\n",
      "         24, 24, 24]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 57, 52)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 57, 52), sparsity=-5427.67%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 57, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-10.6875, -11.3750,  -1.0156,  ..., -10.6875, -10.6875, -10.6875],\n",
      "         [-10.5000, -11.0625,  -2.2656,  ..., -10.5625, -10.5625, -10.5625],\n",
      "         [-12.0625, -12.8750,  -2.1562,  ..., -12.2500, -12.1875, -12.1875]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 58\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25, 25,\n",
      "         25, 25, 25, 25]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 7]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 52, 58)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 52, 58), sparsity=-2616.18%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 58, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [ 0.0120,  0.0254,  0.0420,  ...,  0.0688,  0.0493,  0.1196],\n",
      "         [ 0.0830, -0.0957,  0.0229,  ..., -0.0198,  0.0223, -0.0332],\n",
      "         [-0.0493, -0.0400,  0.0156,  ..., -0.0291,  0.0181,  0.0021]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 58, 1024]), Cross output shape: torch.Size([1, 52, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0061, -0.0135,  0.0126,  ...,  0.0415,  0.0129,  0.0703],\n",
      "        [ 0.0469, -0.0864, -0.0033,  ..., -0.0361, -0.0090, -0.0723],\n",
      "        [-0.0226, -0.0518, -0.0366,  ..., -0.0583, -0.0012, -0.0327]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 26, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [ 0.0850, -0.0007, -0.0059,  ..., -0.0381, -0.0056, -0.0454],\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [-0.0415,  0.0703,  0.0845,  ..., -0.0442,  0.0011, -0.0693]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 26, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.4375, -0.4590,  0.7148,  ..., -0.4961, -3.4375, -3.3438],\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [ 0.3438, -1.1641,  1.9453,  ..., -0.9141, -1.7344, -1.1875]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 58, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0061, -0.0135,  0.0126,  ...,  0.0415,  0.0129,  0.0703],\n",
      "        [ 0.0469, -0.0864, -0.0033,  ..., -0.0361, -0.0090, -0.0723],\n",
      "        [-0.0226, -0.0518, -0.0366,  ..., -0.0583, -0.0012, -0.0327]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 58]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24,\n",
      "         24, 24, 24, 24]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 58, 52)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 58, 52), sparsity=-5332.36%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 58, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-10.5000, -11.0625,  -2.2656,  ..., -10.5625, -10.5625, -10.5625],\n",
      "         [-12.0625, -12.8750,  -2.1562,  ..., -12.2500, -12.1875, -12.1875],\n",
      "         [-11.8125, -12.0000,  -3.3750,  ..., -11.5000, -11.5000, -11.5000]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 59\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25, 25,\n",
      "         25, 25, 25, 25, 25]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 7, 1]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 54, 59)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 54, 59), sparsity=-32297.74%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 59, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [ 0.0830, -0.0957,  0.0229,  ..., -0.0198,  0.0223, -0.0332],\n",
      "         [-0.0493, -0.0400,  0.0156,  ..., -0.0291,  0.0181,  0.0021],\n",
      "         [-0.1211,  0.0175,  0.0339,  ...,  0.0713, -0.0305, -0.0231]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 59, 1024]), Cross output shape: torch.Size([1, 54, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [ 0.0469, -0.0864, -0.0033,  ..., -0.0361, -0.0090, -0.0723],\n",
      "        [-0.0226, -0.0518, -0.0366,  ..., -0.0583, -0.0012, -0.0327],\n",
      "        [-0.0879,  0.0093, -0.0106,  ...,  0.0215, -0.0332, -0.0742]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 27, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [-0.0635,  0.0640,  0.0840,  ..., -0.0339,  0.0053, -0.0610],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 27, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [-0.6719, -1.9766,  2.0625,  ...,  0.9766, -2.0781, -1.6406],\n",
      "         [ 0.2812, -1.3672, -0.7422,  ..., -0.1777, -2.2031, -0.7500]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 59, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [ 0.0469, -0.0864, -0.0033,  ..., -0.0361, -0.0090, -0.0723],\n",
      "        [-0.0226, -0.0518, -0.0366,  ..., -0.0583, -0.0012, -0.0327],\n",
      "        [-0.0879,  0.0093, -0.0106,  ...,  0.0215, -0.0332, -0.0742]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 59]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24,\n",
      "         24, 24, 24, 24, 25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 59, 54)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 59, 54), sparsity=-4528.25%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 59, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-12.0625, -12.8750,  -2.1562,  ..., -12.2500, -12.1875, -12.1875],\n",
      "         [-11.8125, -12.0000,  -3.3750,  ..., -11.5000, -11.5000, -11.5000],\n",
      "         [-15.5000, -15.8125,  -8.7500,  ..., -15.3750, -15.3750, -15.3750]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "batch size: 1, sequence length: 60\n",
      "Patch IDs: tensor([[ 0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  6,  6,  6,  6,  6,  7,\n",
      "          8,  9,  9, 10, 10, 10, 10, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "         14, 15, 16, 16, 16, 17, 18, 18, 19, 20, 20, 20, 21, 22, 23, 24, 25, 25,\n",
      "         25, 25, 25, 25, 25, 26]], device='cuda:0'), Patch lengths: tensor([[1, 1, 3, 3, 3, 1, 5, 1, 1, 2, 6, 1, 4, 4, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1,\n",
      "         1, 7, 2]], device='cuda:0')\n",
      "Cross attention mask encoder shape: (1, 1, 54, 60)\n",
      "Cross attention mask encoder: BlockMask(shape=(1, 1, 54, 60), sparsity=-2428.40%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "local_encoder_embeds.shape=torch.Size([1, 60, 1024])\n",
      "local_encoder_embeds=tensor([[[ 0.1084,  0.0052, -0.0938,  ..., -0.0598, -0.1279,  0.0181],\n",
      "         [-0.0135,  0.0476,  0.0266,  ..., -0.0229, -0.0381, -0.0337],\n",
      "         [ 0.0349,  0.0091, -0.0310,  ...,  0.0245, -0.0337,  0.0134],\n",
      "         ...,\n",
      "         [-0.0493, -0.0400,  0.0156,  ..., -0.0291,  0.0181,  0.0021],\n",
      "         [-0.1211,  0.0175,  0.0339,  ...,  0.0713, -0.0305, -0.0231],\n",
      "         [-0.0153, -0.0054,  0.0193,  ...,  0.0289,  0.0386, -0.0200]]],\n",
      "       device='cuda:0')\n",
      "Encoder output shape: torch.Size([1, 60, 1024]), Cross output shape: torch.Size([1, 54, 1024])\n",
      "Encoder `h_encoder` output: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0226, -0.0518, -0.0366,  ..., -0.0583, -0.0012, -0.0327],\n",
      "        [-0.0879,  0.0093, -0.0106,  ...,  0.0215, -0.0332, -0.0742],\n",
      "        [-0.0486, -0.0101, -0.0317,  ...,  0.0209,  0.0125, -0.0269]],\n",
      "       device='cuda:0')\n",
      "Global transformer input shape: torch.Size([1, 27, 2048]), Global transformer input: tensor([[[-0.0723, -0.0277, -0.0481,  ...,  0.0210,  0.0820,  0.0259],\n",
      "         [-0.0713,  0.0791, -0.0386,  ...,  0.0432, -0.0125,  0.0066],\n",
      "         [ 0.0186,  0.0022,  0.0435,  ...,  0.0052,  0.0339, -0.0188],\n",
      "         ...,\n",
      "         [-0.0101, -0.0344,  0.0361,  ..., -0.0359, -0.0172, -0.0388],\n",
      "         [-0.0635,  0.0640,  0.0840,  ..., -0.0339,  0.0053, -0.0610],\n",
      "         [ 0.0260,  0.0254, -0.0583,  ..., -0.0177, -0.0143, -0.0500]]],\n",
      "       device='cuda:0')\n",
      "Global transformer output shape: torch.Size([1, 27, 2048]), Global transformer output: tensor([[[21.6250,  9.8125,  1.9297,  ..., -6.6250,  5.5312, 10.1875],\n",
      "         [ 1.4609,  1.8516,  1.1250,  ..., -0.1680, -0.9727,  2.6406],\n",
      "         [ 1.4688,  0.3984, -1.0859,  ..., -1.2656,  2.0938,  1.4766],\n",
      "         ...,\n",
      "         [-0.1641,  1.3281,  2.0312,  ..., -0.3047, -1.5234, -2.2188],\n",
      "         [-0.6719, -1.9766,  2.0625,  ...,  0.9766, -2.0781, -1.6406],\n",
      "         [ 0.4727, -3.2188,  0.3652,  ...,  1.5156, -2.5156, -2.3125]]],\n",
      "       device='cuda:0')\n",
      "Decoder embeddings `dec_embeds` shape: torch.Size([1, 60, 1024]), Decoder embeddings: tensor([[ 0.1279, -0.0454, -0.3789,  ..., -0.1533, -0.6094, -0.3203],\n",
      "        [-0.0098,  0.0117, -0.0168,  ..., -0.0674, -0.0894, -0.0493],\n",
      "        [ 0.0327, -0.0508, -0.0859,  ..., -0.0079, -0.1084, -0.0425],\n",
      "        ...,\n",
      "        [-0.0226, -0.0518, -0.0366,  ..., -0.0583, -0.0012, -0.0327],\n",
      "        [-0.0879,  0.0093, -0.0106,  ...,  0.0215, -0.0332, -0.0742],\n",
      "        [-0.0486, -0.0101, -0.0317,  ...,  0.0209,  0.0125, -0.0269]],\n",
      "       device='cuda:0')\n",
      "Decoder patch IDs shape: torch.Size([1, 60]), Decoder patch IDs: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  5,  5,  5,  5,  5,  6,  7,\n",
      "          8,  8,  9,  9,  9,  9,  9,  9, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13,\n",
      "         14, 15, 15, 15, 16, 17, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24,\n",
      "         24, 24, 24, 24, 25, 25]], device='cuda:0')\n",
      "Cross attention mask decoder shape: (1, 1, 60, 54)\n",
      "Cross attention mask decoder: BlockMask(shape=(1, 1, 60, 54), sparsity=-43388.40%, \n",
      "(0, 0)\n",
      "\n",
      ")\n",
      "Decoder logits shape: torch.Size([1, 60, 260]), Decoder logits: tensor([[[-10.4375, -10.6875,  -6.1875,  ..., -10.5000, -10.5000, -10.5000],\n",
      "         [-12.6875, -12.7500,  -7.6562,  ..., -12.6875, -12.6875, -12.6875],\n",
      "         [ -9.4375,  -9.5000,  -5.7812,  ...,  -9.5625,  -9.5625,  -9.5625],\n",
      "         ...,\n",
      "         [-11.8125, -12.0000,  -3.3750,  ..., -11.5000, -11.5000, -11.5000],\n",
      "         [-15.5000, -15.8125,  -8.7500,  ..., -15.3750, -15.3750, -15.3750],\n",
      "         [-11.1250, -12.0000,  -4.5938,  ..., -11.3750, -11.3750, -11.3125]]],\n",
      "       device='cuda:0', dtype=torch.float32)\n",
      "Prompt: \"The quick brown fox jumps over the lazy dog \" Completion: \"is a sentence th\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompts = [prompt]\n",
    "# outputs = generate_nocache(\n",
    "#     prompts, model=model, tokenizer=tokenizer, patcher=patcher, max_gen_len=16\n",
    "# )\n",
    "# text_outputs = [tokenizer.decode(t) for t in outputs]\n",
    "# for p, t in zip(prompts, text_outputs):\n",
    "#     print(f'Prompt: \"{p}\" Completion: \"{t}\"')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0a267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67396ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# NotImplementedError                       Traceback (most recent call last)\n",
    "# Cell In[5], line 2\n",
    "#       1 prompts = [prompt]\n",
    "# ----> 2 outputs = generate_nocache(\n",
    "#       3     prompts, model=model, tokenizer=tokenizer, patcher=patcher\n",
    "#       4 )\n",
    "#       5 text_outputs = [tokenizer.decode(t) for t in outputs]\n",
    "#       6 for p, t in zip(prompts, text_outputs):\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\bytelatent\\generate_blt.py:131, in generate_nocache(prompts, model, tokenizer, patcher, max_prompt_len, max_gen_len, use_sampling, temp, top_k, top_p, remove_prompts)\n",
    "#     129 for i, curr_pos in enumerate(range(start_pos, end_pos)):\n",
    "#     130     current_tokens = tokens[:, :curr_pos]\n",
    "# --> 131     patch_lengths, _ = patcher.patch(current_tokens, include_next_token=True)\n",
    "#     132     logits = model(current_tokens, patch_lengths=patch_lengths)[:, -1]\n",
    "#     134     if use_sampling:\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\bytelatent\\data\\patcher.py:565, in Patcher.patch(self, tokens, include_next_token, preds, entropies, threshold)\n",
    "#     563 else:\n",
    "#     564     start_entropies = time.time()\n",
    "# --> 565     scores, _ = calculate_entropies(\n",
    "#     566         tokens,\n",
    "#     567         self.entropy_model,\n",
    "#     568         self.patching_batch_size,\n",
    "#     569         self.device,\n",
    "#     570     )\n",
    "#     571 if self.log_time:\n",
    "#     572     self.log[\"calculate_entropies\"] += time.time() - s\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\bytelatent\\data\\patcher.py:96, in calculate_entropies(tokens, entropy_model, patching_batch_size, device, enable_grad)\n",
    "#      94     split = split.to(device)\n",
    "#      95 # assert torch.all(split >= 0) and torch.all(split < 260)\n",
    "# ---> 96 pred = entropy_model(split)\n",
    "#      97 pred = pred.reshape(-1, pred.shape[-1])[\n",
    "#      98     : split.numel() - pad_size, :\n",
    "#      99 ]  # [batch_size * seq_len, vocab]\n",
    "#     100 preds.append(pred)\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
    "#    1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
    "#    1735 else:\n",
    "# -> 1736     return self._call_impl(*args, **kwargs)\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747, in Module._call_impl(self, *args, **kwargs)\n",
    "#    1742 # If we don't have any hooks, we want to skip the rest of the logic in\n",
    "#    1743 # this function, and just call forward.\n",
    "#    1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
    "#    1745         or _global_backward_pre_hooks or _global_backward_hooks\n",
    "#    1746         or _global_forward_hooks or _global_forward_pre_hooks):\n",
    "# -> 1747     return forward_call(*args, **kwargs)\n",
    "#    1749 result = None\n",
    "#    1750 called_always_called_hooks = set()\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\bytelatent\\transformer.py:131, in LMTransformer.forward(self, token_values, target, tok_idx, mask, attn_impl)\n",
    "#     117 h = self.tok_embeddings(token_values)\n",
    "#     119 mask = (\n",
    "#     120     mask\n",
    "#     121     if mask is not None\n",
    "#    (...)    129     )\n",
    "#     130 )\n",
    "# --> 131 h = super().forward(h, tok_idx=tok_idx, mask=mask, attn_impl=attn_impl)\n",
    "#     133 logits = self.output(self.norm(h))\n",
    "#     134 if target is not None:\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\bytelatent\\base_transformer.py:617, in BaseTransformer.forward(self, h, tok_idx, mask, attn_impl)\n",
    "#     614 freq_cis = self.rope_embeddings(seqlen=self.max_seqlen, tok_idx=tok_idx)\n",
    "#     616 for i, layer in enumerate(self.layers):\n",
    "# --> 617     h = layer(h, freq_cis, tok_idx=tok_idx, mask=mask, attn_impl=attn_impl)\n",
    "#     618 return h\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
    "#    1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
    "#    1735 else:\n",
    "# -> 1736     return self._call_impl(*args, **kwargs)\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747, in Module._call_impl(self, *args, **kwargs)\n",
    "#    1742 # If we don't have any hooks, we want to skip the rest of the logic in\n",
    "#    1743 # this function, and just call forward.\n",
    "#    1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
    "#    1745         or _global_backward_pre_hooks or _global_backward_hooks\n",
    "#    1746         or _global_forward_hooks or _global_forward_pre_hooks):\n",
    "# -> 1747     return forward_call(*args, **kwargs)\n",
    "#    1749 result = None\n",
    "#    1750 called_always_called_hooks = set()\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\bytelatent\\base_transformer.py:556, in TransformerBlock.forward(self, x, freq_cis, tok_idx, mask, attn_impl)\n",
    "#     548 def forward(\n",
    "#     549     self,\n",
    "#     550     x: torch.Tensor,\n",
    "#    (...)    554     attn_impl: str = \"sdpa\",\n",
    "#     555 ) -> torch.Tensor:\n",
    "# --> 556     attn_out = self.attention(\n",
    "#     557         self.attention_norm(x),\n",
    "#     558         freq_cis,\n",
    "#     559         tok_idx=tok_idx,\n",
    "#     560         mask=mask,\n",
    "#     561         attn_impl=attn_impl,\n",
    "#     562     )\n",
    "#     563     h = x + attn_out\n",
    "#     564     h_norm = self.ffn_norm(h)\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
    "#    1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
    "#    1735 else:\n",
    "# -> 1736     return self._call_impl(*args, **kwargs)\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747, in Module._call_impl(self, *args, **kwargs)\n",
    "#    1742 # If we don't have any hooks, we want to skip the rest of the logic in\n",
    "#    1743 # this function, and just call forward.\n",
    "#    1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
    "#    1745         or _global_backward_pre_hooks or _global_backward_hooks\n",
    "#    1746         or _global_forward_hooks or _global_forward_pre_hooks):\n",
    "# -> 1747     return forward_call(*args, **kwargs)\n",
    "#    1749 result = None\n",
    "#    1750 called_always_called_hooks = set()\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\bytelatent\\base_transformer.py:401, in Attention.forward(self, x, freq_cis, tok_idx, mask, attn_impl)\n",
    "#     399 query_shape = xq.shape\n",
    "#     400 xq, xk, xv = _reshape_for_attn_bias(mask, xq, xk, xv)\n",
    "# --> 401 output = fmha.memory_efficient_attention(xq, xk, xv, attn_bias=mask)\n",
    "#     402 output = output.view(query_shape)\n",
    "#     403 # This uses B S H D instead of B H S D of pytorch\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\xformers\\ops\\fmha\\__init__.py:306, in memory_efficient_attention(query, key, value, attn_bias, p, scale, op, output_dtype)\n",
    "#     194 def memory_efficient_attention(\n",
    "#     195     query: torch.Tensor,\n",
    "#     196     key: torch.Tensor,\n",
    "#    (...)    203     output_dtype: Optional[torch.dtype] = None,\n",
    "#     204 ) -> torch.Tensor:\n",
    "#     205     \"\"\"Implements the memory-efficient attention mechanism following\n",
    "#     206     `\"Self-Attention Does Not Need O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\n",
    "#     207 \n",
    "#    (...)    304     :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\n",
    "#     305     \"\"\"\n",
    "# --> 306     return _memory_efficient_attention(\n",
    "#     307         Inputs(\n",
    "#     308             query=query,\n",
    "#     309             key=key,\n",
    "#     310             value=value,\n",
    "#     311             p=p,\n",
    "#     312             attn_bias=attn_bias,\n",
    "#     313             scale=scale,\n",
    "#     314             output_dtype=output_dtype,\n",
    "#     315         ),\n",
    "#     316         op=op,\n",
    "#     317     )\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\xformers\\ops\\fmha\\__init__.py:467, in _memory_efficient_attention(inp, op)\n",
    "#     462 def _memory_efficient_attention(\n",
    "#     463     inp: Inputs, op: Optional[AttentionOp] = None\n",
    "#     464 ) -> torch.Tensor:\n",
    "#     465     # fast-path that doesn't require computing the logsumexp for backward computation\n",
    "#     466     if all(x.requires_grad is False for x in [inp.query, inp.key, inp.value]):\n",
    "# --> 467         return _memory_efficient_attention_forward(\n",
    "#     468             inp, op=op[0] if op is not None else None\n",
    "#     469         )\n",
    "#     471     output_shape = inp.normalize_bmhk()\n",
    "#     473     op_fw = _serialize_op(op[0] if op is not None else None)\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\xformers\\ops\\fmha\\__init__.py:486, in _memory_efficient_attention_forward(inp, op)\n",
    "#     484 output_shape = inp.normalize_bmhk()\n",
    "#     485 if op is None:\n",
    "# --> 486     op = _dispatch_fw(inp, False)\n",
    "#     487 else:\n",
    "#     488     _ensure_op_supports_or_raise(ValueError, \"memory_efficient_attention\", op, inp)\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\xformers\\ops\\fmha\\dispatch.py:135, in _dispatch_fw(inp, needs_gradient)\n",
    "#     126 def _dispatch_fw(inp: Inputs, needs_gradient: bool) -> Type[AttentionFwOpBase]:\n",
    "#     127     \"\"\"Computes the best operator for forward\n",
    "#     128 \n",
    "#     129     Raises:\n",
    "#    (...)    133         AttentionOp: The best operator for the configuration\n",
    "#     134     \"\"\"\n",
    "# --> 135     return _run_priority_list(\n",
    "#     136         \"memory_efficient_attention_forward\",\n",
    "#     137         _dispatch_fw_priority_list(inp, needs_gradient),\n",
    "#     138         inp,\n",
    "#     139     )\n",
    "\n",
    "# File c:\\Users\\leoni\\Documents\\projects\\blt\\venv\\Lib\\site-packages\\xformers\\ops\\fmha\\dispatch.py:76, in _run_priority_list(name, priority_list, inp, extra_op_reasons)\n",
    "#      74     for op, not_supported in extra_op_reasons:\n",
    "#      75         msg += \"\\n\" + _format_not_supported_reasons(op, not_supported)\n",
    "# ---> 76 raise NotImplementedError(msg)\n",
    "\n",
    "# NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:\n",
    "#      query       : shape=(1, 8192, 12, 64) (torch.bfloat16)\n",
    "#      key         : shape=(1, 8192, 12, 64) (torch.bfloat16)\n",
    "#      value       : shape=(1, 8192, 12, 64) (torch.bfloat16)\n",
    "#      attn_bias   : <class 'xformers.ops.fmha.attn_bias.BlockDiagonalCausalLocalAttentionMask'>\n",
    "#      p           : 0.0\n",
    "# `fa2F@v2.5.7-pt` is not supported because:\n",
    "#     xFormers wasn't build with CUDA support\n",
    "# `cutlassF-pt` is not supported because:\n",
    "#     xFormers wasn't build with CUDA support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1c2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf44ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df5714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
